{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navaneethclt/Mah/blob/master/ARLLSTMIA_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3io9S4IlEoPr"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Dec  9 15:37:33 2023\n",
        "\n",
        "@author: navan\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Dec  1 14:45:05 2023\n",
        "\n",
        "@author: navan\n",
        "\"\"\"\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, LSTM, Dense,BatchNormalization,MaxPooling1D,Flatten,Bidirectional,TimeDistributed\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X89SBhzo6QqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEWL7y0zFQFA",
        "outputId": "7d9cdccd-0027-4905-a9d0-9f708b9f95fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fi3El5qqFTqq"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/ARL/TimeS2 -d /content/sample_data &> /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REg_sGD_FOLp"
      },
      "outputs": [],
      "source": [
        "\n",
        "fnames = glob.glob('/content/sample_data/TimeS2/25_1/*.csv')\n",
        "\n",
        "\n",
        "\n",
        "fnames2 = []\n",
        "i = 0\n",
        "for f in fnames:\n",
        " fnames2.append(os.path.splitext(os.path.split(f)[-1])[-2])\n",
        " i = i + 1\n",
        "\n",
        "operation_condition = [int(s) for s in fnames2 if s.isdigit()]\n",
        "operation_condition = sorted(operation_condition)\n",
        "#operation_condition = list(range(46, 91)) + list(range(99, 114))\n",
        "\n",
        "\n",
        "\n",
        "XData1 = []\n",
        "XData2 = []\n",
        "column_names = list(range(23))  # Creates a list of integers from 0 to 22\n",
        "\n",
        "XData3 = pd.DataFrame()\n",
        "XData3val= pd.DataFrame()\n",
        "XData3test= pd.DataFrame()\n",
        "\n",
        "#data3 = pd.read_excel('/content/sample_data/DLdata/dataLMS/RunTable.xlsx')\n",
        "path1 ='/content/sample_data/TimeS2/25_1/TimeS/LMS/'\n",
        "path2 ='/content/sample_data/TimeS2/25_1/TimeS/RMS/'\n",
        "path3 ='/content/sample_data/TimeS2/25_1/TimeS/D/'\n",
        "path4 ='/content/sample_data/TimeS2/25_1/'\n",
        "\n",
        "\n",
        "df1 = pd.DataFrame(columns = ['filenames','class'])\n",
        "df2 = pd.DataFrame(columns = ['filenames','class'])\n",
        "df3 = pd.DataFrame(columns = ['filenames','class'])\n",
        "\n",
        "count2 = 0\n",
        "count3 = 0\n",
        "valN = 10\n",
        "for i  in operation_condition: #len(fnames2)\n",
        "  filename3 = os.path.join(path4,str(i)+'.csv')\n",
        "  data3 = pd.read_csv(filename3,header = None)\n",
        "  if(min(data3.iloc[:,0]-data3.iloc[:,16])>0):\n",
        "\n",
        "   if(i%valN)==0:\n",
        "\n",
        "    df2.loc[count2,'filenames'] = filename3\n",
        "    df2.loc[count2,'class'] = str(i)\n",
        "    count2 = count2 + 1\n",
        "    XData3val = pd.concat([XData3val,data3],ignore_index=True,axis=0)\n",
        "   else:\n",
        "    df1.loc[count3,'filenames'] = filename3\n",
        "    df1.loc[count3,'class'] = str(i)\n",
        "    count3 = count3 + 1\n",
        "    XData3 = pd.concat([XData3,data3],ignore_index=True,axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################\n",
        "fnames = glob.glob('/content/sample_data/TimeS2/30_2/*.csv')\n",
        "\n",
        "\n",
        "\n",
        "fnames2 = []\n",
        "i = 0\n",
        "for f in fnames:\n",
        " fnames2.append(os.path.splitext(os.path.split(f)[-1])[-2])\n",
        " i = i + 1\n",
        "\n",
        "operation_condition = [int(s) for s in fnames2 if s.isdigit()]\n",
        "operation_condition = sorted(operation_condition)\n",
        "#operation_condition = list(range(46, 91)) + list(range(99, 114))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data3 = pd.read_excel('/content/sample_data/DLdata/dataLMS/RunTable.xlsx')\n",
        "path1 ='/content/sample_data/TimeS2/30_2/TimeS/LMS/'\n",
        "path2 ='/content/sample_data/TimeS2/30_2/TimeS/RMS/'\n",
        "path3 ='/content/sample_data/TimeS2/30_2/TimeS/D/'\n",
        "path4 ='/content/sample_data/TimeS2/30_2/'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i  in operation_condition: #len(fnames2)\n",
        "\n",
        "  filename3 = os.path.join(path4,str(i)+'.csv')\n",
        "  data3 = pd.read_csv(filename3,header = None)\n",
        "\n",
        "\n",
        "\n",
        "  if(min(data3.iloc[:,0]-data3.iloc[:,16])>0):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   if(i%valN)==0:\n",
        "\n",
        "    df2.loc[count2,'filenames'] = filename3\n",
        "    df2.loc[count2,'class'] = str(i)\n",
        "    count2 = count2 + 1\n",
        "    XData3val = pd.concat([XData3val,data3],ignore_index=True,axis=0)\n",
        "   else:\n",
        "    df1.loc[count3,'filenames'] = filename3\n",
        "    df1.loc[count3,'class'] = str(i)\n",
        "    count3 = count3 + 1\n",
        "    XData3 = pd.concat([XData3,data3],ignore_index=True,axis=0)\n",
        "\n",
        "\n",
        "########################################################\n",
        "fnames = glob.glob('/content/sample_data/TimeS2/35_1/*.csv')\n",
        "\n",
        "\n",
        "\n",
        "fnames2 = []\n",
        "i = 0\n",
        "for f in fnames:\n",
        " fnames2.append(os.path.splitext(os.path.split(f)[-1])[-2])\n",
        " i = i + 1\n",
        "\n",
        "operation_condition = [int(s) for s in fnames2 if s.isdigit()]\n",
        "operation_condition = sorted(operation_condition)\n",
        "#operation_condition = list(range(46, 91)) + list(range(99, 114))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data3 = pd.read_excel('/content/sample_data/DLdata/dataLMS/RunTable.xlsx')\n",
        "path1 ='/content/sample_data/TimeS2/35_1/TimeS/LMS/'\n",
        "path2 ='/content/sample_data/TimeS2/35_1/TimeS/RMS/'\n",
        "path3 ='/content/sample_data/TimeS2/35_1/TimeS/D/'\n",
        "path4 ='/content/sample_data/TimeS2/35_1/'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i  in operation_condition: #len(fnames2)\n",
        "\n",
        "  filename3 = os.path.join(path4,str(i)+'.csv')\n",
        "  data3 = pd.read_csv(filename3,header = None)\n",
        "  if(min(data3.iloc[:,0]-data3.iloc[:,16])>0):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   if(i%valN)==0:\n",
        "\n",
        "    df2.loc[count2,'filenames'] = filename3\n",
        "    df2.loc[count2,'class'] = str(i)\n",
        "    count2 = count2 + 1\n",
        "    XData3val = pd.concat([XData3val,data3],ignore_index=True,axis=0)\n",
        "   else:\n",
        "    df1.loc[count3,'filenames'] = filename3\n",
        "    df1.loc[count3,'class'] = str(i)\n",
        "    count3 = count3 + 1\n",
        "    XData3 = pd.concat([XData3,data3],ignore_index=True,axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################\n",
        "fnames = glob.glob('/content/sample_data/TimeS2/48_2/*.csv')\n",
        "\n",
        "\n",
        "\n",
        "fnames2 = []\n",
        "i = 0\n",
        "for f in fnames:\n",
        " fnames2.append(os.path.splitext(os.path.split(f)[-1])[-2])\n",
        " i = i + 1\n",
        "\n",
        "operation_condition = [int(s) for s in fnames2 if s.isdigit()]\n",
        "operation_condition = sorted(operation_condition)\n",
        "#operation_condition = list(range(46, 91)) + list(range(99, 114))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data3 = pd.read_excel('/content/sample_data/DLdata/dataLMS/RunTable.xlsx')\n",
        "path1 ='/content/sample_data/TimeS2/48_2/TimeS/LMS/'\n",
        "path2 ='/content/sample_data/TimeS2/48_2/TimeS/RMS/'\n",
        "path3 ='/content/sample_data/TimeS2/48_2/TimeS/D/'\n",
        "path4 ='/content/sample_data/TimeS2/48_2/'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i  in operation_condition: #len(fnames2)\n",
        "\n",
        "  filename3 = os.path.join(path4,str(i)+'.csv')\n",
        "  data3 = pd.read_csv(filename3,header = None)\n",
        "  if(min(data3.iloc[:,0]-data3.iloc[:,16])>0):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   if(i%valN)==0:\n",
        "\n",
        "    df2.loc[count2,'filenames'] = filename3\n",
        "    df2.loc[count2,'class'] = str(i)\n",
        "    count2 = count2 + 1\n",
        "    XData3val = pd.concat([XData3val,data3],ignore_index=True,axis=0)\n",
        "   else:\n",
        "    df1.loc[count3,'filenames'] = filename3\n",
        "    df1.loc[count3,'class'] = str(i)\n",
        "    count3 = count3 + 1\n",
        "    XData3 = pd.concat([XData3,data3],ignore_index=True,axis=0)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "####################################################################\n",
        "\n",
        "fnames = glob.glob('/content/sample_data/TimeS2/30_1/*.csv')\n",
        "\n",
        "\n",
        "\n",
        "fnames2 = []\n",
        "i = 0\n",
        "for f in fnames:\n",
        " fnames2.append(os.path.splitext(os.path.split(f)[-1])[-2])\n",
        " i = i + 1\n",
        "\n",
        "operation_condition = [int(s) for s in fnames2 if s.isdigit()]\n",
        "operation_condition = sorted(operation_condition)\n",
        "#operation_condition = list(range(46, 91)) + list(range(99, 114))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data3 = pd.read_excel('/content/sample_data/DLdata/dataLMS/RunTable.xlsx')\n",
        "path1 ='/content/sample_data/TimeS2/30_1/TimeS/LMS/'\n",
        "path2 ='/content/sample_data/TimeS2/30_1/TimeS/RMS/'\n",
        "path3 ='/content/sample_data/TimeS2/30_1/TimeS/D/'\n",
        "path4 ='/content/sample_data/TimeS2/30_1/'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count1 = 0\n",
        "for i  in operation_condition: #len(fnames2)\n",
        "\n",
        "  filename3 = os.path.join(path4,str(i)+'.csv')\n",
        "  data3 = pd.read_csv(filename3,header = None)\n",
        "  if(min(data3.iloc[:,0]-data3.iloc[:,16])>0):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   df3.loc[count1,'filenames'] = filename3\n",
        "   df3.loc[count1,'class'] = str(i)\n",
        "   count1 = count1 + 1\n",
        "   XData3test = pd.concat([XData3test,data3],ignore_index=True,axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################\n",
        "fnames = glob.glob('/content/sample_data/TimeS2/48_1/*.csv')\n",
        "\n",
        "\n",
        "\n",
        "fnames2 = []\n",
        "i = 0\n",
        "for f in fnames:\n",
        " fnames2.append(os.path.splitext(os.path.split(f)[-1])[-2])\n",
        " i = i + 1\n",
        "\n",
        "operation_condition = [int(s) for s in fnames2 if s.isdigit()]\n",
        "operation_condition = sorted(operation_condition)\n",
        "#operation_condition = list(range(46, 91)) + list(range(99, 114))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data3 = pd.read_excel('/content/sample_data/DLdata/dataLMS/RunTable.xlsx')\n",
        "path1 ='/content/sample_data/TimeS2/48_1/TimeS/LMS/'\n",
        "path2 ='/content/sample_data/TimeS2/48_1/TimeS/RMS/'\n",
        "path3 ='/content/sample_data/TimeS2/48_1/TimeS/D/'\n",
        "path4 ='/content/sample_data/TimeS2/48_1/'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i  in operation_condition: #len(fnames2)\n",
        "\n",
        "  filename3 = os.path.join(path4,str(i)+'.csv')\n",
        "  data3 = pd.read_csv(filename3,header = None)\n",
        "  if(min(data3.iloc[:,0]-data3.iloc[:,16])>0):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    df3.loc[count1,'filenames'] = filename3\n",
        "    df3.loc[count1,'class'] = str(i)\n",
        "    count1 = count1 + 1\n",
        "    XData3test = pd.concat([XData3test,data3],ignore_index=True,axis=0)\n",
        "\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "YData = XData3.iloc[:,0]-XData3.iloc[:,16]\n",
        "YDatatest = XData3test.iloc[:,0]-XData3test.iloc[:,16]\n",
        "YDataval = XData3val.iloc[:,0]-XData3val.iloc[:,16]\n",
        "\n",
        "X_Trainnum = XData3.iloc[:,16]\n",
        "X_valnum = XData3val.iloc[:,16]\n",
        "X_Testnum = XData3test.iloc[:,16]\n",
        "\n",
        "###############################################################################\n",
        "###############################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImzeMAdxqcR3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "num_samples = len(df1)\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "\n",
        "      # Initialize empty arrays to store batch data\n",
        "batch_LMS = pd.DataFrame()\n",
        "batch_RMS = pd.DataFrame()\n",
        "batch_CD = pd.DataFrame()\n",
        "\n",
        "for idx in indices:\n",
        "          # Load and preprocess the images\n",
        "          fpath1 = os.path.split(df1.iloc[idx]['filenames'])[-2] + '/TimeS/LMS/' + df1.iloc[idx]['class'] + '.csv'\n",
        "          fpath2 = os.path.split(df1.iloc[idx]['filenames'])[-2] + '/TimeS/RMS/' + df1.iloc[idx]['class'] + '.csv'\n",
        "          fpath3 = os.path.split(df1.iloc[idx]['filenames'])[-2] + '/TimeS/D/' + df1.iloc[idx]['class'] + '.csv'\n",
        "\n",
        "          data1 = pd.read_csv(fpath1,header=None)\n",
        "          data2 = pd.read_csv(fpath2,header=None)\n",
        "          data3 = pd.read_csv(fpath3,header=None)/100\n",
        "        #  data1 = data1/np.max(data1,axis = 0)\n",
        "        #  data2 = data2/np.max(data2,axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          batch_LMS = pd.concat([batch_LMS,data1],ignore_index=True,axis = 1)\n",
        "          batch_RMS = pd.concat([batch_RMS,data2],axis = 1)\n",
        "          batch_CD = pd.concat([batch_CD,data3],axis = 1)\n",
        "\n",
        "batch_LMS = np.array(batch_LMS.T)\n",
        "batch_RMS = np.array(batch_RMS.T)\n",
        "batch_CD = np.array(batch_CD.T)\n",
        "\n",
        "X_Train = np.stack((batch_LMS, batch_RMS, batch_CD), axis=2)\n",
        "\n",
        "num_samples = len(df2)\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "\n",
        "      # Initialize empty arrays to store batch data\n",
        "batch_LMS = pd.DataFrame()\n",
        "batch_RMS = pd.DataFrame()\n",
        "batch_CD = pd.DataFrame()\n",
        "\n",
        "for idx in indices:\n",
        "          # Load and preprocess the images\n",
        "          fpath1 = os.path.split(df2.iloc[idx]['filenames'])[-2] + '/TimeS/LMS/' + df2.iloc[idx]['class'] + '.csv'\n",
        "          fpath2 = os.path.split(df2.iloc[idx]['filenames'])[-2] + '/TimeS/RMS/' + df2.iloc[idx]['class'] + '.csv'\n",
        "          fpath3 = os.path.split(df2.iloc[idx]['filenames'])[-2] + '/TimeS/D/' + df2.iloc[idx]['class'] + '.csv'\n",
        "\n",
        "          data1 = pd.read_csv(fpath1,header=None)\n",
        "          data2 = pd.read_csv(fpath2,header=None)\n",
        "          data3 = pd.read_csv(fpath3,header=None)/100\n",
        "          #data1 = data1/np.max(data1,axis = 0)\n",
        "          #data2 = data2/np.max(data2,axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          batch_LMS = pd.concat([batch_LMS,data1],ignore_index=True,axis = 1)\n",
        "          batch_RMS = pd.concat([batch_RMS,data2],axis = 1)\n",
        "          batch_CD = pd.concat([batch_CD,data3],axis = 1)\n",
        "\n",
        "batch_LMS = np.array(batch_LMS.T)\n",
        "batch_RMS = np.array(batch_RMS.T)\n",
        "batch_CD = np.array(batch_CD.T)\n",
        "\n",
        "X_val = np.stack((batch_LMS, batch_RMS, batch_CD), axis=2)\n",
        "\n",
        "num_samples = len(df3)\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "\n",
        "      # Initialize empty arrays to store batch data\n",
        "batch_LMS = pd.DataFrame()\n",
        "batch_RMS = pd.DataFrame()\n",
        "batch_CD = pd.DataFrame()\n",
        "\n",
        "for idx in indices:\n",
        "          # Load and preprocess the images\n",
        "          fpath1 = os.path.split(df3.iloc[idx]['filenames'])[-2] + '/TimeS/LMS/' + df3.iloc[idx]['class'] + '.csv'\n",
        "          fpath2 = os.path.split(df3.iloc[idx]['filenames'])[-2] + '/TimeS/RMS/' + df3.iloc[idx]['class'] + '.csv'\n",
        "          fpath3 = os.path.split(df3.iloc[idx]['filenames'])[-2] + '/TimeS/D/' + df3.iloc[idx]['class'] + '.csv'\n",
        "\n",
        "          data1 = pd.read_csv(fpath1,header=None)\n",
        "          data2 = pd.read_csv(fpath2,header=None)\n",
        "          data3 = pd.read_csv(fpath3,header=None)/100\n",
        "         # data1 = data1/np.max(data1,axis = 0)\n",
        "          #data2 = data2/np.max(data2,axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          batch_LMS = pd.concat([batch_LMS,data1],ignore_index=True,axis = 1)\n",
        "          batch_RMS = pd.concat([batch_RMS,data2],axis = 1)\n",
        "          batch_CD = pd.concat([batch_CD,data3],axis = 1)\n",
        "\n",
        "batch_LMS = np.array(batch_LMS.T)\n",
        "batch_RMS = np.array(batch_RMS.T)\n",
        "batch_CD = np.array(batch_CD.T)\n",
        "\n",
        "X_Test = np.stack((batch_LMS, batch_RMS, batch_CD), axis=2)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Assuming your original matrix is named 'original_matrix'\n",
        "# and it has dimensions n x 1510 x 3\n",
        "\n",
        "# Create a sample matrix with dimensions n x 1510 x 3\n",
        "def interp_data(data,new_columns):\n",
        "\n",
        "\n",
        "  original_matrix = data\n",
        "\n",
        "\n",
        "# Create linear interpolation functions for each channel (3 channels)\n",
        "  interpolated_channels = [interp1d(np.arange(data.shape[1]), original_matrix[:, :, i], kind='linear', axis=1, fill_value=\"extrapolate\") for i in range(3)]\n",
        "\n",
        "# Define the new number of columns (500)\n",
        "\n",
        "# Interpolate each channel to get the new matrix with dimensions n x 500 x 3\n",
        "\n",
        "  interpolated_matrix = np.stack([interp(np.linspace(0, 1509, new_columns)) for interp in interpolated_channels], axis=-1)\n",
        "  return interpolated_matrix\n",
        "# 'interpolated_matrix' now has dimensions n x 500 x 3\n",
        "\n",
        "\n",
        "\n",
        "X_Train2 = X_Train[:,0:1000,:]\n",
        "X_Test2 = X_Test[:,0:1000,:]\n",
        "X_val2 = X_val[:,0:1000,:]\n",
        "\n",
        "\n",
        "X_Train3 = interp_data(X_Train2,500)\n",
        "X_Test3 = interp_data(X_Test2,500)\n",
        "X_val3 = interp_data(X_val2,500)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(X_Train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N60bbdS8NscL",
        "outputId": "fa68c4ee-487a-42e3-c0a6-c8b30c2e2827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18721, 1510, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxnA502iqyPy"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "\n",
        "# 1D ResNet Block\n",
        "class ResidualBlock1D(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='same'):\n",
        "        super(ResidualBlock1D, self).__init__()\n",
        "\n",
        "        self.conv1 = layers.Conv1D(filters, kernel_size, strides=strides, padding=padding)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.relu = layers.ReLU()\n",
        "\n",
        "        self.conv2 = layers.Conv1D(filters, kernel_size, strides=strides, padding=padding)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        self.shortcut = layers.Conv1D(filters, kernel_size=1, strides=strides, padding='same')\n",
        "        self.shortcut_bn = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        shortcut = self.shortcut(inputs)\n",
        "        shortcut = self.shortcut_bn(shortcut, training=training)\n",
        "\n",
        "        x += shortcut\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 1D ResNet Model\n",
        "class ResNet1D(tf.keras.Model):\n",
        "    def __init__(self, input_shape, num_blocks=1, filters=64):\n",
        "        super(ResNet1D, self).__init__()\n",
        "\n",
        "        self.in_channels = filters\n",
        "\n",
        "        self.conv1 = layers.Conv1D(self.in_channels, kernel_size=3, strides=2, padding='same', input_shape=input_shape)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.relu = layers.ReLU()\n",
        "\n",
        "        self.res_blocks = [ResidualBlock1D(filters, kernel_size=3) for _ in range(num_blocks)]\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Numeric Input Model\n",
        "class NumericModel(tf.keras.Model):\n",
        "    def __init__(self, num_numeric_features):\n",
        "        super(NumericModel, self).__init__()\n",
        "\n",
        "        self.dense2 = layers.Dense(2, activation='relu')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.dense2(inputs)\n",
        "        return x\n",
        "\n",
        "# Combined Model with LSTM\n",
        "class CombinedModelWithLSTM(tf.keras.Model):\n",
        "    def __init__(self, input_shape_time_series, num_numeric_features):\n",
        "        super(CombinedModelWithLSTM, self).__init__()\n",
        "\n",
        "        self.resnet = ResNet1D(input_shape_time_series)\n",
        "        self.numeric_model = NumericModel(num_numeric_features)\n",
        "\n",
        "        # Adjust the number of units in the dense layer to match the ResNet output\n",
        "\n",
        "        # Concatenate the output of the ResNet and Numeric branches\n",
        "        self.lstm = layers.Bidirectional(layers.LSTM(2))  # You can adjust the number of LSTM units\n",
        "        self.concat = layers.Concatenate()\n",
        "        self.dense = layers.Dense(2, activation='relu')  # For regression\n",
        "\n",
        "\n",
        "        self.output_layer = layers.Dense(1, activation='linear')  # For regression\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        time_series_data, numeric_data = inputs\n",
        "\n",
        "        # Process time series data through ResNet branch\n",
        "        x_time_series = self.resnet(time_series_data, training=training)\n",
        "\n",
        "        # Process numeric data through Numeric branch\n",
        "        x_numeric = self.numeric_model(numeric_data, training=training)\n",
        "\n",
        "\n",
        "        # Process the  features through LSTM layer\n",
        "        x = self.lstm(x_time_series)  # LSTM expects 3D input, add an extra dimension\n",
        "\n",
        "        # Concatenate the outputs of both branches\n",
        "        x = self.concat([x, x_numeric])\n",
        "\n",
        "        x = self.dense(x)\n",
        "\n",
        "\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(x)\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t0dyNoyKq5oL",
        "outputId": "10bf0f49-3b58-4b94-8f0f-086945ece93d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8174 - mean_absolute_error: 0.5215 - val_loss: 4.8875 - val_mean_absolute_error: 1.0760\n",
            "Epoch 2832/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6869 - mean_absolute_error: 0.4836\n",
            "Epoch 2832: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6879 - mean_absolute_error: 0.4841 - val_loss: 4.9013 - val_mean_absolute_error: 1.1482\n",
            "Epoch 2833/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6709 - mean_absolute_error: 0.4824\n",
            "Epoch 2833: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6713 - mean_absolute_error: 0.4826 - val_loss: 5.0273 - val_mean_absolute_error: 1.0459\n",
            "Epoch 2834/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.7117 - mean_absolute_error: 0.4967\n",
            "Epoch 2834: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7117 - mean_absolute_error: 0.4967 - val_loss: 5.0550 - val_mean_absolute_error: 1.0322\n",
            "Epoch 2835/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6803 - mean_absolute_error: 0.4793\n",
            "Epoch 2835: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6820 - mean_absolute_error: 0.4796 - val_loss: 4.7191 - val_mean_absolute_error: 1.0103\n",
            "Epoch 2836/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.7737 - mean_absolute_error: 0.6874\n",
            "Epoch 2836: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.7729 - mean_absolute_error: 0.6874 - val_loss: 5.7929 - val_mean_absolute_error: 1.0912\n",
            "Epoch 2837/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8635 - mean_absolute_error: 0.5236\n",
            "Epoch 2837: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8633 - mean_absolute_error: 0.5237 - val_loss: 5.7064 - val_mean_absolute_error: 1.0611\n",
            "Epoch 2838/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6816 - mean_absolute_error: 0.4740\n",
            "Epoch 2838: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6817 - mean_absolute_error: 0.4739 - val_loss: 4.6897 - val_mean_absolute_error: 0.9909\n",
            "Epoch 2839/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6857 - mean_absolute_error: 0.4804\n",
            "Epoch 2839: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6861 - mean_absolute_error: 0.4805 - val_loss: 4.9295 - val_mean_absolute_error: 1.0203\n",
            "Epoch 2840/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6718 - mean_absolute_error: 0.4734\n",
            "Epoch 2840: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6754 - mean_absolute_error: 0.4741 - val_loss: 5.1567 - val_mean_absolute_error: 1.0337\n",
            "Epoch 2841/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7404 - mean_absolute_error: 0.4935\n",
            "Epoch 2841: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7405 - mean_absolute_error: 0.4936 - val_loss: 5.0946 - val_mean_absolute_error: 1.0360\n",
            "Epoch 2842/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6907 - mean_absolute_error: 0.4791\n",
            "Epoch 2842: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6928 - mean_absolute_error: 0.4800 - val_loss: 4.6955 - val_mean_absolute_error: 1.0135\n",
            "Epoch 2843/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6619 - mean_absolute_error: 0.4767\n",
            "Epoch 2843: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6615 - mean_absolute_error: 0.4766 - val_loss: 4.7965 - val_mean_absolute_error: 1.0146\n",
            "Epoch 2844/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6359 - mean_absolute_error: 0.4679\n",
            "Epoch 2844: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6359 - mean_absolute_error: 0.4680 - val_loss: 5.0976 - val_mean_absolute_error: 1.0442\n",
            "Epoch 2845/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7070 - mean_absolute_error: 0.4862\n",
            "Epoch 2845: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7070 - mean_absolute_error: 0.4863 - val_loss: 5.2754 - val_mean_absolute_error: 1.0701\n",
            "Epoch 2846/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6820 - mean_absolute_error: 0.4822\n",
            "Epoch 2846: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6820 - mean_absolute_error: 0.4825 - val_loss: 5.3496 - val_mean_absolute_error: 1.1404\n",
            "Epoch 2847/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8129 - mean_absolute_error: 0.5139\n",
            "Epoch 2847: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8130 - mean_absolute_error: 0.5140 - val_loss: 5.3353 - val_mean_absolute_error: 1.0613\n",
            "Epoch 2848/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7624 - mean_absolute_error: 0.5111\n",
            "Epoch 2848: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7618 - mean_absolute_error: 0.5109 - val_loss: 5.4438 - val_mean_absolute_error: 1.1240\n",
            "Epoch 2849/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7091 - mean_absolute_error: 0.4816\n",
            "Epoch 2849: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7099 - mean_absolute_error: 0.4817 - val_loss: 4.9423 - val_mean_absolute_error: 1.0194\n",
            "Epoch 2850/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7306 - mean_absolute_error: 0.4914\n",
            "Epoch 2850: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7297 - mean_absolute_error: 0.4911 - val_loss: 5.4320 - val_mean_absolute_error: 1.0988\n",
            "Epoch 2851/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7185 - mean_absolute_error: 0.4946\n",
            "Epoch 2851: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7181 - mean_absolute_error: 0.4946 - val_loss: 5.2384 - val_mean_absolute_error: 1.0671\n",
            "Epoch 2852/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7168 - mean_absolute_error: 0.4948\n",
            "Epoch 2852: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7165 - mean_absolute_error: 0.4948 - val_loss: 5.2175 - val_mean_absolute_error: 1.0630\n",
            "Epoch 2853/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7242 - mean_absolute_error: 0.4937\n",
            "Epoch 2853: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7243 - mean_absolute_error: 0.4940 - val_loss: 5.1797 - val_mean_absolute_error: 1.0802\n",
            "Epoch 2854/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6729 - mean_absolute_error: 0.4799\n",
            "Epoch 2854: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6728 - mean_absolute_error: 0.4800 - val_loss: 4.9799 - val_mean_absolute_error: 1.0384\n",
            "Epoch 2855/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7841 - mean_absolute_error: 0.5191\n",
            "Epoch 2855: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7837 - mean_absolute_error: 0.5190 - val_loss: 4.7435 - val_mean_absolute_error: 1.0418\n",
            "Epoch 2856/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7437 - mean_absolute_error: 0.4980\n",
            "Epoch 2856: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7437 - mean_absolute_error: 0.4980 - val_loss: 5.1389 - val_mean_absolute_error: 1.0382\n",
            "Epoch 2857/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7411 - mean_absolute_error: 0.4925\n",
            "Epoch 2857: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7395 - mean_absolute_error: 0.4921 - val_loss: 4.7262 - val_mean_absolute_error: 0.9850\n",
            "Epoch 2858/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7838 - mean_absolute_error: 0.5061\n",
            "Epoch 2858: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7835 - mean_absolute_error: 0.5060 - val_loss: 5.0749 - val_mean_absolute_error: 1.0553\n",
            "Epoch 2859/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1495 - mean_absolute_error: 0.5834\n",
            "Epoch 2859: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1583 - mean_absolute_error: 0.5844 - val_loss: 6.3858 - val_mean_absolute_error: 1.3403\n",
            "Epoch 2860/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 1.2351 - mean_absolute_error: 0.5930\n",
            "Epoch 2860: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.2346 - mean_absolute_error: 0.5926 - val_loss: 5.2482 - val_mean_absolute_error: 1.0593\n",
            "Epoch 2861/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7518 - mean_absolute_error: 0.5014\n",
            "Epoch 2861: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7511 - mean_absolute_error: 0.5012 - val_loss: 5.1980 - val_mean_absolute_error: 1.0341\n",
            "Epoch 2862/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6828 - mean_absolute_error: 0.4850\n",
            "Epoch 2862: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6887 - mean_absolute_error: 0.4858 - val_loss: 4.7704 - val_mean_absolute_error: 1.0213\n",
            "Epoch 2863/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9239 - mean_absolute_error: 0.5385\n",
            "Epoch 2863: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9238 - mean_absolute_error: 0.5386 - val_loss: 4.7542 - val_mean_absolute_error: 1.0076\n",
            "Epoch 2864/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6740 - mean_absolute_error: 0.4783\n",
            "Epoch 2864: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 5s 24ms/step - loss: 0.6748 - mean_absolute_error: 0.4788 - val_loss: 4.6221 - val_mean_absolute_error: 1.0221\n",
            "Epoch 2865/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6420 - mean_absolute_error: 0.4750\n",
            "Epoch 2865: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6416 - mean_absolute_error: 0.4748 - val_loss: 5.0360 - val_mean_absolute_error: 1.0397\n",
            "Epoch 2866/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6783 - mean_absolute_error: 0.4783\n",
            "Epoch 2866: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6790 - mean_absolute_error: 0.4787 - val_loss: 5.6610 - val_mean_absolute_error: 1.0901\n",
            "Epoch 2867/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1335 - mean_absolute_error: 0.5971\n",
            "Epoch 2867: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.1337 - mean_absolute_error: 0.5972 - val_loss: 4.7082 - val_mean_absolute_error: 1.0534\n",
            "Epoch 2868/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1168 - mean_absolute_error: 0.5806\n",
            "Epoch 2868: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1177 - mean_absolute_error: 0.5808 - val_loss: 5.7810 - val_mean_absolute_error: 1.1067\n",
            "Epoch 2869/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1012 - mean_absolute_error: 0.5674\n",
            "Epoch 2869: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1015 - mean_absolute_error: 0.5677 - val_loss: 5.2246 - val_mean_absolute_error: 1.0713\n",
            "Epoch 2870/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8069 - mean_absolute_error: 0.5128\n",
            "Epoch 2870: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.8065 - mean_absolute_error: 0.5127 - val_loss: 5.3406 - val_mean_absolute_error: 1.0614\n",
            "Epoch 2871/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7748 - mean_absolute_error: 0.4958\n",
            "Epoch 2871: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7747 - mean_absolute_error: 0.4960 - val_loss: 6.0702 - val_mean_absolute_error: 1.2801\n",
            "Epoch 2872/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8498 - mean_absolute_error: 0.5210\n",
            "Epoch 2872: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8496 - mean_absolute_error: 0.5212 - val_loss: 5.2345 - val_mean_absolute_error: 1.1344\n",
            "Epoch 2873/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7610 - mean_absolute_error: 0.5055\n",
            "Epoch 2873: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7641 - mean_absolute_error: 0.5054 - val_loss: 5.0477 - val_mean_absolute_error: 1.0231\n",
            "Epoch 2874/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6316 - mean_absolute_error: 0.4617\n",
            "Epoch 2874: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6318 - mean_absolute_error: 0.4618 - val_loss: 4.7865 - val_mean_absolute_error: 1.0439\n",
            "Epoch 2875/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6193 - mean_absolute_error: 0.4589\n",
            "Epoch 2875: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6215 - mean_absolute_error: 0.4592 - val_loss: 4.6449 - val_mean_absolute_error: 1.0004\n",
            "Epoch 2876/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7156 - mean_absolute_error: 0.4871\n",
            "Epoch 2876: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7185 - mean_absolute_error: 0.4875 - val_loss: 5.0573 - val_mean_absolute_error: 1.0323\n",
            "Epoch 2877/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.7144 - mean_absolute_error: 0.4960\n",
            "Epoch 2877: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7144 - mean_absolute_error: 0.4960 - val_loss: 5.4579 - val_mean_absolute_error: 1.0738\n",
            "Epoch 2878/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6422 - mean_absolute_error: 0.4685\n",
            "Epoch 2878: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6423 - mean_absolute_error: 0.4686 - val_loss: 5.1424 - val_mean_absolute_error: 1.0787\n",
            "Epoch 2879/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6550 - mean_absolute_error: 0.4775\n",
            "Epoch 2879: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6552 - mean_absolute_error: 0.4778 - val_loss: 5.6460 - val_mean_absolute_error: 1.0862\n",
            "Epoch 2880/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6079 - mean_absolute_error: 0.4619\n",
            "Epoch 2880: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6075 - mean_absolute_error: 0.4617 - val_loss: 4.9658 - val_mean_absolute_error: 1.0318\n",
            "Epoch 2881/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6681 - mean_absolute_error: 0.4766\n",
            "Epoch 2881: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6685 - mean_absolute_error: 0.4770 - val_loss: 4.8725 - val_mean_absolute_error: 0.9944\n",
            "Epoch 2882/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.5683 - mean_absolute_error: 0.6339\n",
            "Epoch 2882: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.5667 - mean_absolute_error: 0.6335 - val_loss: 5.1069 - val_mean_absolute_error: 1.0219\n",
            "Epoch 2883/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7890 - mean_absolute_error: 0.5006\n",
            "Epoch 2883: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7889 - mean_absolute_error: 0.5005 - val_loss: 5.0905 - val_mean_absolute_error: 1.0342\n",
            "Epoch 2884/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6744 - mean_absolute_error: 0.4763\n",
            "Epoch 2884: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6747 - mean_absolute_error: 0.4765 - val_loss: 5.3003 - val_mean_absolute_error: 1.0165\n",
            "Epoch 2885/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6475 - mean_absolute_error: 0.4696\n",
            "Epoch 2885: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6483 - mean_absolute_error: 0.4702 - val_loss: 4.8267 - val_mean_absolute_error: 0.9918\n",
            "Epoch 2886/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6750 - mean_absolute_error: 0.4774\n",
            "Epoch 2886: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6755 - mean_absolute_error: 0.4778 - val_loss: 5.0811 - val_mean_absolute_error: 1.0539\n",
            "Epoch 2887/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6425 - mean_absolute_error: 0.4702\n",
            "Epoch 2887: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6422 - mean_absolute_error: 0.4701 - val_loss: 4.8705 - val_mean_absolute_error: 1.0394\n",
            "Epoch 2888/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6603 - mean_absolute_error: 0.4695\n",
            "Epoch 2888: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6618 - mean_absolute_error: 0.4697 - val_loss: 4.8599 - val_mean_absolute_error: 1.0391\n",
            "Epoch 2889/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7059 - mean_absolute_error: 0.4872\n",
            "Epoch 2889: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7055 - mean_absolute_error: 0.4872 - val_loss: 5.2030 - val_mean_absolute_error: 1.1222\n",
            "Epoch 2890/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7082 - mean_absolute_error: 0.4870\n",
            "Epoch 2890: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7105 - mean_absolute_error: 0.4874 - val_loss: 5.1676 - val_mean_absolute_error: 1.0377\n",
            "Epoch 2891/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6663 - mean_absolute_error: 0.4728\n",
            "Epoch 2891: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6729 - mean_absolute_error: 0.4741 - val_loss: 5.0160 - val_mean_absolute_error: 1.0444\n",
            "Epoch 2892/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9153 - mean_absolute_error: 0.5342\n",
            "Epoch 2892: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9152 - mean_absolute_error: 0.5345 - val_loss: 5.0176 - val_mean_absolute_error: 0.9985\n",
            "Epoch 2893/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7067 - mean_absolute_error: 0.4907\n",
            "Epoch 2893: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7082 - mean_absolute_error: 0.4911 - val_loss: 5.7549 - val_mean_absolute_error: 1.1151\n",
            "Epoch 2894/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8016 - mean_absolute_error: 0.5141\n",
            "Epoch 2894: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8014 - mean_absolute_error: 0.5139 - val_loss: 5.0326 - val_mean_absolute_error: 1.0543\n",
            "Epoch 2895/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6910 - mean_absolute_error: 0.4844\n",
            "Epoch 2895: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6953 - mean_absolute_error: 0.4853 - val_loss: 5.3003 - val_mean_absolute_error: 1.0633\n",
            "Epoch 2896/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8756 - mean_absolute_error: 0.5272\n",
            "Epoch 2896: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8769 - mean_absolute_error: 0.5275 - val_loss: 4.7846 - val_mean_absolute_error: 0.9979\n",
            "Epoch 2897/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6655 - mean_absolute_error: 0.4801\n",
            "Epoch 2897: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6658 - mean_absolute_error: 0.4803 - val_loss: 4.8042 - val_mean_absolute_error: 0.9971\n",
            "Epoch 2898/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6454 - mean_absolute_error: 0.4725\n",
            "Epoch 2898: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6528 - mean_absolute_error: 0.4733 - val_loss: 5.2691 - val_mean_absolute_error: 1.0728\n",
            "Epoch 2899/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8872 - mean_absolute_error: 0.5331\n",
            "Epoch 2899: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8864 - mean_absolute_error: 0.5328 - val_loss: 5.1361 - val_mean_absolute_error: 1.0170\n",
            "Epoch 2900/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.6880 - mean_absolute_error: 0.6306\n",
            "Epoch 2900: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.6883 - mean_absolute_error: 0.6307 - val_loss: 6.3854 - val_mean_absolute_error: 1.3479\n",
            "Epoch 2901/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.6721 - mean_absolute_error: 0.6433\n",
            "Epoch 2901: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.6743 - mean_absolute_error: 0.6437 - val_loss: 5.1081 - val_mean_absolute_error: 1.0588\n",
            "Epoch 2902/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9812 - mean_absolute_error: 0.5384\n",
            "Epoch 2902: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9823 - mean_absolute_error: 0.5389 - val_loss: 5.2108 - val_mean_absolute_error: 1.0687\n",
            "Epoch 2903/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7696 - mean_absolute_error: 0.4934\n",
            "Epoch 2903: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7748 - mean_absolute_error: 0.4944 - val_loss: 4.7790 - val_mean_absolute_error: 1.0042\n",
            "Epoch 2904/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7568 - mean_absolute_error: 0.4953\n",
            "Epoch 2904: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7565 - mean_absolute_error: 0.4953 - val_loss: 4.9663 - val_mean_absolute_error: 1.0368\n",
            "Epoch 2905/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6836 - mean_absolute_error: 0.4779\n",
            "Epoch 2905: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6834 - mean_absolute_error: 0.4779 - val_loss: 4.8750 - val_mean_absolute_error: 1.0288\n",
            "Epoch 2906/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6639 - mean_absolute_error: 0.4740\n",
            "Epoch 2906: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6649 - mean_absolute_error: 0.4744 - val_loss: 4.7636 - val_mean_absolute_error: 1.0121\n",
            "Epoch 2907/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.6727 - mean_absolute_error: 0.4704\n",
            "Epoch 2907: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6727 - mean_absolute_error: 0.4704 - val_loss: 4.8512 - val_mean_absolute_error: 1.0101\n",
            "Epoch 2908/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6521 - mean_absolute_error: 0.4778\n",
            "Epoch 2908: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6520 - mean_absolute_error: 0.4778 - val_loss: 4.7845 - val_mean_absolute_error: 1.0277\n",
            "Epoch 2909/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6129 - mean_absolute_error: 0.4630\n",
            "Epoch 2909: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6140 - mean_absolute_error: 0.4634 - val_loss: 5.2846 - val_mean_absolute_error: 1.1010\n",
            "Epoch 2910/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6408 - mean_absolute_error: 0.4712\n",
            "Epoch 2910: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6411 - mean_absolute_error: 0.4715 - val_loss: 5.1374 - val_mean_absolute_error: 1.0203\n",
            "Epoch 2911/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6918 - mean_absolute_error: 0.4833\n",
            "Epoch 2911: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6948 - mean_absolute_error: 0.4839 - val_loss: 5.0652 - val_mean_absolute_error: 1.0385\n",
            "Epoch 2912/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6561 - mean_absolute_error: 0.4778\n",
            "Epoch 2912: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6566 - mean_absolute_error: 0.4779 - val_loss: 5.3456 - val_mean_absolute_error: 1.0871\n",
            "Epoch 2913/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7323 - mean_absolute_error: 0.5066\n",
            "Epoch 2913: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7349 - mean_absolute_error: 0.5076 - val_loss: 4.8801 - val_mean_absolute_error: 1.0666\n",
            "Epoch 2914/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7656 - mean_absolute_error: 0.5012\n",
            "Epoch 2914: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7651 - mean_absolute_error: 0.5011 - val_loss: 4.7876 - val_mean_absolute_error: 1.0547\n",
            "Epoch 2915/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6768 - mean_absolute_error: 0.4788\n",
            "Epoch 2915: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6770 - mean_absolute_error: 0.4792 - val_loss: 5.2146 - val_mean_absolute_error: 1.0715\n",
            "Epoch 2916/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6616 - mean_absolute_error: 0.4732\n",
            "Epoch 2916: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6616 - mean_absolute_error: 0.4734 - val_loss: 5.5700 - val_mean_absolute_error: 1.0902\n",
            "Epoch 2917/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7189 - mean_absolute_error: 0.4849\n",
            "Epoch 2917: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7199 - mean_absolute_error: 0.4851 - val_loss: 4.9715 - val_mean_absolute_error: 1.0120\n",
            "Epoch 2918/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6774 - mean_absolute_error: 0.4828\n",
            "Epoch 2918: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6823 - mean_absolute_error: 0.4837 - val_loss: 5.0979 - val_mean_absolute_error: 1.0349\n",
            "Epoch 2919/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2987 - mean_absolute_error: 0.6154\n",
            "Epoch 2919: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2984 - mean_absolute_error: 0.6154 - val_loss: 5.3161 - val_mean_absolute_error: 1.0679\n",
            "Epoch 2920/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.7766 - mean_absolute_error: 0.5030\n",
            "Epoch 2920: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7766 - mean_absolute_error: 0.5030 - val_loss: 5.5395 - val_mean_absolute_error: 1.0673\n",
            "Epoch 2921/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6413 - mean_absolute_error: 0.4719\n",
            "Epoch 2921: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6410 - mean_absolute_error: 0.4718 - val_loss: 5.4527 - val_mean_absolute_error: 1.0890\n",
            "Epoch 2922/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6402 - mean_absolute_error: 0.4636\n",
            "Epoch 2922: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6425 - mean_absolute_error: 0.4644 - val_loss: 5.6387 - val_mean_absolute_error: 1.1619\n",
            "Epoch 2923/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2383 - mean_absolute_error: 0.6044\n",
            "Epoch 2923: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2379 - mean_absolute_error: 0.6046 - val_loss: 5.1501 - val_mean_absolute_error: 1.0419\n",
            "Epoch 2924/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7015 - mean_absolute_error: 0.4841\n",
            "Epoch 2924: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7011 - mean_absolute_error: 0.4841 - val_loss: 5.0416 - val_mean_absolute_error: 1.0454\n",
            "Epoch 2925/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6564 - mean_absolute_error: 0.4741\n",
            "Epoch 2925: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6564 - mean_absolute_error: 0.4743 - val_loss: 5.1245 - val_mean_absolute_error: 1.0284\n",
            "Epoch 2926/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6770 - mean_absolute_error: 0.4771\n",
            "Epoch 2926: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6782 - mean_absolute_error: 0.4773 - val_loss: 4.9241 - val_mean_absolute_error: 1.0280\n",
            "Epoch 2927/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7479 - mean_absolute_error: 0.4940\n",
            "Epoch 2927: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7475 - mean_absolute_error: 0.4940 - val_loss: 5.0572 - val_mean_absolute_error: 1.0416\n",
            "Epoch 2928/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6442 - mean_absolute_error: 0.4727\n",
            "Epoch 2928: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6454 - mean_absolute_error: 0.4730 - val_loss: 4.7156 - val_mean_absolute_error: 1.0110\n",
            "Epoch 2929/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6699 - mean_absolute_error: 0.4779\n",
            "Epoch 2929: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6740 - mean_absolute_error: 0.4787 - val_loss: 5.8687 - val_mean_absolute_error: 1.1551\n",
            "Epoch 2930/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.9881 - mean_absolute_error: 0.7534\n",
            "Epoch 2930: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.9864 - mean_absolute_error: 0.7531 - val_loss: 4.9927 - val_mean_absolute_error: 1.0289\n",
            "Epoch 2931/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8669 - mean_absolute_error: 0.5379\n",
            "Epoch 2931: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8668 - mean_absolute_error: 0.5379 - val_loss: 4.6910 - val_mean_absolute_error: 1.0065\n",
            "Epoch 2932/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6837 - mean_absolute_error: 0.4854\n",
            "Epoch 2932: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6875 - mean_absolute_error: 0.4857 - val_loss: 5.0111 - val_mean_absolute_error: 1.0295\n",
            "Epoch 2933/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6817 - mean_absolute_error: 0.4836\n",
            "Epoch 2933: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6817 - mean_absolute_error: 0.4838 - val_loss: 4.8161 - val_mean_absolute_error: 1.0203\n",
            "Epoch 2934/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6517 - mean_absolute_error: 0.4716\n",
            "Epoch 2934: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6530 - mean_absolute_error: 0.4720 - val_loss: 4.7655 - val_mean_absolute_error: 1.0476\n",
            "Epoch 2935/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7249 - mean_absolute_error: 0.4884\n",
            "Epoch 2935: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7259 - mean_absolute_error: 0.4888 - val_loss: 4.9704 - val_mean_absolute_error: 1.0818\n",
            "Epoch 2936/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6705 - mean_absolute_error: 0.4798\n",
            "Epoch 2936: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6703 - mean_absolute_error: 0.4798 - val_loss: 5.0932 - val_mean_absolute_error: 1.0531\n",
            "Epoch 2937/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6257 - mean_absolute_error: 0.4641\n",
            "Epoch 2937: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6253 - mean_absolute_error: 0.4640 - val_loss: 5.4097 - val_mean_absolute_error: 1.0698\n",
            "Epoch 2938/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6934 - mean_absolute_error: 0.4860\n",
            "Epoch 2938: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6940 - mean_absolute_error: 0.4864 - val_loss: 5.7387 - val_mean_absolute_error: 1.0988\n",
            "Epoch 2939/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6411 - mean_absolute_error: 0.4741\n",
            "Epoch 2939: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6431 - mean_absolute_error: 0.4746 - val_loss: 5.0500 - val_mean_absolute_error: 1.0490\n",
            "Epoch 2940/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7029 - mean_absolute_error: 0.4835\n",
            "Epoch 2940: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7025 - mean_absolute_error: 0.4835 - val_loss: 4.6170 - val_mean_absolute_error: 1.0133\n",
            "Epoch 2941/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6344 - mean_absolute_error: 0.4730\n",
            "Epoch 2941: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6347 - mean_absolute_error: 0.4731 - val_loss: 5.4223 - val_mean_absolute_error: 1.0899\n",
            "Epoch 2942/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6517 - mean_absolute_error: 0.4712\n",
            "Epoch 2942: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6515 - mean_absolute_error: 0.4712 - val_loss: 4.7843 - val_mean_absolute_error: 1.0438\n",
            "Epoch 2943/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.7517 - mean_absolute_error: 0.5046\n",
            "Epoch 2943: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7517 - mean_absolute_error: 0.5046 - val_loss: 5.4192 - val_mean_absolute_error: 1.0847\n",
            "Epoch 2944/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6466 - mean_absolute_error: 0.4781\n",
            "Epoch 2944: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6496 - mean_absolute_error: 0.4788 - val_loss: 5.2429 - val_mean_absolute_error: 1.0577\n",
            "Epoch 2945/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8400 - mean_absolute_error: 0.5248\n",
            "Epoch 2945: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8397 - mean_absolute_error: 0.5248 - val_loss: 5.3926 - val_mean_absolute_error: 1.0494\n",
            "Epoch 2946/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9016 - mean_absolute_error: 0.5349\n",
            "Epoch 2946: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9047 - mean_absolute_error: 0.5356 - val_loss: 5.1423 - val_mean_absolute_error: 1.1088\n",
            "Epoch 2947/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 2.1408 - mean_absolute_error: 0.7408\n",
            "Epoch 2947: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 2.1403 - mean_absolute_error: 0.7409 - val_loss: 5.4990 - val_mean_absolute_error: 1.2449\n",
            "Epoch 2948/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2449 - mean_absolute_error: 0.5889\n",
            "Epoch 2948: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2452 - mean_absolute_error: 0.5892 - val_loss: 5.0203 - val_mean_absolute_error: 1.0401\n",
            "Epoch 2949/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8039 - mean_absolute_error: 0.5070\n",
            "Epoch 2949: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8048 - mean_absolute_error: 0.5073 - val_loss: 5.0817 - val_mean_absolute_error: 1.0208\n",
            "Epoch 2950/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6338 - mean_absolute_error: 0.4625\n",
            "Epoch 2950: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6337 - mean_absolute_error: 0.4626 - val_loss: 4.8686 - val_mean_absolute_error: 1.0215\n",
            "Epoch 2951/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6425 - mean_absolute_error: 0.4617\n",
            "Epoch 2951: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6426 - mean_absolute_error: 0.4617 - val_loss: 4.7920 - val_mean_absolute_error: 0.9930\n",
            "Epoch 2952/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6516 - mean_absolute_error: 0.4713\n",
            "Epoch 2952: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6528 - mean_absolute_error: 0.4715 - val_loss: 4.8174 - val_mean_absolute_error: 1.0761\n",
            "Epoch 2953/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7957 - mean_absolute_error: 0.5110\n",
            "Epoch 2953: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7956 - mean_absolute_error: 0.5112 - val_loss: 5.4191 - val_mean_absolute_error: 1.0803\n",
            "Epoch 2954/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8114 - mean_absolute_error: 0.5130\n",
            "Epoch 2954: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8121 - mean_absolute_error: 0.5133 - val_loss: 5.0430 - val_mean_absolute_error: 1.0763\n",
            "Epoch 2955/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6450 - mean_absolute_error: 0.4699\n",
            "Epoch 2955: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6453 - mean_absolute_error: 0.4701 - val_loss: 5.0109 - val_mean_absolute_error: 1.1007\n",
            "Epoch 2956/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6401 - mean_absolute_error: 0.4675\n",
            "Epoch 2956: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6407 - mean_absolute_error: 0.4678 - val_loss: 5.4818 - val_mean_absolute_error: 1.0907\n",
            "Epoch 2957/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6777 - mean_absolute_error: 0.4786\n",
            "Epoch 2957: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6778 - mean_absolute_error: 0.4786 - val_loss: 5.3755 - val_mean_absolute_error: 1.0742\n",
            "Epoch 2958/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6314 - mean_absolute_error: 0.4666\n",
            "Epoch 2958: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6399 - mean_absolute_error: 0.4674 - val_loss: 5.4453 - val_mean_absolute_error: 1.0994\n",
            "Epoch 2959/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.6336 - mean_absolute_error: 0.6699\n",
            "Epoch 2959: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.6323 - mean_absolute_error: 0.6698 - val_loss: 5.3995 - val_mean_absolute_error: 1.1479\n",
            "Epoch 2960/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0157 - mean_absolute_error: 0.5582\n",
            "Epoch 2960: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0148 - mean_absolute_error: 0.5580 - val_loss: 4.4598 - val_mean_absolute_error: 1.0326\n",
            "Epoch 2961/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6414 - mean_absolute_error: 0.4683\n",
            "Epoch 2961: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6410 - mean_absolute_error: 0.4682 - val_loss: 4.8715 - val_mean_absolute_error: 1.0411\n",
            "Epoch 2962/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6192 - mean_absolute_error: 0.4622\n",
            "Epoch 2962: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6189 - mean_absolute_error: 0.4622 - val_loss: 4.8759 - val_mean_absolute_error: 1.0535\n",
            "Epoch 2963/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6099 - mean_absolute_error: 0.4563\n",
            "Epoch 2963: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6115 - mean_absolute_error: 0.4568 - val_loss: 4.9832 - val_mean_absolute_error: 1.0288\n",
            "Epoch 2964/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6324 - mean_absolute_error: 0.4690\n",
            "Epoch 2964: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6328 - mean_absolute_error: 0.4693 - val_loss: 4.9374 - val_mean_absolute_error: 1.0599\n",
            "Epoch 2965/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6213 - mean_absolute_error: 0.4634\n",
            "Epoch 2965: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6241 - mean_absolute_error: 0.4644 - val_loss: 5.4968 - val_mean_absolute_error: 1.0611\n",
            "Epoch 2966/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6623 - mean_absolute_error: 0.4828\n",
            "Epoch 2966: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6631 - mean_absolute_error: 0.4831 - val_loss: 4.7890 - val_mean_absolute_error: 1.0182\n",
            "Epoch 2967/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7274 - mean_absolute_error: 0.4804\n",
            "Epoch 2967: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7287 - mean_absolute_error: 0.4808 - val_loss: 5.2653 - val_mean_absolute_error: 1.0450\n",
            "Epoch 2968/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7913 - mean_absolute_error: 0.5030\n",
            "Epoch 2968: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7994 - mean_absolute_error: 0.5042 - val_loss: 6.2975 - val_mean_absolute_error: 1.2696\n",
            "Epoch 2969/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1412 - mean_absolute_error: 0.6076\n",
            "Epoch 2969: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1404 - mean_absolute_error: 0.6075 - val_loss: 5.2343 - val_mean_absolute_error: 1.0528\n",
            "Epoch 2970/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6846 - mean_absolute_error: 0.4801\n",
            "Epoch 2970: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6880 - mean_absolute_error: 0.4812 - val_loss: 4.7825 - val_mean_absolute_error: 1.0443\n",
            "Epoch 2971/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6626 - mean_absolute_error: 0.4797\n",
            "Epoch 2971: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6627 - mean_absolute_error: 0.4800 - val_loss: 5.4210 - val_mean_absolute_error: 1.0863\n",
            "Epoch 2972/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6706 - mean_absolute_error: 0.4802\n",
            "Epoch 2972: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6732 - mean_absolute_error: 0.4808 - val_loss: 4.6906 - val_mean_absolute_error: 1.0228\n",
            "Epoch 2973/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6760 - mean_absolute_error: 0.4827\n",
            "Epoch 2973: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6758 - mean_absolute_error: 0.4827 - val_loss: 4.8438 - val_mean_absolute_error: 1.0089\n",
            "Epoch 2974/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6205 - mean_absolute_error: 0.4628\n",
            "Epoch 2974: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6203 - mean_absolute_error: 0.4627 - val_loss: 4.8399 - val_mean_absolute_error: 1.0129\n",
            "Epoch 2975/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6056 - mean_absolute_error: 0.4568\n",
            "Epoch 2975: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6061 - mean_absolute_error: 0.4571 - val_loss: 4.8314 - val_mean_absolute_error: 1.0065\n",
            "Epoch 2976/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9284 - mean_absolute_error: 0.5315\n",
            "Epoch 2976: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9301 - mean_absolute_error: 0.5317 - val_loss: 6.3490 - val_mean_absolute_error: 1.1200\n",
            "Epoch 2977/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7378 - mean_absolute_error: 0.4934\n",
            "Epoch 2977: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7371 - mean_absolute_error: 0.4932 - val_loss: 5.6062 - val_mean_absolute_error: 1.0787\n",
            "Epoch 2978/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8543 - mean_absolute_error: 0.5186\n",
            "Epoch 2978: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8554 - mean_absolute_error: 0.5191 - val_loss: 5.3126 - val_mean_absolute_error: 1.0424\n",
            "Epoch 2979/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6941 - mean_absolute_error: 0.4814\n",
            "Epoch 2979: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6952 - mean_absolute_error: 0.4816 - val_loss: 4.8148 - val_mean_absolute_error: 1.0056\n",
            "Epoch 2980/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7629 - mean_absolute_error: 0.5041\n",
            "Epoch 2980: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7629 - mean_absolute_error: 0.5042 - val_loss: 4.8993 - val_mean_absolute_error: 1.0336\n",
            "Epoch 2981/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6449 - mean_absolute_error: 0.4720\n",
            "Epoch 2981: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6458 - mean_absolute_error: 0.4723 - val_loss: 4.9218 - val_mean_absolute_error: 1.0183\n",
            "Epoch 2982/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6577 - mean_absolute_error: 0.4760\n",
            "Epoch 2982: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6582 - mean_absolute_error: 0.4762 - val_loss: 5.5567 - val_mean_absolute_error: 1.1156\n",
            "Epoch 2983/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6448 - mean_absolute_error: 0.4708\n",
            "Epoch 2983: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6448 - mean_absolute_error: 0.4711 - val_loss: 4.9139 - val_mean_absolute_error: 1.0379\n",
            "Epoch 2984/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6479 - mean_absolute_error: 0.4729\n",
            "Epoch 2984: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6474 - mean_absolute_error: 0.4728 - val_loss: 4.7692 - val_mean_absolute_error: 1.0518\n",
            "Epoch 2985/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6301 - mean_absolute_error: 0.4625\n",
            "Epoch 2985: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6351 - mean_absolute_error: 0.4634 - val_loss: 4.7530 - val_mean_absolute_error: 1.0184\n",
            "Epoch 2986/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8006 - mean_absolute_error: 0.5215\n",
            "Epoch 2986: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8021 - mean_absolute_error: 0.5218 - val_loss: 4.6355 - val_mean_absolute_error: 0.9963\n",
            "Epoch 2987/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6932 - mean_absolute_error: 0.4908\n",
            "Epoch 2987: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6934 - mean_absolute_error: 0.4911 - val_loss: 6.2802 - val_mean_absolute_error: 1.1049\n",
            "Epoch 2988/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6722 - mean_absolute_error: 0.4786\n",
            "Epoch 2988: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6722 - mean_absolute_error: 0.4788 - val_loss: 5.6814 - val_mean_absolute_error: 1.0866\n",
            "Epoch 2989/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7415 - mean_absolute_error: 0.4980\n",
            "Epoch 2989: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7528 - mean_absolute_error: 0.4990 - val_loss: 5.4128 - val_mean_absolute_error: 1.1427\n",
            "Epoch 2990/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.4881 - mean_absolute_error: 0.6603\n",
            "Epoch 2990: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.4879 - mean_absolute_error: 0.6603 - val_loss: 7.4415 - val_mean_absolute_error: 1.3194\n",
            "Epoch 2991/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9606 - mean_absolute_error: 0.5371\n",
            "Epoch 2991: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9604 - mean_absolute_error: 0.5372 - val_loss: 4.8169 - val_mean_absolute_error: 1.0530\n",
            "Epoch 2992/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7425 - mean_absolute_error: 0.4945\n",
            "Epoch 2992: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7421 - mean_absolute_error: 0.4944 - val_loss: 5.0406 - val_mean_absolute_error: 1.1019\n",
            "Epoch 2993/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6449 - mean_absolute_error: 0.4704\n",
            "Epoch 2993: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6490 - mean_absolute_error: 0.4713 - val_loss: 5.1092 - val_mean_absolute_error: 1.1317\n",
            "Epoch 2994/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7645 - mean_absolute_error: 0.5040\n",
            "Epoch 2994: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7640 - mean_absolute_error: 0.5038 - val_loss: 4.8833 - val_mean_absolute_error: 1.0689\n",
            "Epoch 2995/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6290 - mean_absolute_error: 0.4614\n",
            "Epoch 2995: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6291 - mean_absolute_error: 0.4615 - val_loss: 5.1444 - val_mean_absolute_error: 1.0307\n",
            "Epoch 2996/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6230 - mean_absolute_error: 0.4608\n",
            "Epoch 2996: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6295 - mean_absolute_error: 0.4620 - val_loss: 5.0487 - val_mean_absolute_error: 1.0424\n",
            "Epoch 2997/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1139 - mean_absolute_error: 0.5744\n",
            "Epoch 2997: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1162 - mean_absolute_error: 0.5750 - val_loss: 5.3895 - val_mean_absolute_error: 1.0963\n",
            "Epoch 2998/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0130 - mean_absolute_error: 0.5486\n",
            "Epoch 2998: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0178 - mean_absolute_error: 0.5494 - val_loss: 5.1920 - val_mean_absolute_error: 1.0546\n",
            "Epoch 2999/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7301 - mean_absolute_error: 0.4973\n",
            "Epoch 2999: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7304 - mean_absolute_error: 0.4975 - val_loss: 4.9348 - val_mean_absolute_error: 1.0315\n",
            "Epoch 3000/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6116 - mean_absolute_error: 0.4596\n",
            "Epoch 3000: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6125 - mean_absolute_error: 0.4600 - val_loss: 6.5579 - val_mean_absolute_error: 1.1389\n",
            "Epoch 3001/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6617 - mean_absolute_error: 0.4708\n",
            "Epoch 3001: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6623 - mean_absolute_error: 0.4711 - val_loss: 4.9669 - val_mean_absolute_error: 1.0165\n",
            "Epoch 3002/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6509 - mean_absolute_error: 0.4697\n",
            "Epoch 3002: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6539 - mean_absolute_error: 0.4705 - val_loss: 4.9816 - val_mean_absolute_error: 1.0356\n",
            "Epoch 3003/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7012 - mean_absolute_error: 0.4814\n",
            "Epoch 3003: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7017 - mean_absolute_error: 0.4816 - val_loss: 5.5211 - val_mean_absolute_error: 1.0496\n",
            "Epoch 3004/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6316 - mean_absolute_error: 0.4650\n",
            "Epoch 3004: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6321 - mean_absolute_error: 0.4652 - val_loss: 5.1357 - val_mean_absolute_error: 1.0389\n",
            "Epoch 3005/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6294 - mean_absolute_error: 0.4717\n",
            "Epoch 3005: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6290 - mean_absolute_error: 0.4717 - val_loss: 4.7645 - val_mean_absolute_error: 1.0229\n",
            "Epoch 3006/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6897 - mean_absolute_error: 0.4867\n",
            "Epoch 3006: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6922 - mean_absolute_error: 0.4876 - val_loss: 4.9014 - val_mean_absolute_error: 1.0473\n",
            "Epoch 3007/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6414 - mean_absolute_error: 0.4700\n",
            "Epoch 3007: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6414 - mean_absolute_error: 0.4699 - val_loss: 4.8578 - val_mean_absolute_error: 1.0277\n",
            "Epoch 3008/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6328 - mean_absolute_error: 0.4680\n",
            "Epoch 3008: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6341 - mean_absolute_error: 0.4684 - val_loss: 4.9399 - val_mean_absolute_error: 1.0573\n",
            "Epoch 3009/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7083 - mean_absolute_error: 0.4867\n",
            "Epoch 3009: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7077 - mean_absolute_error: 0.4865 - val_loss: 5.5736 - val_mean_absolute_error: 1.0815\n",
            "Epoch 3010/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6109 - mean_absolute_error: 0.4613\n",
            "Epoch 3010: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6120 - mean_absolute_error: 0.4615 - val_loss: 5.0416 - val_mean_absolute_error: 1.0742\n",
            "Epoch 3011/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7218 - mean_absolute_error: 0.4883\n",
            "Epoch 3011: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7238 - mean_absolute_error: 0.4887 - val_loss: 5.5202 - val_mean_absolute_error: 1.1239\n",
            "Epoch 3012/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6586 - mean_absolute_error: 0.4756\n",
            "Epoch 3012: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6617 - mean_absolute_error: 0.4760 - val_loss: 5.0341 - val_mean_absolute_error: 1.0461\n",
            "Epoch 3013/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7010 - mean_absolute_error: 0.4845\n",
            "Epoch 3013: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7009 - mean_absolute_error: 0.4847 - val_loss: 4.9107 - val_mean_absolute_error: 1.0398\n",
            "Epoch 3014/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6516 - mean_absolute_error: 0.4740\n",
            "Epoch 3014: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6517 - mean_absolute_error: 0.4741 - val_loss: 4.9894 - val_mean_absolute_error: 1.0219\n",
            "Epoch 3015/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6662 - mean_absolute_error: 0.4744\n",
            "Epoch 3015: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6663 - mean_absolute_error: 0.4745 - val_loss: 5.1171 - val_mean_absolute_error: 1.0591\n",
            "Epoch 3016/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9150 - mean_absolute_error: 0.5311\n",
            "Epoch 3016: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9153 - mean_absolute_error: 0.5313 - val_loss: 5.6932 - val_mean_absolute_error: 1.0954\n",
            "Epoch 3017/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7573 - mean_absolute_error: 0.5065\n",
            "Epoch 3017: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7589 - mean_absolute_error: 0.5070 - val_loss: 5.1802 - val_mean_absolute_error: 1.0368\n",
            "Epoch 3018/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6850 - mean_absolute_error: 0.4853\n",
            "Epoch 3018: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6845 - mean_absolute_error: 0.4851 - val_loss: 4.7981 - val_mean_absolute_error: 1.0021\n",
            "Epoch 3019/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6239 - mean_absolute_error: 0.4654\n",
            "Epoch 3019: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6245 - mean_absolute_error: 0.4656 - val_loss: 4.9596 - val_mean_absolute_error: 1.0376\n",
            "Epoch 3020/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6200 - mean_absolute_error: 0.4619\n",
            "Epoch 3020: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6228 - mean_absolute_error: 0.4624 - val_loss: 5.1211 - val_mean_absolute_error: 1.0494\n",
            "Epoch 3021/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6891 - mean_absolute_error: 0.4794\n",
            "Epoch 3021: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6912 - mean_absolute_error: 0.4797 - val_loss: 5.4059 - val_mean_absolute_error: 1.0841\n",
            "Epoch 3022/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6518 - mean_absolute_error: 0.4733\n",
            "Epoch 3022: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6544 - mean_absolute_error: 0.4739 - val_loss: 5.4547 - val_mean_absolute_error: 1.1501\n",
            "Epoch 3023/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7022 - mean_absolute_error: 0.4948\n",
            "Epoch 3023: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7048 - mean_absolute_error: 0.4951 - val_loss: 5.2226 - val_mean_absolute_error: 1.0374\n",
            "Epoch 3024/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9102 - mean_absolute_error: 0.5246\n",
            "Epoch 3024: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9097 - mean_absolute_error: 0.5245 - val_loss: 6.1113 - val_mean_absolute_error: 1.1422\n",
            "Epoch 3025/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7110 - mean_absolute_error: 0.4930\n",
            "Epoch 3025: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7106 - mean_absolute_error: 0.4929 - val_loss: 5.1715 - val_mean_absolute_error: 1.0383\n",
            "Epoch 3026/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6324 - mean_absolute_error: 0.4660\n",
            "Epoch 3026: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6319 - mean_absolute_error: 0.4661 - val_loss: 5.3927 - val_mean_absolute_error: 1.0964\n",
            "Epoch 3027/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6058 - mean_absolute_error: 0.4588\n",
            "Epoch 3027: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6106 - mean_absolute_error: 0.4597 - val_loss: 4.8818 - val_mean_absolute_error: 1.0650\n",
            "Epoch 3028/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7345 - mean_absolute_error: 0.4938\n",
            "Epoch 3028: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7346 - mean_absolute_error: 0.4939 - val_loss: 5.3532 - val_mean_absolute_error: 1.0774\n",
            "Epoch 3029/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7100 - mean_absolute_error: 0.4866\n",
            "Epoch 3029: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7105 - mean_absolute_error: 0.4867 - val_loss: 5.0070 - val_mean_absolute_error: 1.0616\n",
            "Epoch 3030/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6400 - mean_absolute_error: 0.4667\n",
            "Epoch 3030: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6409 - mean_absolute_error: 0.4672 - val_loss: 5.1825 - val_mean_absolute_error: 1.0449\n",
            "Epoch 3031/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6993 - mean_absolute_error: 0.4975\n",
            "Epoch 3031: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6993 - mean_absolute_error: 0.4976 - val_loss: 5.3175 - val_mean_absolute_error: 1.0494\n",
            "Epoch 3032/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7955 - mean_absolute_error: 0.5066\n",
            "Epoch 3032: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7955 - mean_absolute_error: 0.5068 - val_loss: 4.6152 - val_mean_absolute_error: 1.0021\n",
            "Epoch 3033/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6286 - mean_absolute_error: 0.4647\n",
            "Epoch 3033: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6292 - mean_absolute_error: 0.4651 - val_loss: 5.4884 - val_mean_absolute_error: 1.0772\n",
            "Epoch 3034/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6783 - mean_absolute_error: 0.4756\n",
            "Epoch 3034: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6779 - mean_absolute_error: 0.4755 - val_loss: 5.3868 - val_mean_absolute_error: 1.0827\n",
            "Epoch 3035/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7444 - mean_absolute_error: 0.4939\n",
            "Epoch 3035: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7444 - mean_absolute_error: 0.4940 - val_loss: 5.0980 - val_mean_absolute_error: 1.0783\n",
            "Epoch 3036/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.7149 - mean_absolute_error: 0.4929\n",
            "Epoch 3036: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.7149 - mean_absolute_error: 0.4929 - val_loss: 4.8091 - val_mean_absolute_error: 1.0124\n",
            "Epoch 3037/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6582 - mean_absolute_error: 0.4699\n",
            "Epoch 3037: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6586 - mean_absolute_error: 0.4701 - val_loss: 5.7384 - val_mean_absolute_error: 1.1193\n",
            "Epoch 3038/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 2.0525 - mean_absolute_error: 0.7217\n",
            "Epoch 3038: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 2.0509 - mean_absolute_error: 0.7216 - val_loss: 6.4200 - val_mean_absolute_error: 1.1570\n",
            "Epoch 3039/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.9328 - mean_absolute_error: 0.5374\n",
            "Epoch 3039: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9299 - mean_absolute_error: 0.5368 - val_loss: 4.8291 - val_mean_absolute_error: 1.0430\n",
            "Epoch 3040/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6690 - mean_absolute_error: 0.4766\n",
            "Epoch 3040: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6691 - mean_absolute_error: 0.4770 - val_loss: 4.9227 - val_mean_absolute_error: 1.0216\n",
            "Epoch 3041/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6351 - mean_absolute_error: 0.4664\n",
            "Epoch 3041: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6350 - mean_absolute_error: 0.4664 - val_loss: 4.6676 - val_mean_absolute_error: 1.0348\n",
            "Epoch 3042/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5716 - mean_absolute_error: 0.4490\n",
            "Epoch 3042: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5723 - mean_absolute_error: 0.4493 - val_loss: 5.3230 - val_mean_absolute_error: 1.0440\n",
            "Epoch 3043/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5967 - mean_absolute_error: 0.4527\n",
            "Epoch 3043: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5963 - mean_absolute_error: 0.4526 - val_loss: 5.1500 - val_mean_absolute_error: 1.0150\n",
            "Epoch 3044/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.6078 - mean_absolute_error: 0.4570\n",
            "Epoch 3044: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6078 - mean_absolute_error: 0.4570 - val_loss: 4.7922 - val_mean_absolute_error: 1.0159\n",
            "Epoch 3045/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 1.4763 - mean_absolute_error: 0.6552\n",
            "Epoch 3045: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.4730 - mean_absolute_error: 0.6549 - val_loss: 5.3427 - val_mean_absolute_error: 1.0769\n",
            "Epoch 3046/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6536 - mean_absolute_error: 0.4756\n",
            "Epoch 3046: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6532 - mean_absolute_error: 0.4757 - val_loss: 4.6762 - val_mean_absolute_error: 1.0089\n",
            "Epoch 3047/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6569 - mean_absolute_error: 0.4718\n",
            "Epoch 3047: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6572 - mean_absolute_error: 0.4719 - val_loss: 5.0240 - val_mean_absolute_error: 1.0015\n",
            "Epoch 3048/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6070 - mean_absolute_error: 0.4565\n",
            "Epoch 3048: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6103 - mean_absolute_error: 0.4570 - val_loss: 5.0504 - val_mean_absolute_error: 1.0450\n",
            "Epoch 3049/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6016 - mean_absolute_error: 0.4551\n",
            "Epoch 3049: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6015 - mean_absolute_error: 0.4552 - val_loss: 4.8236 - val_mean_absolute_error: 1.0001\n",
            "Epoch 3050/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5797 - mean_absolute_error: 0.4497\n",
            "Epoch 3050: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5802 - mean_absolute_error: 0.4498 - val_loss: 5.0457 - val_mean_absolute_error: 1.0435\n",
            "Epoch 3051/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6521 - mean_absolute_error: 0.4762\n",
            "Epoch 3051: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6526 - mean_absolute_error: 0.4764 - val_loss: 4.9981 - val_mean_absolute_error: 1.0122\n",
            "Epoch 3052/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1710 - mean_absolute_error: 0.5728\n",
            "Epoch 3052: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.1725 - mean_absolute_error: 0.5734 - val_loss: 10.2156 - val_mean_absolute_error: 1.7412\n",
            "Epoch 3053/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 2.0592 - mean_absolute_error: 0.7087\n",
            "Epoch 3053: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 2.0543 - mean_absolute_error: 0.7085 - val_loss: 4.6302 - val_mean_absolute_error: 1.0612\n",
            "Epoch 3054/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8933 - mean_absolute_error: 0.5211\n",
            "Epoch 3054: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8941 - mean_absolute_error: 0.5213 - val_loss: 5.5137 - val_mean_absolute_error: 1.0683\n",
            "Epoch 3055/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7388 - mean_absolute_error: 0.4824\n",
            "Epoch 3055: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7408 - mean_absolute_error: 0.4830 - val_loss: 5.5682 - val_mean_absolute_error: 1.0501\n",
            "Epoch 3056/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6627 - mean_absolute_error: 0.4720\n",
            "Epoch 3056: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6629 - mean_absolute_error: 0.4722 - val_loss: 5.0981 - val_mean_absolute_error: 1.0179\n",
            "Epoch 3057/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6228 - mean_absolute_error: 0.4598\n",
            "Epoch 3057: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6233 - mean_absolute_error: 0.4601 - val_loss: 5.0650 - val_mean_absolute_error: 1.0235\n",
            "Epoch 3058/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7128 - mean_absolute_error: 0.4795\n",
            "Epoch 3058: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7128 - mean_absolute_error: 0.4798 - val_loss: 4.8077 - val_mean_absolute_error: 1.0282\n",
            "Epoch 3059/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6232 - mean_absolute_error: 0.4708\n",
            "Epoch 3059: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6228 - mean_absolute_error: 0.4708 - val_loss: 4.9705 - val_mean_absolute_error: 1.0255\n",
            "Epoch 3060/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5912 - mean_absolute_error: 0.4497\n",
            "Epoch 3060: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5926 - mean_absolute_error: 0.4501 - val_loss: 5.0418 - val_mean_absolute_error: 1.0172\n",
            "Epoch 3061/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5890 - mean_absolute_error: 0.4506\n",
            "Epoch 3061: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5886 - mean_absolute_error: 0.4505 - val_loss: 5.0714 - val_mean_absolute_error: 1.0192\n",
            "Epoch 3062/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6097 - mean_absolute_error: 0.4621\n",
            "Epoch 3062: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6135 - mean_absolute_error: 0.4628 - val_loss: 4.9915 - val_mean_absolute_error: 1.0869\n",
            "Epoch 3063/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6513 - mean_absolute_error: 0.4749\n",
            "Epoch 3063: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6525 - mean_absolute_error: 0.4753 - val_loss: 4.9942 - val_mean_absolute_error: 1.0157\n",
            "Epoch 3064/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6870 - mean_absolute_error: 0.4855\n",
            "Epoch 3064: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6902 - mean_absolute_error: 0.4861 - val_loss: 5.9465 - val_mean_absolute_error: 1.1052\n",
            "Epoch 3065/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7269 - mean_absolute_error: 0.4899\n",
            "Epoch 3065: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7274 - mean_absolute_error: 0.4902 - val_loss: 5.4348 - val_mean_absolute_error: 1.0768\n",
            "Epoch 3066/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6977 - mean_absolute_error: 0.4830\n",
            "Epoch 3066: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7014 - mean_absolute_error: 0.4837 - val_loss: 4.9480 - val_mean_absolute_error: 1.0942\n",
            "Epoch 3067/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6239 - mean_absolute_error: 0.4653\n",
            "Epoch 3067: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6237 - mean_absolute_error: 0.4653 - val_loss: 5.0875 - val_mean_absolute_error: 1.0678\n",
            "Epoch 3068/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6040 - mean_absolute_error: 0.4632\n",
            "Epoch 3068: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6038 - mean_absolute_error: 0.4631 - val_loss: 5.2723 - val_mean_absolute_error: 1.0517\n",
            "Epoch 3069/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6343 - mean_absolute_error: 0.4717\n",
            "Epoch 3069: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6363 - mean_absolute_error: 0.4718 - val_loss: 5.0168 - val_mean_absolute_error: 1.0228\n",
            "Epoch 3070/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7016 - mean_absolute_error: 0.4861\n",
            "Epoch 3070: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7014 - mean_absolute_error: 0.4861 - val_loss: 5.0811 - val_mean_absolute_error: 1.0422\n",
            "Epoch 3071/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6065 - mean_absolute_error: 0.4613\n",
            "Epoch 3071: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6136 - mean_absolute_error: 0.4629 - val_loss: 5.2168 - val_mean_absolute_error: 1.0407\n",
            "Epoch 3072/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.8316 - mean_absolute_error: 0.5234\n",
            "Epoch 3072: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8309 - mean_absolute_error: 0.5231 - val_loss: 5.1539 - val_mean_absolute_error: 1.1406\n",
            "Epoch 3073/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6395 - mean_absolute_error: 0.4672\n",
            "Epoch 3073: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6401 - mean_absolute_error: 0.4674 - val_loss: 4.7106 - val_mean_absolute_error: 0.9879\n",
            "Epoch 3074/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7527 - mean_absolute_error: 0.4948\n",
            "Epoch 3074: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7525 - mean_absolute_error: 0.4948 - val_loss: 5.0749 - val_mean_absolute_error: 1.0316\n",
            "Epoch 3075/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5998 - mean_absolute_error: 0.4531\n",
            "Epoch 3075: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6004 - mean_absolute_error: 0.4534 - val_loss: 4.7969 - val_mean_absolute_error: 1.0270\n",
            "Epoch 3076/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7059 - mean_absolute_error: 0.4838\n",
            "Epoch 3076: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7396 - mean_absolute_error: 0.4855 - val_loss: 4.9390 - val_mean_absolute_error: 1.0316\n",
            "Epoch 3077/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6824 - mean_absolute_error: 0.4807\n",
            "Epoch 3077: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6822 - mean_absolute_error: 0.4808 - val_loss: 5.1262 - val_mean_absolute_error: 1.0708\n",
            "Epoch 3078/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7802 - mean_absolute_error: 0.5050\n",
            "Epoch 3078: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7801 - mean_absolute_error: 0.5050 - val_loss: 5.0874 - val_mean_absolute_error: 1.0521\n",
            "Epoch 3079/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.7235 - mean_absolute_error: 0.4902\n",
            "Epoch 3079: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7235 - mean_absolute_error: 0.4902 - val_loss: 4.8896 - val_mean_absolute_error: 1.0360\n",
            "Epoch 3080/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7560 - mean_absolute_error: 0.5022\n",
            "Epoch 3080: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7558 - mean_absolute_error: 0.5023 - val_loss: 5.2758 - val_mean_absolute_error: 1.1072\n",
            "Epoch 3081/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6293 - mean_absolute_error: 0.4639\n",
            "Epoch 3081: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6297 - mean_absolute_error: 0.4642 - val_loss: 5.1118 - val_mean_absolute_error: 1.0468\n",
            "Epoch 3082/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6313 - mean_absolute_error: 0.4763\n",
            "Epoch 3082: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6324 - mean_absolute_error: 0.4767 - val_loss: 5.1575 - val_mean_absolute_error: 1.0495\n",
            "Epoch 3083/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6567 - mean_absolute_error: 0.4748\n",
            "Epoch 3083: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6564 - mean_absolute_error: 0.4747 - val_loss: 4.8241 - val_mean_absolute_error: 1.0305\n",
            "Epoch 3084/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7189 - mean_absolute_error: 0.4880\n",
            "Epoch 3084: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7194 - mean_absolute_error: 0.4882 - val_loss: 5.1371 - val_mean_absolute_error: 1.0805\n",
            "Epoch 3085/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6646 - mean_absolute_error: 0.4795\n",
            "Epoch 3085: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6734 - mean_absolute_error: 0.4803 - val_loss: 5.7161 - val_mean_absolute_error: 1.0895\n",
            "Epoch 3086/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0636 - mean_absolute_error: 0.5630\n",
            "Epoch 3086: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0648 - mean_absolute_error: 0.5633 - val_loss: 4.9811 - val_mean_absolute_error: 1.0587\n",
            "Epoch 3087/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9293 - mean_absolute_error: 0.5373\n",
            "Epoch 3087: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9291 - mean_absolute_error: 0.5375 - val_loss: 5.7140 - val_mean_absolute_error: 1.0799\n",
            "Epoch 3088/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6530 - mean_absolute_error: 0.4781\n",
            "Epoch 3088: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6532 - mean_absolute_error: 0.4781 - val_loss: 5.1042 - val_mean_absolute_error: 1.0419\n",
            "Epoch 3089/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6576 - mean_absolute_error: 0.4703\n",
            "Epoch 3089: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6574 - mean_absolute_error: 0.4703 - val_loss: 5.3926 - val_mean_absolute_error: 1.1006\n",
            "Epoch 3090/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0671 - mean_absolute_error: 0.5526\n",
            "Epoch 3090: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.0667 - mean_absolute_error: 0.5527 - val_loss: 5.3737 - val_mean_absolute_error: 1.0661\n",
            "Epoch 3091/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0374 - mean_absolute_error: 0.5664\n",
            "Epoch 3091: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0380 - mean_absolute_error: 0.5666 - val_loss: 5.2614 - val_mean_absolute_error: 1.0488\n",
            "Epoch 3092/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6514 - mean_absolute_error: 0.4742\n",
            "Epoch 3092: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6515 - mean_absolute_error: 0.4742 - val_loss: 5.2143 - val_mean_absolute_error: 1.0517\n",
            "Epoch 3093/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6072 - mean_absolute_error: 0.4605\n",
            "Epoch 3093: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6076 - mean_absolute_error: 0.4606 - val_loss: 5.5707 - val_mean_absolute_error: 1.0835\n",
            "Epoch 3094/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6021 - mean_absolute_error: 0.4572\n",
            "Epoch 3094: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6031 - mean_absolute_error: 0.4576 - val_loss: 5.2199 - val_mean_absolute_error: 1.1058\n",
            "Epoch 3095/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6077 - mean_absolute_error: 0.4616\n",
            "Epoch 3095: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6090 - mean_absolute_error: 0.4621 - val_loss: 5.0798 - val_mean_absolute_error: 1.0411\n",
            "Epoch 3096/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5612 - mean_absolute_error: 0.4437\n",
            "Epoch 3096: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5612 - mean_absolute_error: 0.4437 - val_loss: 5.0243 - val_mean_absolute_error: 1.0236\n",
            "Epoch 3097/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6474 - mean_absolute_error: 0.4692\n",
            "Epoch 3097: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6536 - mean_absolute_error: 0.4697 - val_loss: 4.9318 - val_mean_absolute_error: 1.0433\n",
            "Epoch 3098/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 2.4179 - mean_absolute_error: 0.7835\n",
            "Epoch 3098: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 2.4160 - mean_absolute_error: 0.7833 - val_loss: 5.2155 - val_mean_absolute_error: 1.0605\n",
            "Epoch 3099/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8492 - mean_absolute_error: 0.5183\n",
            "Epoch 3099: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8486 - mean_absolute_error: 0.5180 - val_loss: 5.1563 - val_mean_absolute_error: 1.0345\n",
            "Epoch 3100/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6256 - mean_absolute_error: 0.4626\n",
            "Epoch 3100: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6257 - mean_absolute_error: 0.4628 - val_loss: 5.1617 - val_mean_absolute_error: 1.0315\n",
            "Epoch 3101/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5791 - mean_absolute_error: 0.4503\n",
            "Epoch 3101: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5788 - mean_absolute_error: 0.4503 - val_loss: 5.0084 - val_mean_absolute_error: 1.0049\n",
            "Epoch 3102/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6166 - mean_absolute_error: 0.4577\n",
            "Epoch 3102: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6170 - mean_absolute_error: 0.4580 - val_loss: 4.8119 - val_mean_absolute_error: 0.9980\n",
            "Epoch 3103/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6105 - mean_absolute_error: 0.4587\n",
            "Epoch 3103: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6117 - mean_absolute_error: 0.4590 - val_loss: 4.8896 - val_mean_absolute_error: 1.0190\n",
            "Epoch 3104/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6287 - mean_absolute_error: 0.4629\n",
            "Epoch 3104: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6288 - mean_absolute_error: 0.4629 - val_loss: 4.9465 - val_mean_absolute_error: 1.0284\n",
            "Epoch 3105/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5690 - mean_absolute_error: 0.4469\n",
            "Epoch 3105: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5688 - mean_absolute_error: 0.4469 - val_loss: 4.9928 - val_mean_absolute_error: 1.0528\n",
            "Epoch 3106/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5905 - mean_absolute_error: 0.4548\n",
            "Epoch 3106: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5924 - mean_absolute_error: 0.4552 - val_loss: 4.9490 - val_mean_absolute_error: 1.0326\n",
            "Epoch 3107/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6824 - mean_absolute_error: 0.4831\n",
            "Epoch 3107: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 5s 25ms/step - loss: 0.6826 - mean_absolute_error: 0.4833 - val_loss: 4.8563 - val_mean_absolute_error: 1.0651\n",
            "Epoch 3108/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6230 - mean_absolute_error: 0.4674\n",
            "Epoch 3108: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6227 - mean_absolute_error: 0.4674 - val_loss: 4.9300 - val_mean_absolute_error: 1.0292\n",
            "Epoch 3109/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6137 - mean_absolute_error: 0.4598\n",
            "Epoch 3109: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6139 - mean_absolute_error: 0.4600 - val_loss: 5.0391 - val_mean_absolute_error: 1.1406\n",
            "Epoch 3110/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6474 - mean_absolute_error: 0.4681\n",
            "Epoch 3110: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6483 - mean_absolute_error: 0.4684 - val_loss: 4.9730 - val_mean_absolute_error: 1.0554\n",
            "Epoch 3111/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6173 - mean_absolute_error: 0.4606\n",
            "Epoch 3111: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6168 - mean_absolute_error: 0.4604 - val_loss: 5.1074 - val_mean_absolute_error: 1.0557\n",
            "Epoch 3112/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6267 - mean_absolute_error: 0.4654\n",
            "Epoch 3112: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6291 - mean_absolute_error: 0.4661 - val_loss: 5.1355 - val_mean_absolute_error: 1.0766\n",
            "Epoch 3113/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6413 - mean_absolute_error: 0.4689\n",
            "Epoch 3113: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6455 - mean_absolute_error: 0.4695 - val_loss: 5.0617 - val_mean_absolute_error: 1.0507\n",
            "Epoch 3114/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6745 - mean_absolute_error: 0.4773\n",
            "Epoch 3114: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6741 - mean_absolute_error: 0.4771 - val_loss: 5.4608 - val_mean_absolute_error: 1.0698\n",
            "Epoch 3115/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6207 - mean_absolute_error: 0.4648\n",
            "Epoch 3115: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.6210 - mean_absolute_error: 0.4650 - val_loss: 5.8840 - val_mean_absolute_error: 1.1188\n",
            "Epoch 3116/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6345 - mean_absolute_error: 0.4678\n",
            "Epoch 3116: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6373 - mean_absolute_error: 0.4682 - val_loss: 5.1955 - val_mean_absolute_error: 1.0272\n",
            "Epoch 3117/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1836 - mean_absolute_error: 0.5820\n",
            "Epoch 3117: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1828 - mean_absolute_error: 0.5819 - val_loss: 6.1518 - val_mean_absolute_error: 1.2711\n",
            "Epoch 3118/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 1.6189 - mean_absolute_error: 0.6556\n",
            "Epoch 3118: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.6192 - mean_absolute_error: 0.6561 - val_loss: 5.9936 - val_mean_absolute_error: 1.1726\n",
            "Epoch 3119/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0131 - mean_absolute_error: 0.5554\n",
            "Epoch 3119: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0145 - mean_absolute_error: 0.5560 - val_loss: 5.1408 - val_mean_absolute_error: 1.0614\n",
            "Epoch 3120/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6671 - mean_absolute_error: 0.4734\n",
            "Epoch 3120: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6667 - mean_absolute_error: 0.4733 - val_loss: 4.8653 - val_mean_absolute_error: 1.0404\n",
            "Epoch 3121/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6240 - mean_absolute_error: 0.4633\n",
            "Epoch 3121: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6251 - mean_absolute_error: 0.4635 - val_loss: 4.8238 - val_mean_absolute_error: 1.0400\n",
            "Epoch 3122/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5786 - mean_absolute_error: 0.4494\n",
            "Epoch 3122: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5786 - mean_absolute_error: 0.4495 - val_loss: 5.0166 - val_mean_absolute_error: 1.0310\n",
            "Epoch 3123/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6829 - mean_absolute_error: 0.4801\n",
            "Epoch 3123: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6826 - mean_absolute_error: 0.4801 - val_loss: 5.2043 - val_mean_absolute_error: 1.1007\n",
            "Epoch 3124/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5973 - mean_absolute_error: 0.4583\n",
            "Epoch 3124: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5992 - mean_absolute_error: 0.4590 - val_loss: 5.2425 - val_mean_absolute_error: 1.0565\n",
            "Epoch 3125/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6427 - mean_absolute_error: 0.4701\n",
            "Epoch 3125: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6462 - mean_absolute_error: 0.4708 - val_loss: 5.5246 - val_mean_absolute_error: 1.0710\n",
            "Epoch 3126/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7016 - mean_absolute_error: 0.4943\n",
            "Epoch 3126: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7010 - mean_absolute_error: 0.4941 - val_loss: 5.1509 - val_mean_absolute_error: 1.1191\n",
            "Epoch 3127/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6214 - mean_absolute_error: 0.4645\n",
            "Epoch 3127: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6212 - mean_absolute_error: 0.4647 - val_loss: 5.1166 - val_mean_absolute_error: 1.0429\n",
            "Epoch 3128/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6192 - mean_absolute_error: 0.4657\n",
            "Epoch 3128: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6195 - mean_absolute_error: 0.4662 - val_loss: 4.8798 - val_mean_absolute_error: 1.0708\n",
            "Epoch 3129/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6278 - mean_absolute_error: 0.4705\n",
            "Epoch 3129: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6280 - mean_absolute_error: 0.4707 - val_loss: 4.8336 - val_mean_absolute_error: 1.0091\n",
            "Epoch 3130/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6175 - mean_absolute_error: 0.4612\n",
            "Epoch 3130: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6173 - mean_absolute_error: 0.4613 - val_loss: 5.3978 - val_mean_absolute_error: 1.0534\n",
            "Epoch 3131/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6231 - mean_absolute_error: 0.4655\n",
            "Epoch 3131: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6239 - mean_absolute_error: 0.4660 - val_loss: 5.0163 - val_mean_absolute_error: 1.0645\n",
            "Epoch 3132/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6063 - mean_absolute_error: 0.4624\n",
            "Epoch 3132: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6064 - mean_absolute_error: 0.4626 - val_loss: 5.2770 - val_mean_absolute_error: 1.1277\n",
            "Epoch 3133/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6662 - mean_absolute_error: 0.4733\n",
            "Epoch 3133: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6658 - mean_absolute_error: 0.4731 - val_loss: 5.2335 - val_mean_absolute_error: 1.0764\n",
            "Epoch 3134/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6264 - mean_absolute_error: 0.4658\n",
            "Epoch 3134: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6267 - mean_absolute_error: 0.4658 - val_loss: 5.7063 - val_mean_absolute_error: 1.1286\n",
            "Epoch 3135/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7182 - mean_absolute_error: 0.4923\n",
            "Epoch 3135: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7179 - mean_absolute_error: 0.4923 - val_loss: 5.1976 - val_mean_absolute_error: 1.0397\n",
            "Epoch 3136/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6401 - mean_absolute_error: 0.4772\n",
            "Epoch 3136: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6408 - mean_absolute_error: 0.4776 - val_loss: 4.8181 - val_mean_absolute_error: 1.0299\n",
            "Epoch 3137/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6690 - mean_absolute_error: 0.4757\n",
            "Epoch 3137: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6688 - mean_absolute_error: 0.4758 - val_loss: 4.9882 - val_mean_absolute_error: 1.0385\n",
            "Epoch 3138/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6855 - mean_absolute_error: 0.4852\n",
            "Epoch 3138: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6857 - mean_absolute_error: 0.4852 - val_loss: 5.4429 - val_mean_absolute_error: 1.1095\n",
            "Epoch 3139/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6712 - mean_absolute_error: 0.4825\n",
            "Epoch 3139: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6722 - mean_absolute_error: 0.4828 - val_loss: 5.1284 - val_mean_absolute_error: 1.1096\n",
            "Epoch 3140/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6524 - mean_absolute_error: 0.4756\n",
            "Epoch 3140: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6522 - mean_absolute_error: 0.4756 - val_loss: 4.9289 - val_mean_absolute_error: 1.0324\n",
            "Epoch 3141/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6444 - mean_absolute_error: 0.4708\n",
            "Epoch 3141: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6486 - mean_absolute_error: 0.4717 - val_loss: 5.5493 - val_mean_absolute_error: 1.0826\n",
            "Epoch 3142/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6290 - mean_absolute_error: 0.4646\n",
            "Epoch 3142: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6292 - mean_absolute_error: 0.4648 - val_loss: 5.0697 - val_mean_absolute_error: 1.0949\n",
            "Epoch 3143/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6237 - mean_absolute_error: 0.4719\n",
            "Epoch 3143: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6233 - mean_absolute_error: 0.4717 - val_loss: 5.1442 - val_mean_absolute_error: 1.0573\n",
            "Epoch 3144/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6550 - mean_absolute_error: 0.4699\n",
            "Epoch 3144: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6553 - mean_absolute_error: 0.4702 - val_loss: 5.0700 - val_mean_absolute_error: 1.0128\n",
            "Epoch 3145/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6983 - mean_absolute_error: 0.4858\n",
            "Epoch 3145: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6979 - mean_absolute_error: 0.4858 - val_loss: 5.4068 - val_mean_absolute_error: 1.0810\n",
            "Epoch 3146/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6695 - mean_absolute_error: 0.4759\n",
            "Epoch 3146: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6701 - mean_absolute_error: 0.4762 - val_loss: 5.5593 - val_mean_absolute_error: 1.0657\n",
            "Epoch 3147/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7125 - mean_absolute_error: 0.4863\n",
            "Epoch 3147: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7131 - mean_absolute_error: 0.4866 - val_loss: 5.1985 - val_mean_absolute_error: 1.0211\n",
            "Epoch 3148/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7585 - mean_absolute_error: 0.4958\n",
            "Epoch 3148: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7585 - mean_absolute_error: 0.4961 - val_loss: 5.5451 - val_mean_absolute_error: 1.1922\n",
            "Epoch 3149/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7040 - mean_absolute_error: 0.4863\n",
            "Epoch 3149: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7035 - mean_absolute_error: 0.4861 - val_loss: 5.1237 - val_mean_absolute_error: 1.0674\n",
            "Epoch 3150/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6581 - mean_absolute_error: 0.4756\n",
            "Epoch 3150: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6584 - mean_absolute_error: 0.4758 - val_loss: 4.7980 - val_mean_absolute_error: 1.0324\n",
            "Epoch 3151/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7456 - mean_absolute_error: 0.5027\n",
            "Epoch 3151: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7468 - mean_absolute_error: 0.5029 - val_loss: 5.3244 - val_mean_absolute_error: 1.0449\n",
            "Epoch 3152/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6893 - mean_absolute_error: 0.4861\n",
            "Epoch 3152: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6908 - mean_absolute_error: 0.4867 - val_loss: 5.3391 - val_mean_absolute_error: 1.0827\n",
            "Epoch 3153/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6356 - mean_absolute_error: 0.4644\n",
            "Epoch 3153: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6362 - mean_absolute_error: 0.4648 - val_loss: 4.8512 - val_mean_absolute_error: 1.0428\n",
            "Epoch 3154/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.6915 - mean_absolute_error: 0.4783\n",
            "Epoch 3154: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6915 - mean_absolute_error: 0.4783 - val_loss: 5.4585 - val_mean_absolute_error: 1.1264\n",
            "Epoch 3155/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6176 - mean_absolute_error: 0.4587\n",
            "Epoch 3155: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6173 - mean_absolute_error: 0.4586 - val_loss: 4.7353 - val_mean_absolute_error: 0.9976\n",
            "Epoch 3156/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6447 - mean_absolute_error: 0.4700\n",
            "Epoch 3156: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6451 - mean_absolute_error: 0.4703 - val_loss: 5.2823 - val_mean_absolute_error: 1.0746\n",
            "Epoch 3157/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6825 - mean_absolute_error: 0.4827\n",
            "Epoch 3157: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6821 - mean_absolute_error: 0.4826 - val_loss: 5.1387 - val_mean_absolute_error: 1.0642\n",
            "Epoch 3158/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6436 - mean_absolute_error: 0.4635\n",
            "Epoch 3158: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6442 - mean_absolute_error: 0.4635 - val_loss: 5.6115 - val_mean_absolute_error: 1.2264\n",
            "Epoch 3159/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8205 - mean_absolute_error: 0.5113\n",
            "Epoch 3159: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8225 - mean_absolute_error: 0.5120 - val_loss: 4.9228 - val_mean_absolute_error: 1.0274\n",
            "Epoch 3160/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7130 - mean_absolute_error: 0.4864\n",
            "Epoch 3160: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7139 - mean_absolute_error: 0.4866 - val_loss: 5.8746 - val_mean_absolute_error: 1.0868\n",
            "Epoch 3161/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7480 - mean_absolute_error: 0.4972\n",
            "Epoch 3161: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7490 - mean_absolute_error: 0.4980 - val_loss: 5.2157 - val_mean_absolute_error: 1.1301\n",
            "Epoch 3162/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6677 - mean_absolute_error: 0.4798\n",
            "Epoch 3162: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6678 - mean_absolute_error: 0.4799 - val_loss: 5.5749 - val_mean_absolute_error: 1.0886\n",
            "Epoch 3163/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6069 - mean_absolute_error: 0.4617\n",
            "Epoch 3163: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6080 - mean_absolute_error: 0.4622 - val_loss: 4.9794 - val_mean_absolute_error: 1.0205\n",
            "Epoch 3164/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6310 - mean_absolute_error: 0.4656\n",
            "Epoch 3164: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6331 - mean_absolute_error: 0.4661 - val_loss: 5.1011 - val_mean_absolute_error: 1.0221\n",
            "Epoch 3165/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7287 - mean_absolute_error: 0.4929\n",
            "Epoch 3165: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7297 - mean_absolute_error: 0.4933 - val_loss: 5.1855 - val_mean_absolute_error: 1.0378\n",
            "Epoch 3166/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6698 - mean_absolute_error: 0.4782\n",
            "Epoch 3166: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6699 - mean_absolute_error: 0.4784 - val_loss: 5.2699 - val_mean_absolute_error: 1.0730\n",
            "Epoch 3167/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6355 - mean_absolute_error: 0.4642\n",
            "Epoch 3167: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6369 - mean_absolute_error: 0.4648 - val_loss: 5.0707 - val_mean_absolute_error: 1.0765\n",
            "Epoch 3168/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6684 - mean_absolute_error: 0.4749\n",
            "Epoch 3168: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6692 - mean_absolute_error: 0.4751 - val_loss: 5.3868 - val_mean_absolute_error: 1.0415\n",
            "Epoch 3169/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6255 - mean_absolute_error: 0.4700\n",
            "Epoch 3169: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6258 - mean_absolute_error: 0.4702 - val_loss: 5.2088 - val_mean_absolute_error: 1.0433\n",
            "Epoch 3170/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5852 - mean_absolute_error: 0.4501\n",
            "Epoch 3170: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5855 - mean_absolute_error: 0.4502 - val_loss: 4.6550 - val_mean_absolute_error: 1.0095\n",
            "Epoch 3171/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6458 - mean_absolute_error: 0.4726\n",
            "Epoch 3171: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6466 - mean_absolute_error: 0.4728 - val_loss: 5.5319 - val_mean_absolute_error: 1.0850\n",
            "Epoch 3172/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7400 - mean_absolute_error: 0.4987\n",
            "Epoch 3172: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7423 - mean_absolute_error: 0.4991 - val_loss: 4.8872 - val_mean_absolute_error: 1.0110\n",
            "Epoch 3173/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7342 - mean_absolute_error: 0.4952\n",
            "Epoch 3173: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7336 - mean_absolute_error: 0.4950 - val_loss: 5.3044 - val_mean_absolute_error: 1.0415\n",
            "Epoch 3174/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7258 - mean_absolute_error: 0.5007\n",
            "Epoch 3174: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7278 - mean_absolute_error: 0.5012 - val_loss: 5.2112 - val_mean_absolute_error: 1.0353\n",
            "Epoch 3175/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6893 - mean_absolute_error: 0.4851\n",
            "Epoch 3175: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6901 - mean_absolute_error: 0.4853 - val_loss: 5.3047 - val_mean_absolute_error: 1.0480\n",
            "Epoch 3176/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6680 - mean_absolute_error: 0.4790\n",
            "Epoch 3176: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6698 - mean_absolute_error: 0.4795 - val_loss: 4.8055 - val_mean_absolute_error: 1.0262\n",
            "Epoch 3177/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6393 - mean_absolute_error: 0.4691\n",
            "Epoch 3177: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6398 - mean_absolute_error: 0.4694 - val_loss: 5.0677 - val_mean_absolute_error: 1.0215\n",
            "Epoch 3178/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6885 - mean_absolute_error: 0.4865\n",
            "Epoch 3178: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6881 - mean_absolute_error: 0.4863 - val_loss: 4.9741 - val_mean_absolute_error: 1.0513\n",
            "Epoch 3179/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7524 - mean_absolute_error: 0.4975\n",
            "Epoch 3179: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7582 - mean_absolute_error: 0.4982 - val_loss: 5.0912 - val_mean_absolute_error: 1.0155\n",
            "Epoch 3180/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6570 - mean_absolute_error: 0.4747\n",
            "Epoch 3180: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6570 - mean_absolute_error: 0.4748 - val_loss: 5.0740 - val_mean_absolute_error: 1.1197\n",
            "Epoch 3181/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6130 - mean_absolute_error: 0.4643\n",
            "Epoch 3181: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6136 - mean_absolute_error: 0.4648 - val_loss: 5.3210 - val_mean_absolute_error: 1.0989\n",
            "Epoch 3182/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6408 - mean_absolute_error: 0.4723\n",
            "Epoch 3182: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6415 - mean_absolute_error: 0.4726 - val_loss: 5.4386 - val_mean_absolute_error: 1.1317\n",
            "Epoch 3183/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6648 - mean_absolute_error: 0.4788\n",
            "Epoch 3183: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6654 - mean_absolute_error: 0.4790 - val_loss: 4.8256 - val_mean_absolute_error: 1.1219\n",
            "Epoch 3184/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7225 - mean_absolute_error: 0.4904\n",
            "Epoch 3184: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7219 - mean_absolute_error: 0.4902 - val_loss: 4.7754 - val_mean_absolute_error: 1.0016\n",
            "Epoch 3185/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6440 - mean_absolute_error: 0.4714\n",
            "Epoch 3185: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6439 - mean_absolute_error: 0.4714 - val_loss: 5.2876 - val_mean_absolute_error: 1.0985\n",
            "Epoch 3186/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6104 - mean_absolute_error: 0.4632\n",
            "Epoch 3186: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6101 - mean_absolute_error: 0.4630 - val_loss: 5.0254 - val_mean_absolute_error: 1.0166\n",
            "Epoch 3187/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9541 - mean_absolute_error: 0.5287\n",
            "Epoch 3187: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9540 - mean_absolute_error: 0.5288 - val_loss: 5.5885 - val_mean_absolute_error: 1.0918\n",
            "Epoch 3188/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7739 - mean_absolute_error: 0.5059\n",
            "Epoch 3188: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7767 - mean_absolute_error: 0.5066 - val_loss: 5.3569 - val_mean_absolute_error: 1.0513\n",
            "Epoch 3189/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7019 - mean_absolute_error: 0.4879\n",
            "Epoch 3189: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7070 - mean_absolute_error: 0.4884 - val_loss: 5.2262 - val_mean_absolute_error: 1.1358\n",
            "Epoch 3190/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2012 - mean_absolute_error: 0.5879\n",
            "Epoch 3190: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2004 - mean_absolute_error: 0.5877 - val_loss: 4.9977 - val_mean_absolute_error: 1.0570\n",
            "Epoch 3191/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6121 - mean_absolute_error: 0.4590\n",
            "Epoch 3191: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6122 - mean_absolute_error: 0.4592 - val_loss: 4.5166 - val_mean_absolute_error: 0.9838\n",
            "Epoch 3192/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5653 - mean_absolute_error: 0.4443\n",
            "Epoch 3192: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5707 - mean_absolute_error: 0.4454 - val_loss: 4.9743 - val_mean_absolute_error: 1.0518\n",
            "Epoch 3193/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7490 - mean_absolute_error: 0.4988\n",
            "Epoch 3193: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7487 - mean_absolute_error: 0.4989 - val_loss: 4.7834 - val_mean_absolute_error: 1.0362\n",
            "Epoch 3194/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5806 - mean_absolute_error: 0.4513\n",
            "Epoch 3194: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5805 - mean_absolute_error: 0.4513 - val_loss: 4.9378 - val_mean_absolute_error: 1.0218\n",
            "Epoch 3195/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5857 - mean_absolute_error: 0.4552\n",
            "Epoch 3195: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5860 - mean_absolute_error: 0.4555 - val_loss: 5.1065 - val_mean_absolute_error: 1.0539\n",
            "Epoch 3196/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5856 - mean_absolute_error: 0.4563\n",
            "Epoch 3196: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5852 - mean_absolute_error: 0.4562 - val_loss: 4.7078 - val_mean_absolute_error: 1.0011\n",
            "Epoch 3197/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6131 - mean_absolute_error: 0.4624\n",
            "Epoch 3197: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6158 - mean_absolute_error: 0.4630 - val_loss: 5.2988 - val_mean_absolute_error: 1.0567\n",
            "Epoch 3198/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7202 - mean_absolute_error: 0.4955\n",
            "Epoch 3198: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7200 - mean_absolute_error: 0.4955 - val_loss: 5.0831 - val_mean_absolute_error: 1.0288\n",
            "Epoch 3199/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6506 - mean_absolute_error: 0.4779\n",
            "Epoch 3199: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6508 - mean_absolute_error: 0.4781 - val_loss: 5.1015 - val_mean_absolute_error: 1.0601\n",
            "Epoch 3200/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5589 - mean_absolute_error: 0.4449\n",
            "Epoch 3200: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5655 - mean_absolute_error: 0.4465 - val_loss: 5.2318 - val_mean_absolute_error: 1.0344\n",
            "Epoch 3201/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6237 - mean_absolute_error: 0.4627\n",
            "Epoch 3201: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6256 - mean_absolute_error: 0.4631 - val_loss: 5.9289 - val_mean_absolute_error: 1.1631\n",
            "Epoch 3202/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.4850 - mean_absolute_error: 0.6480\n",
            "Epoch 3202: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.4867 - mean_absolute_error: 0.6485 - val_loss: 5.4799 - val_mean_absolute_error: 1.0734\n",
            "Epoch 3203/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6716 - mean_absolute_error: 0.4765\n",
            "Epoch 3203: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6710 - mean_absolute_error: 0.4763 - val_loss: 5.1224 - val_mean_absolute_error: 1.0579\n",
            "Epoch 3204/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6214 - mean_absolute_error: 0.4618\n",
            "Epoch 3204: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6239 - mean_absolute_error: 0.4626 - val_loss: 5.1032 - val_mean_absolute_error: 1.0610\n",
            "Epoch 3205/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7430 - mean_absolute_error: 0.4886\n",
            "Epoch 3205: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7427 - mean_absolute_error: 0.4888 - val_loss: 6.0126 - val_mean_absolute_error: 1.2338\n",
            "Epoch 3206/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6054 - mean_absolute_error: 0.4557\n",
            "Epoch 3206: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6048 - mean_absolute_error: 0.4555 - val_loss: 4.6905 - val_mean_absolute_error: 1.0445\n",
            "Epoch 3207/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5303 - mean_absolute_error: 0.4391\n",
            "Epoch 3207: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5330 - mean_absolute_error: 0.4397 - val_loss: 4.6980 - val_mean_absolute_error: 1.0026\n",
            "Epoch 3208/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6590 - mean_absolute_error: 0.4756\n",
            "Epoch 3208: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6586 - mean_absolute_error: 0.4754 - val_loss: 5.9528 - val_mean_absolute_error: 1.1026\n",
            "Epoch 3209/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6495 - mean_absolute_error: 0.4659\n",
            "Epoch 3209: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6494 - mean_absolute_error: 0.4659 - val_loss: 5.8519 - val_mean_absolute_error: 1.1954\n",
            "Epoch 3210/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6402 - mean_absolute_error: 0.4684\n",
            "Epoch 3210: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6406 - mean_absolute_error: 0.4686 - val_loss: 4.8172 - val_mean_absolute_error: 1.0216\n",
            "Epoch 3211/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6928 - mean_absolute_error: 0.4815\n",
            "Epoch 3211: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6943 - mean_absolute_error: 0.4816 - val_loss: 5.7041 - val_mean_absolute_error: 1.0885\n",
            "Epoch 3212/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5849 - mean_absolute_error: 0.4455\n",
            "Epoch 3212: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5850 - mean_absolute_error: 0.4456 - val_loss: 5.1551 - val_mean_absolute_error: 1.1052\n",
            "Epoch 3213/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7719 - mean_absolute_error: 0.4982\n",
            "Epoch 3213: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7719 - mean_absolute_error: 0.4984 - val_loss: 5.8706 - val_mean_absolute_error: 1.1098\n",
            "Epoch 3214/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6210 - mean_absolute_error: 0.4598\n",
            "Epoch 3214: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6209 - mean_absolute_error: 0.4599 - val_loss: 4.9528 - val_mean_absolute_error: 1.0293\n",
            "Epoch 3215/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5742 - mean_absolute_error: 0.4455\n",
            "Epoch 3215: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5743 - mean_absolute_error: 0.4457 - val_loss: 4.7528 - val_mean_absolute_error: 1.0144\n",
            "Epoch 3216/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5996 - mean_absolute_error: 0.4579\n",
            "Epoch 3216: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5996 - mean_absolute_error: 0.4579 - val_loss: 5.0637 - val_mean_absolute_error: 1.0510\n",
            "Epoch 3217/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5850 - mean_absolute_error: 0.4444\n",
            "Epoch 3217: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5847 - mean_absolute_error: 0.4444 - val_loss: 5.5023 - val_mean_absolute_error: 1.0718\n",
            "Epoch 3218/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8658 - mean_absolute_error: 0.5166\n",
            "Epoch 3218: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.8826 - mean_absolute_error: 0.5188 - val_loss: 9.1289 - val_mean_absolute_error: 1.7523\n",
            "Epoch 3219/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2690 - mean_absolute_error: 0.6133\n",
            "Epoch 3219: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2682 - mean_absolute_error: 0.6131 - val_loss: 5.1350 - val_mean_absolute_error: 1.0812\n",
            "Epoch 3220/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6820 - mean_absolute_error: 0.4774\n",
            "Epoch 3220: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6832 - mean_absolute_error: 0.4777 - val_loss: 4.9715 - val_mean_absolute_error: 1.0067\n",
            "Epoch 3221/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7705 - mean_absolute_error: 0.4972\n",
            "Epoch 3221: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7704 - mean_absolute_error: 0.4972 - val_loss: 5.5371 - val_mean_absolute_error: 1.0944\n",
            "Epoch 3222/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6795 - mean_absolute_error: 0.4784\n",
            "Epoch 3222: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6793 - mean_absolute_error: 0.4785 - val_loss: 5.2727 - val_mean_absolute_error: 1.0430\n",
            "Epoch 3223/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5636 - mean_absolute_error: 0.4449\n",
            "Epoch 3223: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5637 - mean_absolute_error: 0.4449 - val_loss: 5.4001 - val_mean_absolute_error: 1.0618\n",
            "Epoch 3224/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5800 - mean_absolute_error: 0.4459\n",
            "Epoch 3224: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5814 - mean_absolute_error: 0.4463 - val_loss: 5.3190 - val_mean_absolute_error: 1.0768\n",
            "Epoch 3225/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6259 - mean_absolute_error: 0.4641\n",
            "Epoch 3225: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6268 - mean_absolute_error: 0.4646 - val_loss: 5.8159 - val_mean_absolute_error: 1.0950\n",
            "Epoch 3226/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6057 - mean_absolute_error: 0.4589\n",
            "Epoch 3226: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6062 - mean_absolute_error: 0.4592 - val_loss: 5.2219 - val_mean_absolute_error: 1.0738\n",
            "Epoch 3227/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5271 - mean_absolute_error: 0.4366\n",
            "Epoch 3227: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5281 - mean_absolute_error: 0.4369 - val_loss: 5.0799 - val_mean_absolute_error: 1.0331\n",
            "Epoch 3228/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5956 - mean_absolute_error: 0.4501\n",
            "Epoch 3228: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5945 - mean_absolute_error: 0.4497 - val_loss: 5.4084 - val_mean_absolute_error: 1.0734\n",
            "Epoch 3229/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6162 - mean_absolute_error: 0.4643\n",
            "Epoch 3229: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6248 - mean_absolute_error: 0.4658 - val_loss: 6.3524 - val_mean_absolute_error: 1.1634\n",
            "Epoch 3230/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6217 - mean_absolute_error: 0.4689\n",
            "Epoch 3230: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6237 - mean_absolute_error: 0.4695 - val_loss: 5.6150 - val_mean_absolute_error: 1.0797\n",
            "Epoch 3231/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6708 - mean_absolute_error: 0.4781\n",
            "Epoch 3231: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6712 - mean_absolute_error: 0.4781 - val_loss: 5.6311 - val_mean_absolute_error: 1.1209\n",
            "Epoch 3232/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6470 - mean_absolute_error: 0.4722\n",
            "Epoch 3232: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6469 - mean_absolute_error: 0.4723 - val_loss: 5.1779 - val_mean_absolute_error: 1.0438\n",
            "Epoch 3233/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6248 - mean_absolute_error: 0.4614\n",
            "Epoch 3233: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6261 - mean_absolute_error: 0.4617 - val_loss: 5.1808 - val_mean_absolute_error: 1.0509\n",
            "Epoch 3234/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7210 - mean_absolute_error: 0.4931\n",
            "Epoch 3234: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7207 - mean_absolute_error: 0.4932 - val_loss: 5.4447 - val_mean_absolute_error: 1.0617\n",
            "Epoch 3235/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6957 - mean_absolute_error: 0.4938\n",
            "Epoch 3235: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6965 - mean_absolute_error: 0.4942 - val_loss: 5.1075 - val_mean_absolute_error: 1.0482\n",
            "Epoch 3236/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6511 - mean_absolute_error: 0.4748\n",
            "Epoch 3236: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6509 - mean_absolute_error: 0.4749 - val_loss: 4.8998 - val_mean_absolute_error: 1.0069\n",
            "Epoch 3237/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5971 - mean_absolute_error: 0.4591\n",
            "Epoch 3237: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5985 - mean_absolute_error: 0.4597 - val_loss: 4.9077 - val_mean_absolute_error: 1.0138\n",
            "Epoch 3238/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5653 - mean_absolute_error: 0.4471\n",
            "Epoch 3238: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5651 - mean_absolute_error: 0.4471 - val_loss: 5.1495 - val_mean_absolute_error: 1.0464\n",
            "Epoch 3239/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5978 - mean_absolute_error: 0.4594\n",
            "Epoch 3239: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6023 - mean_absolute_error: 0.4605 - val_loss: 5.1444 - val_mean_absolute_error: 1.0210\n",
            "Epoch 3240/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7007 - mean_absolute_error: 0.4873\n",
            "Epoch 3240: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7013 - mean_absolute_error: 0.4876 - val_loss: 5.2455 - val_mean_absolute_error: 1.2029\n",
            "Epoch 3241/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6749 - mean_absolute_error: 0.4766\n",
            "Epoch 3241: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6749 - mean_absolute_error: 0.4769 - val_loss: 5.1363 - val_mean_absolute_error: 1.0509\n",
            "Epoch 3242/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6063 - mean_absolute_error: 0.4611\n",
            "Epoch 3242: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6107 - mean_absolute_error: 0.4621 - val_loss: 5.1807 - val_mean_absolute_error: 1.0709\n",
            "Epoch 3243/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9790 - mean_absolute_error: 0.5619\n",
            "Epoch 3243: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9788 - mean_absolute_error: 0.5620 - val_loss: 5.5969 - val_mean_absolute_error: 1.1034\n",
            "Epoch 3244/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6088 - mean_absolute_error: 0.4606\n",
            "Epoch 3244: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6086 - mean_absolute_error: 0.4605 - val_loss: 5.2078 - val_mean_absolute_error: 1.0670\n",
            "Epoch 3245/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6068 - mean_absolute_error: 0.4587\n",
            "Epoch 3245: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6069 - mean_absolute_error: 0.4588 - val_loss: 5.6751 - val_mean_absolute_error: 1.0956\n",
            "Epoch 3246/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5611 - mean_absolute_error: 0.4458\n",
            "Epoch 3246: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5608 - mean_absolute_error: 0.4458 - val_loss: 5.3747 - val_mean_absolute_error: 1.1587\n",
            "Epoch 3247/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5706 - mean_absolute_error: 0.4516\n",
            "Epoch 3247: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5702 - mean_absolute_error: 0.4514 - val_loss: 5.5535 - val_mean_absolute_error: 1.0494\n",
            "Epoch 3248/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5685 - mean_absolute_error: 0.4472\n",
            "Epoch 3248: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5688 - mean_absolute_error: 0.4474 - val_loss: 5.7736 - val_mean_absolute_error: 1.1227\n",
            "Epoch 3249/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6116 - mean_absolute_error: 0.4564\n",
            "Epoch 3249: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6145 - mean_absolute_error: 0.4567 - val_loss: 5.3593 - val_mean_absolute_error: 1.0903\n",
            "Epoch 3250/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6085 - mean_absolute_error: 0.4631\n",
            "Epoch 3250: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6117 - mean_absolute_error: 0.4635 - val_loss: 5.3029 - val_mean_absolute_error: 1.1835\n",
            "Epoch 3251/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6659 - mean_absolute_error: 0.4781\n",
            "Epoch 3251: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6703 - mean_absolute_error: 0.4784 - val_loss: 5.4189 - val_mean_absolute_error: 1.0582\n",
            "Epoch 3252/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7535 - mean_absolute_error: 0.4955\n",
            "Epoch 3252: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7540 - mean_absolute_error: 0.4958 - val_loss: 5.0646 - val_mean_absolute_error: 1.0825\n",
            "Epoch 3253/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6442 - mean_absolute_error: 0.4690\n",
            "Epoch 3253: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6440 - mean_absolute_error: 0.4690 - val_loss: 5.2524 - val_mean_absolute_error: 1.0463\n",
            "Epoch 3254/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5875 - mean_absolute_error: 0.4540\n",
            "Epoch 3254: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5870 - mean_absolute_error: 0.4539 - val_loss: 5.3813 - val_mean_absolute_error: 1.0725\n",
            "Epoch 3255/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6660 - mean_absolute_error: 0.4791\n",
            "Epoch 3255: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6660 - mean_absolute_error: 0.4792 - val_loss: 5.2660 - val_mean_absolute_error: 1.0780\n",
            "Epoch 3256/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6430 - mean_absolute_error: 0.4764\n",
            "Epoch 3256: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6530 - mean_absolute_error: 0.4782 - val_loss: 5.6842 - val_mean_absolute_error: 1.1036\n",
            "Epoch 3257/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.3917 - mean_absolute_error: 0.6133\n",
            "Epoch 3257: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.3933 - mean_absolute_error: 0.6135 - val_loss: 6.0283 - val_mean_absolute_error: 1.1130\n",
            "Epoch 3258/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8679 - mean_absolute_error: 0.5305\n",
            "Epoch 3258: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.8671 - mean_absolute_error: 0.5301 - val_loss: 5.6349 - val_mean_absolute_error: 1.1866\n",
            "Epoch 3259/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6574 - mean_absolute_error: 0.4737\n",
            "Epoch 3259: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6573 - mean_absolute_error: 0.4738 - val_loss: 5.7069 - val_mean_absolute_error: 1.0726\n",
            "Epoch 3260/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5759 - mean_absolute_error: 0.4502\n",
            "Epoch 3260: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5755 - mean_absolute_error: 0.4503 - val_loss: 4.9731 - val_mean_absolute_error: 1.0422\n",
            "Epoch 3261/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5482 - mean_absolute_error: 0.4431\n",
            "Epoch 3261: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5479 - mean_absolute_error: 0.4430 - val_loss: 5.2375 - val_mean_absolute_error: 1.0505\n",
            "Epoch 3262/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5709 - mean_absolute_error: 0.4502\n",
            "Epoch 3262: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5706 - mean_absolute_error: 0.4500 - val_loss: 4.9054 - val_mean_absolute_error: 1.0180\n",
            "Epoch 3263/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6005 - mean_absolute_error: 0.4622\n",
            "Epoch 3263: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6048 - mean_absolute_error: 0.4627 - val_loss: 5.3073 - val_mean_absolute_error: 1.0541\n",
            "Epoch 3264/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5549 - mean_absolute_error: 0.4470\n",
            "Epoch 3264: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5551 - mean_absolute_error: 0.4470 - val_loss: 5.2956 - val_mean_absolute_error: 1.0558\n",
            "Epoch 3265/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5844 - mean_absolute_error: 0.4540\n",
            "Epoch 3265: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5843 - mean_absolute_error: 0.4541 - val_loss: 4.8832 - val_mean_absolute_error: 1.0290\n",
            "Epoch 3266/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6428 - mean_absolute_error: 0.4732\n",
            "Epoch 3266: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6424 - mean_absolute_error: 0.4731 - val_loss: 4.8025 - val_mean_absolute_error: 1.0392\n",
            "Epoch 3267/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6013 - mean_absolute_error: 0.4647\n",
            "Epoch 3267: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6013 - mean_absolute_error: 0.4648 - val_loss: 5.1366 - val_mean_absolute_error: 1.0682\n",
            "Epoch 3268/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6017 - mean_absolute_error: 0.4610\n",
            "Epoch 3268: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6016 - mean_absolute_error: 0.4610 - val_loss: 5.0552 - val_mean_absolute_error: 1.0463\n",
            "Epoch 3269/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6282 - mean_absolute_error: 0.4629\n",
            "Epoch 3269: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6298 - mean_absolute_error: 0.4633 - val_loss: 5.1484 - val_mean_absolute_error: 1.0351\n",
            "Epoch 3270/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6661 - mean_absolute_error: 0.4734\n",
            "Epoch 3270: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6656 - mean_absolute_error: 0.4732 - val_loss: 5.2933 - val_mean_absolute_error: 1.0483\n",
            "Epoch 3271/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.6528 - mean_absolute_error: 0.4714\n",
            "Epoch 3271: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6528 - mean_absolute_error: 0.4714 - val_loss: 5.2901 - val_mean_absolute_error: 1.1677\n",
            "Epoch 3272/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6074 - mean_absolute_error: 0.4611\n",
            "Epoch 3272: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6051 - mean_absolute_error: 0.4604 - val_loss: 5.0012 - val_mean_absolute_error: 1.0381\n",
            "Epoch 3273/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6198 - mean_absolute_error: 0.4642\n",
            "Epoch 3273: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6202 - mean_absolute_error: 0.4644 - val_loss: 5.5157 - val_mean_absolute_error: 1.1688\n",
            "Epoch 3274/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2944 - mean_absolute_error: 0.5918\n",
            "Epoch 3274: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2936 - mean_absolute_error: 0.5916 - val_loss: 5.0422 - val_mean_absolute_error: 1.1615\n",
            "Epoch 3275/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7478 - mean_absolute_error: 0.4923\n",
            "Epoch 3275: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7537 - mean_absolute_error: 0.4929 - val_loss: 4.8313 - val_mean_absolute_error: 1.0258\n",
            "Epoch 3276/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1660 - mean_absolute_error: 0.5810\n",
            "Epoch 3276: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1652 - mean_absolute_error: 0.5809 - val_loss: 5.1876 - val_mean_absolute_error: 1.0911\n",
            "Epoch 3277/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6388 - mean_absolute_error: 0.4666\n",
            "Epoch 3277: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6449 - mean_absolute_error: 0.4676 - val_loss: 4.7446 - val_mean_absolute_error: 1.0202\n",
            "Epoch 3278/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.8336 - mean_absolute_error: 0.5138\n",
            "Epoch 3278: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.8336 - mean_absolute_error: 0.5138 - val_loss: 5.2071 - val_mean_absolute_error: 1.0601\n",
            "Epoch 3279/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7028 - mean_absolute_error: 0.4746\n",
            "Epoch 3279: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7015 - mean_absolute_error: 0.4745 - val_loss: 6.1367 - val_mean_absolute_error: 1.2727\n",
            "Epoch 3280/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6048 - mean_absolute_error: 0.4589\n",
            "Epoch 3280: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6048 - mean_absolute_error: 0.4591 - val_loss: 4.9690 - val_mean_absolute_error: 1.0541\n",
            "Epoch 3281/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5744 - mean_absolute_error: 0.4494\n",
            "Epoch 3281: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5741 - mean_absolute_error: 0.4494 - val_loss: 5.6538 - val_mean_absolute_error: 1.1052\n",
            "Epoch 3282/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5772 - mean_absolute_error: 0.4459\n",
            "Epoch 3282: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5788 - mean_absolute_error: 0.4463 - val_loss: 5.1932 - val_mean_absolute_error: 1.0288\n",
            "Epoch 3283/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5816 - mean_absolute_error: 0.4530\n",
            "Epoch 3283: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5822 - mean_absolute_error: 0.4534 - val_loss: 5.0013 - val_mean_absolute_error: 1.0145\n",
            "Epoch 3284/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5635 - mean_absolute_error: 0.4447\n",
            "Epoch 3284: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5658 - mean_absolute_error: 0.4449 - val_loss: 5.1228 - val_mean_absolute_error: 1.0839\n",
            "Epoch 3285/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5879 - mean_absolute_error: 0.4555\n",
            "Epoch 3285: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5879 - mean_absolute_error: 0.4556 - val_loss: 5.1132 - val_mean_absolute_error: 1.0375\n",
            "Epoch 3286/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5981 - mean_absolute_error: 0.4539\n",
            "Epoch 3286: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6032 - mean_absolute_error: 0.4547 - val_loss: 5.1056 - val_mean_absolute_error: 1.0846\n",
            "Epoch 3287/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0244 - mean_absolute_error: 0.5685\n",
            "Epoch 3287: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0235 - mean_absolute_error: 0.5683 - val_loss: 5.1843 - val_mean_absolute_error: 1.0587\n",
            "Epoch 3288/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6965 - mean_absolute_error: 0.4783\n",
            "Epoch 3288: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6966 - mean_absolute_error: 0.4785 - val_loss: 6.0157 - val_mean_absolute_error: 1.1316\n",
            "Epoch 3289/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7881 - mean_absolute_error: 0.5100\n",
            "Epoch 3289: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7893 - mean_absolute_error: 0.5101 - val_loss: 4.7928 - val_mean_absolute_error: 1.0426\n",
            "Epoch 3290/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7711 - mean_absolute_error: 0.5027\n",
            "Epoch 3290: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7712 - mean_absolute_error: 0.5029 - val_loss: 5.2492 - val_mean_absolute_error: 1.1036\n",
            "Epoch 3291/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6052 - mean_absolute_error: 0.4575\n",
            "Epoch 3291: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6048 - mean_absolute_error: 0.4573 - val_loss: 4.8100 - val_mean_absolute_error: 1.0694\n",
            "Epoch 3292/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5966 - mean_absolute_error: 0.4542\n",
            "Epoch 3292: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5963 - mean_absolute_error: 0.4542 - val_loss: 5.1811 - val_mean_absolute_error: 1.0499\n",
            "Epoch 3293/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5874 - mean_absolute_error: 0.4503\n",
            "Epoch 3293: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5873 - mean_absolute_error: 0.4504 - val_loss: 5.3065 - val_mean_absolute_error: 1.0609\n",
            "Epoch 3294/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5987 - mean_absolute_error: 0.4557\n",
            "Epoch 3294: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5990 - mean_absolute_error: 0.4561 - val_loss: 4.8489 - val_mean_absolute_error: 1.0045\n",
            "Epoch 3295/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6065 - mean_absolute_error: 0.4613\n",
            "Epoch 3295: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6076 - mean_absolute_error: 0.4617 - val_loss: 5.5609 - val_mean_absolute_error: 1.1330\n",
            "Epoch 3296/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7229 - mean_absolute_error: 0.4937\n",
            "Epoch 3296: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7246 - mean_absolute_error: 0.4941 - val_loss: 4.9960 - val_mean_absolute_error: 1.0188\n",
            "Epoch 3297/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6645 - mean_absolute_error: 0.4701\n",
            "Epoch 3297: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6645 - mean_absolute_error: 0.4702 - val_loss: 5.4698 - val_mean_absolute_error: 1.1208\n",
            "Epoch 3298/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6259 - mean_absolute_error: 0.4677\n",
            "Epoch 3298: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6288 - mean_absolute_error: 0.4681 - val_loss: 5.0169 - val_mean_absolute_error: 1.1430\n",
            "Epoch 3299/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1354 - mean_absolute_error: 0.5696\n",
            "Epoch 3299: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1355 - mean_absolute_error: 0.5699 - val_loss: 5.4386 - val_mean_absolute_error: 1.1513\n",
            "Epoch 3300/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5821 - mean_absolute_error: 0.4502\n",
            "Epoch 3300: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5828 - mean_absolute_error: 0.4506 - val_loss: 5.3489 - val_mean_absolute_error: 1.1020\n",
            "Epoch 3301/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5896 - mean_absolute_error: 0.4593\n",
            "Epoch 3301: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5906 - mean_absolute_error: 0.4596 - val_loss: 5.2276 - val_mean_absolute_error: 1.0825\n",
            "Epoch 3302/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6045 - mean_absolute_error: 0.4512\n",
            "Epoch 3302: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6040 - mean_absolute_error: 0.4510 - val_loss: 5.0231 - val_mean_absolute_error: 1.0865\n",
            "Epoch 3303/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5356 - mean_absolute_error: 0.4404\n",
            "Epoch 3303: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5357 - mean_absolute_error: 0.4403 - val_loss: 5.2313 - val_mean_absolute_error: 1.0746\n",
            "Epoch 3304/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5937 - mean_absolute_error: 0.4584\n",
            "Epoch 3304: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5937 - mean_absolute_error: 0.4583 - val_loss: 4.9477 - val_mean_absolute_error: 1.0412\n",
            "Epoch 3305/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5777 - mean_absolute_error: 0.4492\n",
            "Epoch 3305: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5780 - mean_absolute_error: 0.4494 - val_loss: 5.0800 - val_mean_absolute_error: 1.0510\n",
            "Epoch 3306/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5968 - mean_absolute_error: 0.4563\n",
            "Epoch 3306: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5963 - mean_absolute_error: 0.4562 - val_loss: 5.3811 - val_mean_absolute_error: 1.0719\n",
            "Epoch 3307/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6310 - mean_absolute_error: 0.4762\n",
            "Epoch 3307: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6316 - mean_absolute_error: 0.4764 - val_loss: 5.3014 - val_mean_absolute_error: 1.0717\n",
            "Epoch 3308/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6099 - mean_absolute_error: 0.4632\n",
            "Epoch 3308: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.6100 - mean_absolute_error: 0.4633 - val_loss: 5.0138 - val_mean_absolute_error: 1.0312\n",
            "Epoch 3309/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5970 - mean_absolute_error: 0.4657\n",
            "Epoch 3309: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5980 - mean_absolute_error: 0.4660 - val_loss: 4.8822 - val_mean_absolute_error: 1.0199\n",
            "Epoch 3310/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6126 - mean_absolute_error: 0.4568\n",
            "Epoch 3310: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6122 - mean_absolute_error: 0.4567 - val_loss: 5.2796 - val_mean_absolute_error: 1.1267\n",
            "Epoch 3311/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.7026 - mean_absolute_error: 0.4882\n",
            "Epoch 3311: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.7026 - mean_absolute_error: 0.4882 - val_loss: 5.3065 - val_mean_absolute_error: 1.0212\n",
            "Epoch 3312/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6038 - mean_absolute_error: 0.4555\n",
            "Epoch 3312: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6079 - mean_absolute_error: 0.4562 - val_loss: 5.5776 - val_mean_absolute_error: 1.0810\n",
            "Epoch 3313/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7591 - mean_absolute_error: 0.5034\n",
            "Epoch 3313: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7591 - mean_absolute_error: 0.5035 - val_loss: 5.1410 - val_mean_absolute_error: 1.0126\n",
            "Epoch 3314/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5993 - mean_absolute_error: 0.4522\n",
            "Epoch 3314: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6002 - mean_absolute_error: 0.4524 - val_loss: 5.2389 - val_mean_absolute_error: 1.0226\n",
            "Epoch 3315/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5935 - mean_absolute_error: 0.4596\n",
            "Epoch 3315: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5936 - mean_absolute_error: 0.4597 - val_loss: 5.1722 - val_mean_absolute_error: 1.0616\n",
            "Epoch 3316/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5952 - mean_absolute_error: 0.4565\n",
            "Epoch 3316: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5965 - mean_absolute_error: 0.4569 - val_loss: 6.3019 - val_mean_absolute_error: 1.1747\n",
            "Epoch 3317/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6807 - mean_absolute_error: 0.4821\n",
            "Epoch 3317: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6831 - mean_absolute_error: 0.4830 - val_loss: 5.4480 - val_mean_absolute_error: 1.0521\n",
            "Epoch 3318/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6562 - mean_absolute_error: 0.4738\n",
            "Epoch 3318: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6571 - mean_absolute_error: 0.4744 - val_loss: 5.5412 - val_mean_absolute_error: 1.1404\n",
            "Epoch 3319/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6085 - mean_absolute_error: 0.4644\n",
            "Epoch 3319: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6091 - mean_absolute_error: 0.4649 - val_loss: 4.7459 - val_mean_absolute_error: 1.0431\n",
            "Epoch 3320/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.4573 - mean_absolute_error: 0.6238\n",
            "Epoch 3320: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.4574 - mean_absolute_error: 0.6241 - val_loss: 5.6073 - val_mean_absolute_error: 1.1225\n",
            "Epoch 3321/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0990 - mean_absolute_error: 0.5717\n",
            "Epoch 3321: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.0984 - mean_absolute_error: 0.5716 - val_loss: 5.4046 - val_mean_absolute_error: 1.0866\n",
            "Epoch 3322/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6306 - mean_absolute_error: 0.4696\n",
            "Epoch 3322: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6308 - mean_absolute_error: 0.4697 - val_loss: 4.9497 - val_mean_absolute_error: 1.0301\n",
            "Epoch 3323/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5559 - mean_absolute_error: 0.4429\n",
            "Epoch 3323: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5557 - mean_absolute_error: 0.4429 - val_loss: 5.1753 - val_mean_absolute_error: 1.0660\n",
            "Epoch 3324/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6077 - mean_absolute_error: 0.4569\n",
            "Epoch 3324: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6073 - mean_absolute_error: 0.4568 - val_loss: 5.3716 - val_mean_absolute_error: 1.0642\n",
            "Epoch 3325/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5643 - mean_absolute_error: 0.4406\n",
            "Epoch 3325: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5643 - mean_absolute_error: 0.4406 - val_loss: 4.8785 - val_mean_absolute_error: 1.0063\n",
            "Epoch 3326/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5495 - mean_absolute_error: 0.4458\n",
            "Epoch 3326: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5493 - mean_absolute_error: 0.4458 - val_loss: 5.3167 - val_mean_absolute_error: 1.0508\n",
            "Epoch 3327/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5780 - mean_absolute_error: 0.4560\n",
            "Epoch 3327: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5776 - mean_absolute_error: 0.4559 - val_loss: 4.9536 - val_mean_absolute_error: 1.0299\n",
            "Epoch 3328/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5464 - mean_absolute_error: 0.4408\n",
            "Epoch 3328: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5468 - mean_absolute_error: 0.4410 - val_loss: 5.1313 - val_mean_absolute_error: 1.1022\n",
            "Epoch 3329/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6075 - mean_absolute_error: 0.4605\n",
            "Epoch 3329: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6077 - mean_absolute_error: 0.4609 - val_loss: 5.0924 - val_mean_absolute_error: 1.0338\n",
            "Epoch 3330/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7151 - mean_absolute_error: 0.4830\n",
            "Epoch 3330: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7149 - mean_absolute_error: 0.4830 - val_loss: 5.4250 - val_mean_absolute_error: 1.0603\n",
            "Epoch 3331/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6146 - mean_absolute_error: 0.4646\n",
            "Epoch 3331: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6146 - mean_absolute_error: 0.4648 - val_loss: 5.2555 - val_mean_absolute_error: 1.0363\n",
            "Epoch 3332/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5638 - mean_absolute_error: 0.4530\n",
            "Epoch 3332: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5636 - mean_absolute_error: 0.4530 - val_loss: 4.9512 - val_mean_absolute_error: 1.0190\n",
            "Epoch 3333/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6145 - mean_absolute_error: 0.4605\n",
            "Epoch 3333: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6150 - mean_absolute_error: 0.4608 - val_loss: 4.9043 - val_mean_absolute_error: 1.0200\n",
            "Epoch 3334/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5778 - mean_absolute_error: 0.4505\n",
            "Epoch 3334: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5777 - mean_absolute_error: 0.4505 - val_loss: 4.9354 - val_mean_absolute_error: 1.0235\n",
            "Epoch 3335/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5946 - mean_absolute_error: 0.4622\n",
            "Epoch 3335: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5982 - mean_absolute_error: 0.4632 - val_loss: 5.2283 - val_mean_absolute_error: 1.1137\n",
            "Epoch 3336/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6018 - mean_absolute_error: 0.4586\n",
            "Epoch 3336: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6223 - mean_absolute_error: 0.4599 - val_loss: 5.2529 - val_mean_absolute_error: 1.0800\n",
            "Epoch 3337/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 5.0166 - mean_absolute_error: 1.0884\n",
            "Epoch 3337: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 5.0182 - mean_absolute_error: 1.0892 - val_loss: 6.1774 - val_mean_absolute_error: 1.3030\n",
            "Epoch 3338/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 2.4839 - mean_absolute_error: 0.7811\n",
            "Epoch 3338: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 2.4834 - mean_absolute_error: 0.7814 - val_loss: 5.3564 - val_mean_absolute_error: 1.0896\n",
            "Epoch 3339/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.4517 - mean_absolute_error: 0.6351\n",
            "Epoch 3339: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.4547 - mean_absolute_error: 0.6357 - val_loss: 5.1466 - val_mean_absolute_error: 1.0587\n",
            "Epoch 3340/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0233 - mean_absolute_error: 0.5610\n",
            "Epoch 3340: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0228 - mean_absolute_error: 0.5609 - val_loss: 5.5646 - val_mean_absolute_error: 1.1039\n",
            "Epoch 3341/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8200 - mean_absolute_error: 0.5137\n",
            "Epoch 3341: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8209 - mean_absolute_error: 0.5140 - val_loss: 5.2094 - val_mean_absolute_error: 1.0408\n",
            "Epoch 3342/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7129 - mean_absolute_error: 0.4852\n",
            "Epoch 3342: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7136 - mean_absolute_error: 0.4857 - val_loss: 5.0870 - val_mean_absolute_error: 1.0344\n",
            "Epoch 3343/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6113 - mean_absolute_error: 0.4625\n",
            "Epoch 3343: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6121 - mean_absolute_error: 0.4628 - val_loss: 5.2229 - val_mean_absolute_error: 1.0463\n",
            "Epoch 3344/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6022 - mean_absolute_error: 0.4610\n",
            "Epoch 3344: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6033 - mean_absolute_error: 0.4614 - val_loss: 5.2851 - val_mean_absolute_error: 1.0405\n",
            "Epoch 3345/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5610 - mean_absolute_error: 0.4410\n",
            "Epoch 3345: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5645 - mean_absolute_error: 0.4417 - val_loss: 6.1097 - val_mean_absolute_error: 1.1519\n",
            "Epoch 3346/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7365 - mean_absolute_error: 0.4854\n",
            "Epoch 3346: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7361 - mean_absolute_error: 0.4854 - val_loss: 4.9301 - val_mean_absolute_error: 1.0369\n",
            "Epoch 3347/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5443 - mean_absolute_error: 0.4437\n",
            "Epoch 3347: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5453 - mean_absolute_error: 0.4441 - val_loss: 4.7336 - val_mean_absolute_error: 1.0253\n",
            "Epoch 3348/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5340 - mean_absolute_error: 0.4377\n",
            "Epoch 3348: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5342 - mean_absolute_error: 0.4379 - val_loss: 4.6734 - val_mean_absolute_error: 0.9935\n",
            "Epoch 3349/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5545 - mean_absolute_error: 0.4404\n",
            "Epoch 3349: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5542 - mean_absolute_error: 0.4403 - val_loss: 4.9810 - val_mean_absolute_error: 1.0379\n",
            "Epoch 3350/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5444 - mean_absolute_error: 0.4447\n",
            "Epoch 3350: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5448 - mean_absolute_error: 0.4448 - val_loss: 5.1475 - val_mean_absolute_error: 1.0667\n",
            "Epoch 3351/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5894 - mean_absolute_error: 0.4550\n",
            "Epoch 3351: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5899 - mean_absolute_error: 0.4552 - val_loss: 5.1682 - val_mean_absolute_error: 1.0626\n",
            "Epoch 3352/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5164 - mean_absolute_error: 0.4368\n",
            "Epoch 3352: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5168 - mean_absolute_error: 0.4369 - val_loss: 5.0911 - val_mean_absolute_error: 1.0589\n",
            "Epoch 3353/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5499 - mean_absolute_error: 0.4389\n",
            "Epoch 3353: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5512 - mean_absolute_error: 0.4391 - val_loss: 5.1062 - val_mean_absolute_error: 1.0763\n",
            "Epoch 3354/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5954 - mean_absolute_error: 0.4559\n",
            "Epoch 3354: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5951 - mean_absolute_error: 0.4557 - val_loss: 5.0238 - val_mean_absolute_error: 1.0628\n",
            "Epoch 3355/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5559 - mean_absolute_error: 0.4432\n",
            "Epoch 3355: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5556 - mean_absolute_error: 0.4431 - val_loss: 5.3892 - val_mean_absolute_error: 1.0810\n",
            "Epoch 3356/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5578 - mean_absolute_error: 0.4480\n",
            "Epoch 3356: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5586 - mean_absolute_error: 0.4481 - val_loss: 4.9916 - val_mean_absolute_error: 1.0581\n",
            "Epoch 3357/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6032 - mean_absolute_error: 0.4608\n",
            "Epoch 3357: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6031 - mean_absolute_error: 0.4609 - val_loss: 5.4548 - val_mean_absolute_error: 1.0941\n",
            "Epoch 3358/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5535 - mean_absolute_error: 0.4389\n",
            "Epoch 3358: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5538 - mean_absolute_error: 0.4389 - val_loss: 5.0310 - val_mean_absolute_error: 1.0471\n",
            "Epoch 3359/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5616 - mean_absolute_error: 0.4464\n",
            "Epoch 3359: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5641 - mean_absolute_error: 0.4467 - val_loss: 5.3080 - val_mean_absolute_error: 1.0772\n",
            "Epoch 3360/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5712 - mean_absolute_error: 0.4544\n",
            "Epoch 3360: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5713 - mean_absolute_error: 0.4545 - val_loss: 5.0390 - val_mean_absolute_error: 1.0931\n",
            "Epoch 3361/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6216 - mean_absolute_error: 0.4629\n",
            "Epoch 3361: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6221 - mean_absolute_error: 0.4631 - val_loss: 5.1828 - val_mean_absolute_error: 1.0724\n",
            "Epoch 3362/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5827 - mean_absolute_error: 0.4590\n",
            "Epoch 3362: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5824 - mean_absolute_error: 0.4588 - val_loss: 5.1219 - val_mean_absolute_error: 1.1131\n",
            "Epoch 3363/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5481 - mean_absolute_error: 0.4421\n",
            "Epoch 3363: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5482 - mean_absolute_error: 0.4423 - val_loss: 4.9907 - val_mean_absolute_error: 1.0813\n",
            "Epoch 3364/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5726 - mean_absolute_error: 0.4557\n",
            "Epoch 3364: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5725 - mean_absolute_error: 0.4556 - val_loss: 5.1524 - val_mean_absolute_error: 1.0600\n",
            "Epoch 3365/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7286 - mean_absolute_error: 0.4882\n",
            "Epoch 3365: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7359 - mean_absolute_error: 0.4889 - val_loss: 6.7929 - val_mean_absolute_error: 1.2046\n",
            "Epoch 3366/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.3227 - mean_absolute_error: 0.6100\n",
            "Epoch 3366: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.3222 - mean_absolute_error: 0.6101 - val_loss: 4.7350 - val_mean_absolute_error: 1.0292\n",
            "Epoch 3367/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6001 - mean_absolute_error: 0.4581\n",
            "Epoch 3367: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6000 - mean_absolute_error: 0.4581 - val_loss: 4.8784 - val_mean_absolute_error: 1.0546\n",
            "Epoch 3368/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5796 - mean_absolute_error: 0.4534\n",
            "Epoch 3368: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5841 - mean_absolute_error: 0.4538 - val_loss: 4.7893 - val_mean_absolute_error: 1.0097\n",
            "Epoch 3369/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2561 - mean_absolute_error: 0.5911\n",
            "Epoch 3369: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2556 - mean_absolute_error: 0.5911 - val_loss: 5.0451 - val_mean_absolute_error: 1.0516\n",
            "Epoch 3370/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5978 - mean_absolute_error: 0.4572\n",
            "Epoch 3370: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5975 - mean_absolute_error: 0.4571 - val_loss: 4.6780 - val_mean_absolute_error: 1.0024\n",
            "Epoch 3371/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6146 - mean_absolute_error: 0.4616\n",
            "Epoch 3371: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6154 - mean_absolute_error: 0.4621 - val_loss: 4.8218 - val_mean_absolute_error: 0.9981\n",
            "Epoch 3372/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5256 - mean_absolute_error: 0.4327\n",
            "Epoch 3372: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5257 - mean_absolute_error: 0.4328 - val_loss: 4.6581 - val_mean_absolute_error: 1.0188\n",
            "Epoch 3373/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5521 - mean_absolute_error: 0.4443\n",
            "Epoch 3373: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5519 - mean_absolute_error: 0.4444 - val_loss: 5.3198 - val_mean_absolute_error: 1.0672\n",
            "Epoch 3374/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5409 - mean_absolute_error: 0.4435\n",
            "Epoch 3374: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5409 - mean_absolute_error: 0.4437 - val_loss: 5.3395 - val_mean_absolute_error: 1.1511\n",
            "Epoch 3375/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5683 - mean_absolute_error: 0.4503\n",
            "Epoch 3375: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5702 - mean_absolute_error: 0.4507 - val_loss: 5.0529 - val_mean_absolute_error: 1.0724\n",
            "Epoch 3376/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7220 - mean_absolute_error: 0.4836\n",
            "Epoch 3376: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7216 - mean_absolute_error: 0.4835 - val_loss: 5.1291 - val_mean_absolute_error: 1.0510\n",
            "Epoch 3377/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5733 - mean_absolute_error: 0.4522\n",
            "Epoch 3377: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5729 - mean_absolute_error: 0.4520 - val_loss: 5.3462 - val_mean_absolute_error: 1.0558\n",
            "Epoch 3378/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5785 - mean_absolute_error: 0.4543\n",
            "Epoch 3378: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5791 - mean_absolute_error: 0.4545 - val_loss: 5.3288 - val_mean_absolute_error: 1.0703\n",
            "Epoch 3379/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5734 - mean_absolute_error: 0.4476\n",
            "Epoch 3379: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5730 - mean_absolute_error: 0.4474 - val_loss: 5.2635 - val_mean_absolute_error: 1.0778\n",
            "Epoch 3380/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5922 - mean_absolute_error: 0.4610\n",
            "Epoch 3380: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5915 - mean_absolute_error: 0.4610 - val_loss: 5.1751 - val_mean_absolute_error: 1.0300\n",
            "Epoch 3381/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6004 - mean_absolute_error: 0.4613\n",
            "Epoch 3381: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6058 - mean_absolute_error: 0.4620 - val_loss: 5.0587 - val_mean_absolute_error: 1.0204\n",
            "Epoch 3382/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7044 - mean_absolute_error: 0.4830\n",
            "Epoch 3382: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7042 - mean_absolute_error: 0.4830 - val_loss: 5.3287 - val_mean_absolute_error: 1.0460\n",
            "Epoch 3383/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5592 - mean_absolute_error: 0.4425\n",
            "Epoch 3383: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5589 - mean_absolute_error: 0.4425 - val_loss: 5.4856 - val_mean_absolute_error: 1.1574\n",
            "Epoch 3384/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5944 - mean_absolute_error: 0.4613\n",
            "Epoch 3384: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5945 - mean_absolute_error: 0.4614 - val_loss: 5.1137 - val_mean_absolute_error: 1.0484\n",
            "Epoch 3385/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5970 - mean_absolute_error: 0.4568\n",
            "Epoch 3385: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5968 - mean_absolute_error: 0.4568 - val_loss: 5.2452 - val_mean_absolute_error: 1.0563\n",
            "Epoch 3386/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6233 - mean_absolute_error: 0.4688\n",
            "Epoch 3386: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6239 - mean_absolute_error: 0.4689 - val_loss: 5.0824 - val_mean_absolute_error: 1.0663\n",
            "Epoch 3387/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6519 - mean_absolute_error: 0.4804\n",
            "Epoch 3387: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6515 - mean_absolute_error: 0.4802 - val_loss: 5.1027 - val_mean_absolute_error: 1.0568\n",
            "Epoch 3388/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5988 - mean_absolute_error: 0.4614\n",
            "Epoch 3388: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5984 - mean_absolute_error: 0.4612 - val_loss: 5.2532 - val_mean_absolute_error: 1.1152\n",
            "Epoch 3389/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9823 - mean_absolute_error: 0.5477\n",
            "Epoch 3389: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9827 - mean_absolute_error: 0.5480 - val_loss: 4.9682 - val_mean_absolute_error: 1.1475\n",
            "Epoch 3390/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.6741 - mean_absolute_error: 0.4818\n",
            "Epoch 3390: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6741 - mean_absolute_error: 0.4818 - val_loss: 5.3284 - val_mean_absolute_error: 1.1127\n",
            "Epoch 3391/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5891 - mean_absolute_error: 0.4532\n",
            "Epoch 3391: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5889 - mean_absolute_error: 0.4533 - val_loss: 5.2507 - val_mean_absolute_error: 1.0740\n",
            "Epoch 3392/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5691 - mean_absolute_error: 0.4539\n",
            "Epoch 3392: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5698 - mean_absolute_error: 0.4543 - val_loss: 5.0890 - val_mean_absolute_error: 1.0648\n",
            "Epoch 3393/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5453 - mean_absolute_error: 0.4423\n",
            "Epoch 3393: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5450 - mean_absolute_error: 0.4422 - val_loss: 4.7750 - val_mean_absolute_error: 1.0216\n",
            "Epoch 3394/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6230 - mean_absolute_error: 0.4613\n",
            "Epoch 3394: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6221 - mean_absolute_error: 0.4612 - val_loss: 5.5800 - val_mean_absolute_error: 1.0938\n",
            "Epoch 3395/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5589 - mean_absolute_error: 0.4452\n",
            "Epoch 3395: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5599 - mean_absolute_error: 0.4455 - val_loss: 5.1797 - val_mean_absolute_error: 1.0652\n",
            "Epoch 3396/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6232 - mean_absolute_error: 0.4738\n",
            "Epoch 3396: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6231 - mean_absolute_error: 0.4739 - val_loss: 5.1734 - val_mean_absolute_error: 1.0738\n",
            "Epoch 3397/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5761 - mean_absolute_error: 0.4566\n",
            "Epoch 3397: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5770 - mean_absolute_error: 0.4573 - val_loss: 5.1772 - val_mean_absolute_error: 1.0525\n",
            "Epoch 3398/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6014 - mean_absolute_error: 0.4567\n",
            "Epoch 3398: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6019 - mean_absolute_error: 0.4570 - val_loss: 4.6937 - val_mean_absolute_error: 1.0054\n",
            "Epoch 3399/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5913 - mean_absolute_error: 0.4531\n",
            "Epoch 3399: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5915 - mean_absolute_error: 0.4531 - val_loss: 4.9480 - val_mean_absolute_error: 1.0253\n",
            "Epoch 3400/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6337 - mean_absolute_error: 0.4697\n",
            "Epoch 3400: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6333 - mean_absolute_error: 0.4696 - val_loss: 5.3706 - val_mean_absolute_error: 1.0619\n",
            "Epoch 3401/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9589 - mean_absolute_error: 0.5251\n",
            "Epoch 3401: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9597 - mean_absolute_error: 0.5255 - val_loss: 5.4640 - val_mean_absolute_error: 1.0621\n",
            "Epoch 3402/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6262 - mean_absolute_error: 0.4622\n",
            "Epoch 3402: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6258 - mean_absolute_error: 0.4621 - val_loss: 5.2575 - val_mean_absolute_error: 1.1012\n",
            "Epoch 3403/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5741 - mean_absolute_error: 0.4480\n",
            "Epoch 3403: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5744 - mean_absolute_error: 0.4482 - val_loss: 5.6424 - val_mean_absolute_error: 1.1229\n",
            "Epoch 3404/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5726 - mean_absolute_error: 0.4458\n",
            "Epoch 3404: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5726 - mean_absolute_error: 0.4458 - val_loss: 4.9984 - val_mean_absolute_error: 1.0570\n",
            "Epoch 3405/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.9179 - mean_absolute_error: 0.6840\n",
            "Epoch 3405: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.9167 - mean_absolute_error: 0.6839 - val_loss: 8.6106 - val_mean_absolute_error: 1.6264\n",
            "Epoch 3406/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.4916 - mean_absolute_error: 0.6309\n",
            "Epoch 3406: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.4995 - mean_absolute_error: 0.6316 - val_loss: 5.8375 - val_mean_absolute_error: 1.1152\n",
            "Epoch 3407/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9802 - mean_absolute_error: 0.5230\n",
            "Epoch 3407: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9801 - mean_absolute_error: 0.5232 - val_loss: 6.1118 - val_mean_absolute_error: 1.1263\n",
            "Epoch 3408/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8164 - mean_absolute_error: 0.4927\n",
            "Epoch 3408: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8168 - mean_absolute_error: 0.4930 - val_loss: 5.7249 - val_mean_absolute_error: 1.1067\n",
            "Epoch 3409/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7955 - mean_absolute_error: 0.4915\n",
            "Epoch 3409: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7955 - mean_absolute_error: 0.4916 - val_loss: 4.8105 - val_mean_absolute_error: 1.0404\n",
            "Epoch 3410/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6802 - mean_absolute_error: 0.4606\n",
            "Epoch 3410: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6798 - mean_absolute_error: 0.4605 - val_loss: 4.8866 - val_mean_absolute_error: 1.0312\n",
            "Epoch 3411/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6113 - mean_absolute_error: 0.4410\n",
            "Epoch 3411: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6125 - mean_absolute_error: 0.4413 - val_loss: 5.1038 - val_mean_absolute_error: 1.0581\n",
            "Epoch 3412/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7985 - mean_absolute_error: 0.4871\n",
            "Epoch 3412: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7986 - mean_absolute_error: 0.4873 - val_loss: 5.0020 - val_mean_absolute_error: 1.0413\n",
            "Epoch 3413/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6398 - mean_absolute_error: 0.4576\n",
            "Epoch 3413: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6402 - mean_absolute_error: 0.4579 - val_loss: 5.1592 - val_mean_absolute_error: 1.0782\n",
            "Epoch 3414/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6733 - mean_absolute_error: 0.4749\n",
            "Epoch 3414: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6734 - mean_absolute_error: 0.4749 - val_loss: 4.9574 - val_mean_absolute_error: 1.0389\n",
            "Epoch 3415/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5604 - mean_absolute_error: 0.4394\n",
            "Epoch 3415: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5606 - mean_absolute_error: 0.4396 - val_loss: 5.0868 - val_mean_absolute_error: 1.0545\n",
            "Epoch 3416/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5615 - mean_absolute_error: 0.4414\n",
            "Epoch 3416: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5630 - mean_absolute_error: 0.4419 - val_loss: 5.3350 - val_mean_absolute_error: 1.0742\n",
            "Epoch 3417/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5816 - mean_absolute_error: 0.4556\n",
            "Epoch 3417: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5816 - mean_absolute_error: 0.4556 - val_loss: 5.3495 - val_mean_absolute_error: 1.0921\n",
            "Epoch 3418/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5452 - mean_absolute_error: 0.4427\n",
            "Epoch 3418: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5450 - mean_absolute_error: 0.4426 - val_loss: 5.2440 - val_mean_absolute_error: 1.0853\n",
            "Epoch 3419/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5721 - mean_absolute_error: 0.4532\n",
            "Epoch 3419: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5716 - mean_absolute_error: 0.4531 - val_loss: 4.9215 - val_mean_absolute_error: 1.0377\n",
            "Epoch 3420/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7479 - mean_absolute_error: 0.4868\n",
            "Epoch 3420: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7496 - mean_absolute_error: 0.4877 - val_loss: 5.5093 - val_mean_absolute_error: 1.0767\n",
            "Epoch 3421/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7257 - mean_absolute_error: 0.4934\n",
            "Epoch 3421: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7252 - mean_absolute_error: 0.4932 - val_loss: 5.2482 - val_mean_absolute_error: 1.0823\n",
            "Epoch 3422/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6115 - mean_absolute_error: 0.4640\n",
            "Epoch 3422: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6138 - mean_absolute_error: 0.4647 - val_loss: 5.2614 - val_mean_absolute_error: 1.0616\n",
            "Epoch 3423/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6020 - mean_absolute_error: 0.4569\n",
            "Epoch 3423: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6049 - mean_absolute_error: 0.4580 - val_loss: 5.4795 - val_mean_absolute_error: 1.0709\n",
            "Epoch 3424/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5716 - mean_absolute_error: 0.4555\n",
            "Epoch 3424: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5715 - mean_absolute_error: 0.4556 - val_loss: 5.5649 - val_mean_absolute_error: 1.0908\n",
            "Epoch 3425/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5881 - mean_absolute_error: 0.4583\n",
            "Epoch 3425: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5895 - mean_absolute_error: 0.4586 - val_loss: 5.2212 - val_mean_absolute_error: 1.0779\n",
            "Epoch 3426/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6527 - mean_absolute_error: 0.4709\n",
            "Epoch 3426: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6523 - mean_absolute_error: 0.4709 - val_loss: 5.2415 - val_mean_absolute_error: 1.0513\n",
            "Epoch 3427/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7090 - mean_absolute_error: 0.4828\n",
            "Epoch 3427: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7101 - mean_absolute_error: 0.4833 - val_loss: 5.0601 - val_mean_absolute_error: 1.0582\n",
            "Epoch 3428/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5647 - mean_absolute_error: 0.4527\n",
            "Epoch 3428: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5647 - mean_absolute_error: 0.4528 - val_loss: 5.2365 - val_mean_absolute_error: 1.0455\n",
            "Epoch 3429/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8381 - mean_absolute_error: 0.5079\n",
            "Epoch 3429: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8396 - mean_absolute_error: 0.5085 - val_loss: 5.4339 - val_mean_absolute_error: 1.0621\n",
            "Epoch 3430/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7824 - mean_absolute_error: 0.5049\n",
            "Epoch 3430: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7824 - mean_absolute_error: 0.5050 - val_loss: 4.9486 - val_mean_absolute_error: 1.0548\n",
            "Epoch 3431/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6298 - mean_absolute_error: 0.4627\n",
            "Epoch 3431: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6294 - mean_absolute_error: 0.4626 - val_loss: 5.2891 - val_mean_absolute_error: 1.0872\n",
            "Epoch 3432/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5592 - mean_absolute_error: 0.4428\n",
            "Epoch 3432: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5593 - mean_absolute_error: 0.4429 - val_loss: 4.9853 - val_mean_absolute_error: 1.0289\n",
            "Epoch 3433/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6928 - mean_absolute_error: 0.4713\n",
            "Epoch 3433: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7027 - mean_absolute_error: 0.4720 - val_loss: 8.8040 - val_mean_absolute_error: 1.4011\n",
            "Epoch 3434/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 2.7033 - mean_absolute_error: 0.8032\n",
            "Epoch 3434: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 2.7013 - mean_absolute_error: 0.8030 - val_loss: 5.2687 - val_mean_absolute_error: 1.1036\n",
            "Epoch 3435/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7652 - mean_absolute_error: 0.5057\n",
            "Epoch 3435: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7656 - mean_absolute_error: 0.5059 - val_loss: 4.6519 - val_mean_absolute_error: 1.0370\n",
            "Epoch 3436/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5725 - mean_absolute_error: 0.4487\n",
            "Epoch 3436: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5721 - mean_absolute_error: 0.4487 - val_loss: 4.8663 - val_mean_absolute_error: 1.0188\n",
            "Epoch 3437/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5178 - mean_absolute_error: 0.4319\n",
            "Epoch 3437: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5175 - mean_absolute_error: 0.4318 - val_loss: 5.3173 - val_mean_absolute_error: 1.0788\n",
            "Epoch 3438/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5961 - mean_absolute_error: 0.4574\n",
            "Epoch 3438: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5966 - mean_absolute_error: 0.4576 - val_loss: 5.0699 - val_mean_absolute_error: 1.0400\n",
            "Epoch 3439/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5705 - mean_absolute_error: 0.4471\n",
            "Epoch 3439: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5722 - mean_absolute_error: 0.4474 - val_loss: 4.9366 - val_mean_absolute_error: 1.0347\n",
            "Epoch 3440/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5415 - mean_absolute_error: 0.4408\n",
            "Epoch 3440: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5413 - mean_absolute_error: 0.4408 - val_loss: 4.8378 - val_mean_absolute_error: 1.0461\n",
            "Epoch 3441/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6165 - mean_absolute_error: 0.4569\n",
            "Epoch 3441: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6167 - mean_absolute_error: 0.4569 - val_loss: 5.1319 - val_mean_absolute_error: 1.0642\n",
            "Epoch 3442/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6822 - mean_absolute_error: 0.4729\n",
            "Epoch 3442: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6816 - mean_absolute_error: 0.4728 - val_loss: 5.5799 - val_mean_absolute_error: 1.0726\n",
            "Epoch 3443/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6922 - mean_absolute_error: 0.4661\n",
            "Epoch 3443: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.6938 - mean_absolute_error: 0.4668 - val_loss: 5.5293 - val_mean_absolute_error: 1.2008\n",
            "Epoch 3444/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0115 - mean_absolute_error: 0.5487\n",
            "Epoch 3444: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.0108 - mean_absolute_error: 0.5486 - val_loss: 5.4809 - val_mean_absolute_error: 1.0782\n",
            "Epoch 3445/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6401 - mean_absolute_error: 0.4618\n",
            "Epoch 3445: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6399 - mean_absolute_error: 0.4618 - val_loss: 5.2579 - val_mean_absolute_error: 1.0851\n",
            "Epoch 3446/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5666 - mean_absolute_error: 0.4514\n",
            "Epoch 3446: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 24ms/step - loss: 0.5664 - mean_absolute_error: 0.4515 - val_loss: 5.2471 - val_mean_absolute_error: 1.0470\n",
            "Epoch 3447/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5251 - mean_absolute_error: 0.4344\n",
            "Epoch 3447: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5248 - mean_absolute_error: 0.4342 - val_loss: 5.3895 - val_mean_absolute_error: 1.0959\n",
            "Epoch 3448/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5373 - mean_absolute_error: 0.4393\n",
            "Epoch 3448: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5381 - mean_absolute_error: 0.4398 - val_loss: 4.7140 - val_mean_absolute_error: 1.0293\n",
            "Epoch 3449/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5787 - mean_absolute_error: 0.4489\n",
            "Epoch 3449: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5789 - mean_absolute_error: 0.4492 - val_loss: 4.9900 - val_mean_absolute_error: 1.1178\n",
            "Epoch 3450/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6041 - mean_absolute_error: 0.4556\n",
            "Epoch 3450: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6041 - mean_absolute_error: 0.4556 - val_loss: 4.8415 - val_mean_absolute_error: 1.0224\n",
            "Epoch 3451/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6284 - mean_absolute_error: 0.4677\n",
            "Epoch 3451: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6282 - mean_absolute_error: 0.4676 - val_loss: 5.4644 - val_mean_absolute_error: 1.1327\n",
            "Epoch 3452/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5991 - mean_absolute_error: 0.4576\n",
            "Epoch 3452: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5991 - mean_absolute_error: 0.4576 - val_loss: 5.6096 - val_mean_absolute_error: 1.1227\n",
            "Epoch 3453/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6472 - mean_absolute_error: 0.4676\n",
            "Epoch 3453: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6468 - mean_absolute_error: 0.4680 - val_loss: 5.2896 - val_mean_absolute_error: 1.1448\n",
            "Epoch 3454/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8452 - mean_absolute_error: 0.5078\n",
            "Epoch 3454: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8452 - mean_absolute_error: 0.5079 - val_loss: 5.0545 - val_mean_absolute_error: 1.0474\n",
            "Epoch 3455/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7321 - mean_absolute_error: 0.4965\n",
            "Epoch 3455: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7345 - mean_absolute_error: 0.4971 - val_loss: 5.0404 - val_mean_absolute_error: 1.0575\n",
            "Epoch 3456/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 1.0980 - mean_absolute_error: 0.5539\n",
            "Epoch 3456: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.1079 - mean_absolute_error: 0.5549 - val_loss: 5.3406 - val_mean_absolute_error: 1.1219\n",
            "Epoch 3457/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8045 - mean_absolute_error: 0.5129\n",
            "Epoch 3457: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8044 - mean_absolute_error: 0.5130 - val_loss: 5.5870 - val_mean_absolute_error: 1.1067\n",
            "Epoch 3458/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5781 - mean_absolute_error: 0.4399\n",
            "Epoch 3458: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5798 - mean_absolute_error: 0.4403 - val_loss: 5.4020 - val_mean_absolute_error: 1.0853\n",
            "Epoch 3459/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5907 - mean_absolute_error: 0.4584\n",
            "Epoch 3459: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5913 - mean_absolute_error: 0.4587 - val_loss: 4.8064 - val_mean_absolute_error: 1.0207\n",
            "Epoch 3460/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5575 - mean_absolute_error: 0.4448\n",
            "Epoch 3460: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5571 - mean_absolute_error: 0.4446 - val_loss: 5.1376 - val_mean_absolute_error: 1.0491\n",
            "Epoch 3461/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5533 - mean_absolute_error: 0.4444\n",
            "Epoch 3461: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5529 - mean_absolute_error: 0.4442 - val_loss: 5.2687 - val_mean_absolute_error: 1.0714\n",
            "Epoch 3462/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5327 - mean_absolute_error: 0.4388\n",
            "Epoch 3462: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5357 - mean_absolute_error: 0.4390 - val_loss: 4.7104 - val_mean_absolute_error: 1.0252\n",
            "Epoch 3463/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6256 - mean_absolute_error: 0.4698\n",
            "Epoch 3463: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6274 - mean_absolute_error: 0.4701 - val_loss: 5.7059 - val_mean_absolute_error: 1.0959\n",
            "Epoch 3464/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9112 - mean_absolute_error: 0.5200\n",
            "Epoch 3464: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9248 - mean_absolute_error: 0.5212 - val_loss: 7.5843 - val_mean_absolute_error: 1.4354\n",
            "Epoch 3465/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 2.3286 - mean_absolute_error: 0.7493\n",
            "Epoch 3465: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 2.3286 - mean_absolute_error: 0.7493 - val_loss: 5.3798 - val_mean_absolute_error: 1.0855\n",
            "Epoch 3466/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.9725 - mean_absolute_error: 0.5277\n",
            "Epoch 3466: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9741 - mean_absolute_error: 0.5280 - val_loss: 5.3635 - val_mean_absolute_error: 1.0927\n",
            "Epoch 3467/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6520 - mean_absolute_error: 0.4615\n",
            "Epoch 3467: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6567 - mean_absolute_error: 0.4625 - val_loss: 5.1455 - val_mean_absolute_error: 1.1005\n",
            "Epoch 3468/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5684 - mean_absolute_error: 0.4460\n",
            "Epoch 3468: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5695 - mean_absolute_error: 0.4463 - val_loss: 5.7413 - val_mean_absolute_error: 1.0947\n",
            "Epoch 3469/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5514 - mean_absolute_error: 0.4416\n",
            "Epoch 3469: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5515 - mean_absolute_error: 0.4417 - val_loss: 4.9784 - val_mean_absolute_error: 1.0280\n",
            "Epoch 3470/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5300 - mean_absolute_error: 0.4315\n",
            "Epoch 3470: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5297 - mean_absolute_error: 0.4314 - val_loss: 5.7315 - val_mean_absolute_error: 1.0838\n",
            "Epoch 3471/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5118 - mean_absolute_error: 0.4269\n",
            "Epoch 3471: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5119 - mean_absolute_error: 0.4270 - val_loss: 5.0934 - val_mean_absolute_error: 1.0407\n",
            "Epoch 3472/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5176 - mean_absolute_error: 0.4302\n",
            "Epoch 3472: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5186 - mean_absolute_error: 0.4306 - val_loss: 5.0631 - val_mean_absolute_error: 1.0311\n",
            "Epoch 3473/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5203 - mean_absolute_error: 0.4330\n",
            "Epoch 3473: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5204 - mean_absolute_error: 0.4330 - val_loss: 5.0287 - val_mean_absolute_error: 1.0687\n",
            "Epoch 3474/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5298 - mean_absolute_error: 0.4362\n",
            "Epoch 3474: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5302 - mean_absolute_error: 0.4362 - val_loss: 5.3132 - val_mean_absolute_error: 1.2077\n",
            "Epoch 3475/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.9829 - mean_absolute_error: 0.5398\n",
            "Epoch 3475: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9829 - mean_absolute_error: 0.5400 - val_loss: 5.3328 - val_mean_absolute_error: 1.0982\n",
            "Epoch 3476/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6308 - mean_absolute_error: 0.4633\n",
            "Epoch 3476: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6333 - mean_absolute_error: 0.4644 - val_loss: 5.0573 - val_mean_absolute_error: 1.0361\n",
            "Epoch 3477/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5798 - mean_absolute_error: 0.4539\n",
            "Epoch 3477: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5799 - mean_absolute_error: 0.4542 - val_loss: 5.2206 - val_mean_absolute_error: 1.0888\n",
            "Epoch 3478/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5360 - mean_absolute_error: 0.4341\n",
            "Epoch 3478: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5358 - mean_absolute_error: 0.4340 - val_loss: 5.3656 - val_mean_absolute_error: 1.0886\n",
            "Epoch 3479/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5417 - mean_absolute_error: 0.4402\n",
            "Epoch 3479: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5422 - mean_absolute_error: 0.4404 - val_loss: 5.3074 - val_mean_absolute_error: 1.0590\n",
            "Epoch 3480/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5416 - mean_absolute_error: 0.4401\n",
            "Epoch 3480: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5421 - mean_absolute_error: 0.4402 - val_loss: 5.3061 - val_mean_absolute_error: 1.0478\n",
            "Epoch 3481/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5131 - mean_absolute_error: 0.4292\n",
            "Epoch 3481: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5126 - mean_absolute_error: 0.4294 - val_loss: 4.8384 - val_mean_absolute_error: 1.0053\n",
            "Epoch 3482/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5519 - mean_absolute_error: 0.4492\n",
            "Epoch 3482: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5519 - mean_absolute_error: 0.4492 - val_loss: 5.2066 - val_mean_absolute_error: 1.0284\n",
            "Epoch 3483/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5570 - mean_absolute_error: 0.4457\n",
            "Epoch 3483: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5605 - mean_absolute_error: 0.4463 - val_loss: 5.7739 - val_mean_absolute_error: 1.1484\n",
            "Epoch 3484/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6070 - mean_absolute_error: 0.4646\n",
            "Epoch 3484: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6101 - mean_absolute_error: 0.4653 - val_loss: 4.8547 - val_mean_absolute_error: 1.0346\n",
            "Epoch 3485/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6123 - mean_absolute_error: 0.4667\n",
            "Epoch 3485: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6137 - mean_absolute_error: 0.4670 - val_loss: 5.4193 - val_mean_absolute_error: 1.0569\n",
            "Epoch 3486/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6753 - mean_absolute_error: 0.4784\n",
            "Epoch 3486: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6753 - mean_absolute_error: 0.4784 - val_loss: 4.7853 - val_mean_absolute_error: 1.0431\n",
            "Epoch 3487/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5581 - mean_absolute_error: 0.4500\n",
            "Epoch 3487: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5588 - mean_absolute_error: 0.4501 - val_loss: 5.3576 - val_mean_absolute_error: 1.0836\n",
            "Epoch 3488/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5150 - mean_absolute_error: 0.4331\n",
            "Epoch 3488: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5150 - mean_absolute_error: 0.4331 - val_loss: 4.9226 - val_mean_absolute_error: 1.0237\n",
            "Epoch 3489/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5571 - mean_absolute_error: 0.4547\n",
            "Epoch 3489: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5575 - mean_absolute_error: 0.4550 - val_loss: 5.7072 - val_mean_absolute_error: 1.1396\n",
            "Epoch 3490/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5969 - mean_absolute_error: 0.4629\n",
            "Epoch 3490: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5978 - mean_absolute_error: 0.4632 - val_loss: 5.3795 - val_mean_absolute_error: 1.1256\n",
            "Epoch 3491/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6405 - mean_absolute_error: 0.4722\n",
            "Epoch 3491: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6387 - mean_absolute_error: 0.4717 - val_loss: 5.4947 - val_mean_absolute_error: 1.0781\n",
            "Epoch 3492/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6664 - mean_absolute_error: 0.4788\n",
            "Epoch 3492: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6676 - mean_absolute_error: 0.4792 - val_loss: 5.2166 - val_mean_absolute_error: 1.1189\n",
            "Epoch 3493/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7027 - mean_absolute_error: 0.4884\n",
            "Epoch 3493: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7058 - mean_absolute_error: 0.4891 - val_loss: 5.2191 - val_mean_absolute_error: 1.0581\n",
            "Epoch 3494/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6596 - mean_absolute_error: 0.4772\n",
            "Epoch 3494: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6605 - mean_absolute_error: 0.4775 - val_loss: 5.0464 - val_mean_absolute_error: 1.0516\n",
            "Epoch 3495/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5850 - mean_absolute_error: 0.4497\n",
            "Epoch 3495: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5849 - mean_absolute_error: 0.4498 - val_loss: 4.9638 - val_mean_absolute_error: 1.0979\n",
            "Epoch 3496/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6302 - mean_absolute_error: 0.4683\n",
            "Epoch 3496: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6304 - mean_absolute_error: 0.4684 - val_loss: 5.2435 - val_mean_absolute_error: 1.1473\n",
            "Epoch 3497/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0007 - mean_absolute_error: 0.5383\n",
            "Epoch 3497: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.0020 - mean_absolute_error: 0.5387 - val_loss: 6.2382 - val_mean_absolute_error: 1.4763\n",
            "Epoch 3498/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9777 - mean_absolute_error: 0.5607\n",
            "Epoch 3498: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9783 - mean_absolute_error: 0.5609 - val_loss: 5.3653 - val_mean_absolute_error: 1.0661\n",
            "Epoch 3499/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5755 - mean_absolute_error: 0.4450\n",
            "Epoch 3499: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5768 - mean_absolute_error: 0.4454 - val_loss: 5.0350 - val_mean_absolute_error: 1.0425\n",
            "Epoch 3500/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5851 - mean_absolute_error: 0.4499\n",
            "Epoch 3500: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5850 - mean_absolute_error: 0.4499 - val_loss: 4.9773 - val_mean_absolute_error: 1.0353\n",
            "Epoch 3501/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5225 - mean_absolute_error: 0.4333\n",
            "Epoch 3501: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5244 - mean_absolute_error: 0.4343 - val_loss: 5.3182 - val_mean_absolute_error: 1.0795\n",
            "Epoch 3502/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5013 - mean_absolute_error: 0.4239\n",
            "Epoch 3502: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5011 - mean_absolute_error: 0.4239 - val_loss: 5.0885 - val_mean_absolute_error: 1.0844\n",
            "Epoch 3503/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4961 - mean_absolute_error: 0.4252\n",
            "Epoch 3503: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4957 - mean_absolute_error: 0.4250 - val_loss: 4.9384 - val_mean_absolute_error: 1.0441\n",
            "Epoch 3504/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6387 - mean_absolute_error: 0.4679\n",
            "Epoch 3504: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6384 - mean_absolute_error: 0.4678 - val_loss: 4.9466 - val_mean_absolute_error: 1.0190\n",
            "Epoch 3505/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5757 - mean_absolute_error: 0.4645\n",
            "Epoch 3505: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5754 - mean_absolute_error: 0.4645 - val_loss: 5.4602 - val_mean_absolute_error: 1.1051\n",
            "Epoch 3506/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5333 - mean_absolute_error: 0.4366\n",
            "Epoch 3506: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5369 - mean_absolute_error: 0.4371 - val_loss: 5.6591 - val_mean_absolute_error: 1.1494\n",
            "Epoch 3507/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9484 - mean_absolute_error: 0.5511\n",
            "Epoch 3507: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9482 - mean_absolute_error: 0.5511 - val_loss: 5.3069 - val_mean_absolute_error: 1.0532\n",
            "Epoch 3508/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5780 - mean_absolute_error: 0.4530\n",
            "Epoch 3508: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5874 - mean_absolute_error: 0.4545 - val_loss: 5.4124 - val_mean_absolute_error: 1.0829\n",
            "Epoch 3509/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7817 - mean_absolute_error: 0.5075\n",
            "Epoch 3509: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7821 - mean_absolute_error: 0.5077 - val_loss: 5.3035 - val_mean_absolute_error: 1.0593\n",
            "Epoch 3510/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5736 - mean_absolute_error: 0.4517\n",
            "Epoch 3510: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5733 - mean_absolute_error: 0.4516 - val_loss: 5.5775 - val_mean_absolute_error: 1.0986\n",
            "Epoch 3511/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5630 - mean_absolute_error: 0.4436\n",
            "Epoch 3511: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5632 - mean_absolute_error: 0.4438 - val_loss: 5.2779 - val_mean_absolute_error: 1.0882\n",
            "Epoch 3512/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5108 - mean_absolute_error: 0.4296\n",
            "Epoch 3512: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5122 - mean_absolute_error: 0.4298 - val_loss: 5.0604 - val_mean_absolute_error: 1.1226\n",
            "Epoch 3513/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6969 - mean_absolute_error: 0.4686\n",
            "Epoch 3513: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6975 - mean_absolute_error: 0.4687 - val_loss: 5.0944 - val_mean_absolute_error: 1.0514\n",
            "Epoch 3514/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6108 - mean_absolute_error: 0.4636\n",
            "Epoch 3514: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6111 - mean_absolute_error: 0.4636 - val_loss: 5.4185 - val_mean_absolute_error: 1.0672\n",
            "Epoch 3515/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5977 - mean_absolute_error: 0.4517\n",
            "Epoch 3515: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5977 - mean_absolute_error: 0.4518 - val_loss: 5.2439 - val_mean_absolute_error: 1.0206\n",
            "Epoch 3516/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6115 - mean_absolute_error: 0.4647\n",
            "Epoch 3516: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6157 - mean_absolute_error: 0.4654 - val_loss: 5.3346 - val_mean_absolute_error: 1.0709\n",
            "Epoch 3517/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6970 - mean_absolute_error: 0.4890\n",
            "Epoch 3517: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6964 - mean_absolute_error: 0.4888 - val_loss: 5.5599 - val_mean_absolute_error: 1.1668\n",
            "Epoch 3518/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5580 - mean_absolute_error: 0.4468\n",
            "Epoch 3518: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5593 - mean_absolute_error: 0.4474 - val_loss: 5.6135 - val_mean_absolute_error: 1.1793\n",
            "Epoch 3519/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5702 - mean_absolute_error: 0.4543\n",
            "Epoch 3519: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5698 - mean_absolute_error: 0.4542 - val_loss: 4.9879 - val_mean_absolute_error: 1.1123\n",
            "Epoch 3520/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5588 - mean_absolute_error: 0.4438\n",
            "Epoch 3520: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5596 - mean_absolute_error: 0.4441 - val_loss: 5.0547 - val_mean_absolute_error: 1.0403\n",
            "Epoch 3521/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5823 - mean_absolute_error: 0.4534\n",
            "Epoch 3521: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5806 - mean_absolute_error: 0.4531 - val_loss: 5.4460 - val_mean_absolute_error: 1.0833\n",
            "Epoch 3522/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5658 - mean_absolute_error: 0.4547\n",
            "Epoch 3522: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5659 - mean_absolute_error: 0.4547 - val_loss: 5.1168 - val_mean_absolute_error: 1.0465\n",
            "Epoch 3523/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6154 - mean_absolute_error: 0.4679\n",
            "Epoch 3523: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6157 - mean_absolute_error: 0.4680 - val_loss: 5.4947 - val_mean_absolute_error: 1.0759\n",
            "Epoch 3524/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5676 - mean_absolute_error: 0.4460\n",
            "Epoch 3524: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5734 - mean_absolute_error: 0.4473 - val_loss: 5.1188 - val_mean_absolute_error: 1.0762\n",
            "Epoch 3525/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7115 - mean_absolute_error: 0.4969\n",
            "Epoch 3525: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7117 - mean_absolute_error: 0.4970 - val_loss: 5.1843 - val_mean_absolute_error: 1.0857\n",
            "Epoch 3526/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9973 - mean_absolute_error: 0.5420\n",
            "Epoch 3526: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9998 - mean_absolute_error: 0.5425 - val_loss: 5.0328 - val_mean_absolute_error: 1.1733\n",
            "Epoch 3527/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6529 - mean_absolute_error: 0.4759\n",
            "Epoch 3527: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6534 - mean_absolute_error: 0.4761 - val_loss: 5.0988 - val_mean_absolute_error: 1.0505\n",
            "Epoch 3528/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5513 - mean_absolute_error: 0.4416\n",
            "Epoch 3528: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5512 - mean_absolute_error: 0.4416 - val_loss: 5.5923 - val_mean_absolute_error: 1.0898\n",
            "Epoch 3529/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5242 - mean_absolute_error: 0.4340\n",
            "Epoch 3529: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5254 - mean_absolute_error: 0.4344 - val_loss: 4.9665 - val_mean_absolute_error: 1.0316\n",
            "Epoch 3530/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6050 - mean_absolute_error: 0.4598\n",
            "Epoch 3530: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6062 - mean_absolute_error: 0.4603 - val_loss: 5.0848 - val_mean_absolute_error: 1.1343\n",
            "Epoch 3531/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5777 - mean_absolute_error: 0.4528\n",
            "Epoch 3531: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5785 - mean_absolute_error: 0.4531 - val_loss: 5.6349 - val_mean_absolute_error: 1.0997\n",
            "Epoch 3532/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5723 - mean_absolute_error: 0.4532\n",
            "Epoch 3532: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5729 - mean_absolute_error: 0.4533 - val_loss: 5.2083 - val_mean_absolute_error: 1.0526\n",
            "Epoch 3533/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5256 - mean_absolute_error: 0.4377\n",
            "Epoch 3533: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5268 - mean_absolute_error: 0.4380 - val_loss: 5.3491 - val_mean_absolute_error: 1.0638\n",
            "Epoch 3534/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6214 - mean_absolute_error: 0.4623\n",
            "Epoch 3534: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6215 - mean_absolute_error: 0.4625 - val_loss: 4.8362 - val_mean_absolute_error: 1.0258\n",
            "Epoch 3535/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5589 - mean_absolute_error: 0.4466\n",
            "Epoch 3535: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5612 - mean_absolute_error: 0.4470 - val_loss: 4.9936 - val_mean_absolute_error: 1.0797\n",
            "Epoch 3536/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8143 - mean_absolute_error: 0.5096\n",
            "Epoch 3536: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.8141 - mean_absolute_error: 0.5097 - val_loss: 5.3122 - val_mean_absolute_error: 1.0631\n",
            "Epoch 3537/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5492 - mean_absolute_error: 0.4424\n",
            "Epoch 3537: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5499 - mean_absolute_error: 0.4425 - val_loss: 5.2579 - val_mean_absolute_error: 1.0790\n",
            "Epoch 3538/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6651 - mean_absolute_error: 0.4751\n",
            "Epoch 3538: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6649 - mean_absolute_error: 0.4751 - val_loss: 5.2632 - val_mean_absolute_error: 1.0678\n",
            "Epoch 3539/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5737 - mean_absolute_error: 0.4453\n",
            "Epoch 3539: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5734 - mean_absolute_error: 0.4453 - val_loss: 5.1800 - val_mean_absolute_error: 1.0750\n",
            "Epoch 3540/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5402 - mean_absolute_error: 0.4422\n",
            "Epoch 3540: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5403 - mean_absolute_error: 0.4422 - val_loss: 5.2338 - val_mean_absolute_error: 1.0522\n",
            "Epoch 3541/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5706 - mean_absolute_error: 0.4546\n",
            "Epoch 3541: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5712 - mean_absolute_error: 0.4547 - val_loss: 5.0119 - val_mean_absolute_error: 1.0555\n",
            "Epoch 3542/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5782 - mean_absolute_error: 0.4617\n",
            "Epoch 3542: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5807 - mean_absolute_error: 0.4622 - val_loss: 4.8124 - val_mean_absolute_error: 1.0294\n",
            "Epoch 3543/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5909 - mean_absolute_error: 0.4488\n",
            "Epoch 3543: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5914 - mean_absolute_error: 0.4491 - val_loss: 4.8531 - val_mean_absolute_error: 1.0321\n",
            "Epoch 3544/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5304 - mean_absolute_error: 0.4430\n",
            "Epoch 3544: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5304 - mean_absolute_error: 0.4431 - val_loss: 4.7715 - val_mean_absolute_error: 1.0181\n",
            "Epoch 3545/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5645 - mean_absolute_error: 0.4491\n",
            "Epoch 3545: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5697 - mean_absolute_error: 0.4502 - val_loss: 5.8539 - val_mean_absolute_error: 1.1942\n",
            "Epoch 3546/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8539 - mean_absolute_error: 0.5087\n",
            "Epoch 3546: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8585 - mean_absolute_error: 0.5096 - val_loss: 6.7404 - val_mean_absolute_error: 1.4004\n",
            "Epoch 3547/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 1.2687 - mean_absolute_error: 0.5954\n",
            "Epoch 3547: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.2687 - mean_absolute_error: 0.5954 - val_loss: 5.4546 - val_mean_absolute_error: 1.1004\n",
            "Epoch 3548/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5761 - mean_absolute_error: 0.4499\n",
            "Epoch 3548: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5758 - mean_absolute_error: 0.4500 - val_loss: 5.1072 - val_mean_absolute_error: 1.0691\n",
            "Epoch 3549/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5238 - mean_absolute_error: 0.4322\n",
            "Epoch 3549: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5245 - mean_absolute_error: 0.4325 - val_loss: 5.2998 - val_mean_absolute_error: 1.1650\n",
            "Epoch 3550/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5439 - mean_absolute_error: 0.4423\n",
            "Epoch 3550: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5450 - mean_absolute_error: 0.4429 - val_loss: 5.1919 - val_mean_absolute_error: 1.0541\n",
            "Epoch 3551/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5256 - mean_absolute_error: 0.4356\n",
            "Epoch 3551: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5258 - mean_absolute_error: 0.4358 - val_loss: 4.9481 - val_mean_absolute_error: 1.0182\n",
            "Epoch 3552/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5521 - mean_absolute_error: 0.4407\n",
            "Epoch 3552: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5521 - mean_absolute_error: 0.4407 - val_loss: 5.3365 - val_mean_absolute_error: 1.1068\n",
            "Epoch 3553/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6225 - mean_absolute_error: 0.4584\n",
            "Epoch 3553: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6235 - mean_absolute_error: 0.4587 - val_loss: 5.3079 - val_mean_absolute_error: 1.0492\n",
            "Epoch 3554/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5579 - mean_absolute_error: 0.4511\n",
            "Epoch 3554: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5586 - mean_absolute_error: 0.4515 - val_loss: 5.0510 - val_mean_absolute_error: 1.0445\n",
            "Epoch 3555/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5418 - mean_absolute_error: 0.4396\n",
            "Epoch 3555: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5459 - mean_absolute_error: 0.4404 - val_loss: 5.1100 - val_mean_absolute_error: 1.0837\n",
            "Epoch 3556/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.7773 - mean_absolute_error: 0.6718\n",
            "Epoch 3556: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.7770 - mean_absolute_error: 0.6719 - val_loss: 5.2039 - val_mean_absolute_error: 1.1675\n",
            "Epoch 3557/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0573 - mean_absolute_error: 0.5542\n",
            "Epoch 3557: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0570 - mean_absolute_error: 0.5542 - val_loss: 5.2703 - val_mean_absolute_error: 1.0578\n",
            "Epoch 3558/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6038 - mean_absolute_error: 0.4574\n",
            "Epoch 3558: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6038 - mean_absolute_error: 0.4575 - val_loss: 5.1508 - val_mean_absolute_error: 1.0682\n",
            "Epoch 3559/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5549 - mean_absolute_error: 0.4369\n",
            "Epoch 3559: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5548 - mean_absolute_error: 0.4370 - val_loss: 4.9241 - val_mean_absolute_error: 1.0330\n",
            "Epoch 3560/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5310 - mean_absolute_error: 0.4334\n",
            "Epoch 3560: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5302 - mean_absolute_error: 0.4331 - val_loss: 4.8524 - val_mean_absolute_error: 1.0160\n",
            "Epoch 3561/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5251 - mean_absolute_error: 0.4313\n",
            "Epoch 3561: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5251 - mean_absolute_error: 0.4314 - val_loss: 5.2337 - val_mean_absolute_error: 1.0527\n",
            "Epoch 3562/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4997 - mean_absolute_error: 0.4208\n",
            "Epoch 3562: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4999 - mean_absolute_error: 0.4211 - val_loss: 4.7184 - val_mean_absolute_error: 0.9967\n",
            "Epoch 3563/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5189 - mean_absolute_error: 0.4308\n",
            "Epoch 3563: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5188 - mean_absolute_error: 0.4310 - val_loss: 5.0377 - val_mean_absolute_error: 1.0531\n",
            "Epoch 3564/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5227 - mean_absolute_error: 0.4373\n",
            "Epoch 3564: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5234 - mean_absolute_error: 0.4375 - val_loss: 5.1838 - val_mean_absolute_error: 1.0733\n",
            "Epoch 3565/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5158 - mean_absolute_error: 0.4314\n",
            "Epoch 3565: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5205 - mean_absolute_error: 0.4319 - val_loss: 5.2321 - val_mean_absolute_error: 1.0651\n",
            "Epoch 3566/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6978 - mean_absolute_error: 0.4821\n",
            "Epoch 3566: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6974 - mean_absolute_error: 0.4820 - val_loss: 5.7752 - val_mean_absolute_error: 1.0944\n",
            "Epoch 3567/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5348 - mean_absolute_error: 0.4347\n",
            "Epoch 3567: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5385 - mean_absolute_error: 0.4357 - val_loss: 4.8983 - val_mean_absolute_error: 1.0336\n",
            "Epoch 3568/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5446 - mean_absolute_error: 0.4442\n",
            "Epoch 3568: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5446 - mean_absolute_error: 0.4441 - val_loss: 4.9460 - val_mean_absolute_error: 1.0377\n",
            "Epoch 3569/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5566 - mean_absolute_error: 0.4500\n",
            "Epoch 3569: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5567 - mean_absolute_error: 0.4501 - val_loss: 5.0213 - val_mean_absolute_error: 1.0634\n",
            "Epoch 3570/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5235 - mean_absolute_error: 0.4315\n",
            "Epoch 3570: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5255 - mean_absolute_error: 0.4322 - val_loss: 5.1556 - val_mean_absolute_error: 1.0798\n",
            "Epoch 3571/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5684 - mean_absolute_error: 0.4556\n",
            "Epoch 3571: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5683 - mean_absolute_error: 0.4558 - val_loss: 5.2880 - val_mean_absolute_error: 1.0808\n",
            "Epoch 3572/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5563 - mean_absolute_error: 0.4409\n",
            "Epoch 3572: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5567 - mean_absolute_error: 0.4411 - val_loss: 5.4142 - val_mean_absolute_error: 1.0721\n",
            "Epoch 3573/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6279 - mean_absolute_error: 0.4630\n",
            "Epoch 3573: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6279 - mean_absolute_error: 0.4630 - val_loss: 4.8507 - val_mean_absolute_error: 1.0265\n",
            "Epoch 3574/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5703 - mean_absolute_error: 0.4455\n",
            "Epoch 3574: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5717 - mean_absolute_error: 0.4459 - val_loss: 5.2841 - val_mean_absolute_error: 1.0493\n",
            "Epoch 3575/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6351 - mean_absolute_error: 0.4701\n",
            "Epoch 3575: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6355 - mean_absolute_error: 0.4702 - val_loss: 5.5116 - val_mean_absolute_error: 1.0830\n",
            "Epoch 3576/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5600 - mean_absolute_error: 0.4460\n",
            "Epoch 3576: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5608 - mean_absolute_error: 0.4462 - val_loss: 6.1913 - val_mean_absolute_error: 1.1078\n",
            "Epoch 3577/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5428 - mean_absolute_error: 0.4380\n",
            "Epoch 3577: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5426 - mean_absolute_error: 0.4379 - val_loss: 5.2925 - val_mean_absolute_error: 1.0712\n",
            "Epoch 3578/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6510 - mean_absolute_error: 0.4648\n",
            "Epoch 3578: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6508 - mean_absolute_error: 0.4648 - val_loss: 5.4008 - val_mean_absolute_error: 1.1034\n",
            "Epoch 3579/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8658 - mean_absolute_error: 0.5259\n",
            "Epoch 3579: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8656 - mean_absolute_error: 0.5259 - val_loss: 5.9410 - val_mean_absolute_error: 1.2036\n",
            "Epoch 3580/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0774 - mean_absolute_error: 0.5612\n",
            "Epoch 3580: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 1.0831 - mean_absolute_error: 0.5620 - val_loss: 5.7658 - val_mean_absolute_error: 1.1409\n",
            "Epoch 3581/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7899 - mean_absolute_error: 0.5032\n",
            "Epoch 3581: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7897 - mean_absolute_error: 0.5033 - val_loss: 5.0189 - val_mean_absolute_error: 1.0394\n",
            "Epoch 3582/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5617 - mean_absolute_error: 0.4470\n",
            "Epoch 3582: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5638 - mean_absolute_error: 0.4476 - val_loss: 4.7672 - val_mean_absolute_error: 1.0960\n",
            "Epoch 3583/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5926 - mean_absolute_error: 0.4512\n",
            "Epoch 3583: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5943 - mean_absolute_error: 0.4519 - val_loss: 5.0002 - val_mean_absolute_error: 1.0108\n",
            "Epoch 3584/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5749 - mean_absolute_error: 0.4484\n",
            "Epoch 3584: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5752 - mean_absolute_error: 0.4486 - val_loss: 5.0946 - val_mean_absolute_error: 1.0570\n",
            "Epoch 3585/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5016 - mean_absolute_error: 0.4238\n",
            "Epoch 3585: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5035 - mean_absolute_error: 0.4248 - val_loss: 5.0248 - val_mean_absolute_error: 1.0318\n",
            "Epoch 3586/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5631 - mean_absolute_error: 0.4476\n",
            "Epoch 3586: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5635 - mean_absolute_error: 0.4478 - val_loss: 5.0413 - val_mean_absolute_error: 1.0540\n",
            "Epoch 3587/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5509 - mean_absolute_error: 0.4411\n",
            "Epoch 3587: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5506 - mean_absolute_error: 0.4410 - val_loss: 5.5481 - val_mean_absolute_error: 1.0705\n",
            "Epoch 3588/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5674 - mean_absolute_error: 0.4475\n",
            "Epoch 3588: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5677 - mean_absolute_error: 0.4477 - val_loss: 5.3641 - val_mean_absolute_error: 1.1519\n",
            "Epoch 3589/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5211 - mean_absolute_error: 0.4324\n",
            "Epoch 3589: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5220 - mean_absolute_error: 0.4330 - val_loss: 4.8757 - val_mean_absolute_error: 1.0614\n",
            "Epoch 3590/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5398 - mean_absolute_error: 0.4399\n",
            "Epoch 3590: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5403 - mean_absolute_error: 0.4401 - val_loss: 4.5051 - val_mean_absolute_error: 0.9999\n",
            "Epoch 3591/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5559 - mean_absolute_error: 0.4427\n",
            "Epoch 3591: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5561 - mean_absolute_error: 0.4429 - val_loss: 5.0266 - val_mean_absolute_error: 1.0387\n",
            "Epoch 3592/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6039 - mean_absolute_error: 0.4588\n",
            "Epoch 3592: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6048 - mean_absolute_error: 0.4591 - val_loss: 5.2168 - val_mean_absolute_error: 1.0594\n",
            "Epoch 3593/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9078 - mean_absolute_error: 0.5267\n",
            "Epoch 3593: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9073 - mean_absolute_error: 0.5267 - val_loss: 5.1793 - val_mean_absolute_error: 1.0673\n",
            "Epoch 3594/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5756 - mean_absolute_error: 0.4510\n",
            "Epoch 3594: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5753 - mean_absolute_error: 0.4511 - val_loss: 4.9391 - val_mean_absolute_error: 1.0226\n",
            "Epoch 3595/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6066 - mean_absolute_error: 0.4609\n",
            "Epoch 3595: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6079 - mean_absolute_error: 0.4614 - val_loss: 5.3378 - val_mean_absolute_error: 1.0589\n",
            "Epoch 3596/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5796 - mean_absolute_error: 0.4516\n",
            "Epoch 3596: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5800 - mean_absolute_error: 0.4517 - val_loss: 4.9954 - val_mean_absolute_error: 1.0691\n",
            "Epoch 3597/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5164 - mean_absolute_error: 0.4289\n",
            "Epoch 3597: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5162 - mean_absolute_error: 0.4289 - val_loss: 4.8291 - val_mean_absolute_error: 1.0335\n",
            "Epoch 3598/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4938 - mean_absolute_error: 0.4211\n",
            "Epoch 3598: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4940 - mean_absolute_error: 0.4212 - val_loss: 5.0544 - val_mean_absolute_error: 1.0672\n",
            "Epoch 3599/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0091 - mean_absolute_error: 0.5534\n",
            "Epoch 3599: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0122 - mean_absolute_error: 0.5544 - val_loss: 5.0825 - val_mean_absolute_error: 1.1177\n",
            "Epoch 3600/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6410 - mean_absolute_error: 0.4697\n",
            "Epoch 3600: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6411 - mean_absolute_error: 0.4698 - val_loss: 4.8385 - val_mean_absolute_error: 1.0163\n",
            "Epoch 3601/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5466 - mean_absolute_error: 0.4391\n",
            "Epoch 3601: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5482 - mean_absolute_error: 0.4397 - val_loss: 4.7828 - val_mean_absolute_error: 1.0170\n",
            "Epoch 3602/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0480 - mean_absolute_error: 0.5427\n",
            "Epoch 3602: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0473 - mean_absolute_error: 0.5426 - val_loss: 4.7452 - val_mean_absolute_error: 1.0236\n",
            "Epoch 3603/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5424 - mean_absolute_error: 0.4415\n",
            "Epoch 3603: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5423 - mean_absolute_error: 0.4415 - val_loss: 4.7041 - val_mean_absolute_error: 1.0318\n",
            "Epoch 3604/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5150 - mean_absolute_error: 0.4300\n",
            "Epoch 3604: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5148 - mean_absolute_error: 0.4303 - val_loss: 5.0071 - val_mean_absolute_error: 1.0341\n",
            "Epoch 3605/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5193 - mean_absolute_error: 0.4281\n",
            "Epoch 3605: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5221 - mean_absolute_error: 0.4284 - val_loss: 5.0400 - val_mean_absolute_error: 1.0403\n",
            "Epoch 3606/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5800 - mean_absolute_error: 0.4508\n",
            "Epoch 3606: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5806 - mean_absolute_error: 0.4508 - val_loss: 5.2406 - val_mean_absolute_error: 1.0439\n",
            "Epoch 3607/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5120 - mean_absolute_error: 0.4273\n",
            "Epoch 3607: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5120 - mean_absolute_error: 0.4273 - val_loss: 4.9148 - val_mean_absolute_error: 1.0687\n",
            "Epoch 3608/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5107 - mean_absolute_error: 0.4294\n",
            "Epoch 3608: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5114 - mean_absolute_error: 0.4297 - val_loss: 4.8040 - val_mean_absolute_error: 1.0217\n",
            "Epoch 3609/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7128 - mean_absolute_error: 0.4816\n",
            "Epoch 3609: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7124 - mean_absolute_error: 0.4815 - val_loss: 5.9514 - val_mean_absolute_error: 1.2194\n",
            "Epoch 3610/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5971 - mean_absolute_error: 0.4579\n",
            "Epoch 3610: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6006 - mean_absolute_error: 0.4584 - val_loss: 5.2651 - val_mean_absolute_error: 1.1207\n",
            "Epoch 3611/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1029 - mean_absolute_error: 0.5816\n",
            "Epoch 3611: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1023 - mean_absolute_error: 0.5815 - val_loss: 5.0842 - val_mean_absolute_error: 1.0296\n",
            "Epoch 3612/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5777 - mean_absolute_error: 0.4452\n",
            "Epoch 3612: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5787 - mean_absolute_error: 0.4455 - val_loss: 5.1262 - val_mean_absolute_error: 1.0340\n",
            "Epoch 3613/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5540 - mean_absolute_error: 0.4394\n",
            "Epoch 3613: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5539 - mean_absolute_error: 0.4395 - val_loss: 5.0598 - val_mean_absolute_error: 1.0544\n",
            "Epoch 3614/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5436 - mean_absolute_error: 0.4435\n",
            "Epoch 3614: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5454 - mean_absolute_error: 0.4438 - val_loss: 5.4804 - val_mean_absolute_error: 1.0804\n",
            "Epoch 3615/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6388 - mean_absolute_error: 0.4605\n",
            "Epoch 3615: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6383 - mean_absolute_error: 0.4603 - val_loss: 5.3691 - val_mean_absolute_error: 1.0667\n",
            "Epoch 3616/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5400 - mean_absolute_error: 0.4401\n",
            "Epoch 3616: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5414 - mean_absolute_error: 0.4408 - val_loss: 5.4055 - val_mean_absolute_error: 1.1215\n",
            "Epoch 3617/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5670 - mean_absolute_error: 0.4457\n",
            "Epoch 3617: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5678 - mean_absolute_error: 0.4460 - val_loss: 5.4752 - val_mean_absolute_error: 1.0652\n",
            "Epoch 3618/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5606 - mean_absolute_error: 0.4430\n",
            "Epoch 3618: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5607 - mean_absolute_error: 0.4432 - val_loss: 5.3227 - val_mean_absolute_error: 1.0550\n",
            "Epoch 3619/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5282 - mean_absolute_error: 0.4348\n",
            "Epoch 3619: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5289 - mean_absolute_error: 0.4353 - val_loss: 5.9570 - val_mean_absolute_error: 1.1923\n",
            "Epoch 3620/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5558 - mean_absolute_error: 0.4452\n",
            "Epoch 3620: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5567 - mean_absolute_error: 0.4457 - val_loss: 5.4963 - val_mean_absolute_error: 1.0934\n",
            "Epoch 3621/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5687 - mean_absolute_error: 0.4437\n",
            "Epoch 3621: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5690 - mean_absolute_error: 0.4441 - val_loss: 4.6497 - val_mean_absolute_error: 1.0060\n",
            "Epoch 3622/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6271 - mean_absolute_error: 0.4641\n",
            "Epoch 3622: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6274 - mean_absolute_error: 0.4641 - val_loss: 5.3391 - val_mean_absolute_error: 1.1014\n",
            "Epoch 3623/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6181 - mean_absolute_error: 0.4600\n",
            "Epoch 3623: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6178 - mean_absolute_error: 0.4600 - val_loss: 5.3492 - val_mean_absolute_error: 1.0570\n",
            "Epoch 3624/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5922 - mean_absolute_error: 0.4549\n",
            "Epoch 3624: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5925 - mean_absolute_error: 0.4551 - val_loss: 5.4950 - val_mean_absolute_error: 1.1074\n",
            "Epoch 3625/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5792 - mean_absolute_error: 0.4469\n",
            "Epoch 3625: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5790 - mean_absolute_error: 0.4469 - val_loss: 5.0465 - val_mean_absolute_error: 1.0844\n",
            "Epoch 3626/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5528 - mean_absolute_error: 0.4422\n",
            "Epoch 3626: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5535 - mean_absolute_error: 0.4423 - val_loss: 5.0128 - val_mean_absolute_error: 1.0177\n",
            "Epoch 3627/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5737 - mean_absolute_error: 0.4501\n",
            "Epoch 3627: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5736 - mean_absolute_error: 0.4501 - val_loss: 4.9199 - val_mean_absolute_error: 1.0348\n",
            "Epoch 3628/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6682 - mean_absolute_error: 0.4707\n",
            "Epoch 3628: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6766 - mean_absolute_error: 0.4723 - val_loss: 5.8185 - val_mean_absolute_error: 1.1915\n",
            "Epoch 3629/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7507 - mean_absolute_error: 0.5046\n",
            "Epoch 3629: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7511 - mean_absolute_error: 0.5046 - val_loss: 5.0755 - val_mean_absolute_error: 1.0514\n",
            "Epoch 3630/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6307 - mean_absolute_error: 0.4634\n",
            "Epoch 3630: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6304 - mean_absolute_error: 0.4634 - val_loss: 5.3464 - val_mean_absolute_error: 1.1008\n",
            "Epoch 3631/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.7127 - mean_absolute_error: 0.6502\n",
            "Epoch 3631: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.7129 - mean_absolute_error: 0.6503 - val_loss: 7.6388 - val_mean_absolute_error: 1.3151\n",
            "Epoch 3632/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8785 - mean_absolute_error: 0.5304\n",
            "Epoch 3632: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8779 - mean_absolute_error: 0.5302 - val_loss: 5.5518 - val_mean_absolute_error: 1.0977\n",
            "Epoch 3633/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6267 - mean_absolute_error: 0.4651\n",
            "Epoch 3633: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6273 - mean_absolute_error: 0.4652 - val_loss: 5.1572 - val_mean_absolute_error: 1.0379\n",
            "Epoch 3634/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5409 - mean_absolute_error: 0.4362\n",
            "Epoch 3634: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5413 - mean_absolute_error: 0.4365 - val_loss: 4.9883 - val_mean_absolute_error: 1.0378\n",
            "Epoch 3635/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4951 - mean_absolute_error: 0.4243\n",
            "Epoch 3635: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4962 - mean_absolute_error: 0.4248 - val_loss: 5.1417 - val_mean_absolute_error: 1.0592\n",
            "Epoch 3636/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5124 - mean_absolute_error: 0.4261\n",
            "Epoch 3636: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5126 - mean_absolute_error: 0.4262 - val_loss: 4.8198 - val_mean_absolute_error: 1.0751\n",
            "Epoch 3637/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5042 - mean_absolute_error: 0.4333\n",
            "Epoch 3637: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5049 - mean_absolute_error: 0.4337 - val_loss: 5.0933 - val_mean_absolute_error: 1.0538\n",
            "Epoch 3638/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5035 - mean_absolute_error: 0.4336\n",
            "Epoch 3638: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5038 - mean_absolute_error: 0.4338 - val_loss: 4.8438 - val_mean_absolute_error: 1.0193\n",
            "Epoch 3639/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5276 - mean_absolute_error: 0.4323\n",
            "Epoch 3639: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5285 - mean_absolute_error: 0.4324 - val_loss: 5.1912 - val_mean_absolute_error: 1.0648\n",
            "Epoch 3640/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5084 - mean_absolute_error: 0.4313\n",
            "Epoch 3640: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5090 - mean_absolute_error: 0.4317 - val_loss: 4.7673 - val_mean_absolute_error: 1.0538\n",
            "Epoch 3641/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6143 - mean_absolute_error: 0.4509\n",
            "Epoch 3641: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6171 - mean_absolute_error: 0.4513 - val_loss: 5.1035 - val_mean_absolute_error: 1.1531\n",
            "Epoch 3642/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8271 - mean_absolute_error: 0.5198\n",
            "Epoch 3642: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8267 - mean_absolute_error: 0.5197 - val_loss: 4.7656 - val_mean_absolute_error: 1.0875\n",
            "Epoch 3643/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5724 - mean_absolute_error: 0.4494\n",
            "Epoch 3643: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5725 - mean_absolute_error: 0.4495 - val_loss: 5.4594 - val_mean_absolute_error: 1.0610\n",
            "Epoch 3644/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5232 - mean_absolute_error: 0.4453\n",
            "Epoch 3644: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5234 - mean_absolute_error: 0.4455 - val_loss: 4.9765 - val_mean_absolute_error: 1.0421\n",
            "Epoch 3645/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5097 - mean_absolute_error: 0.4319\n",
            "Epoch 3645: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5097 - mean_absolute_error: 0.4320 - val_loss: 5.1754 - val_mean_absolute_error: 1.0424\n",
            "Epoch 3646/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4819 - mean_absolute_error: 0.4125\n",
            "Epoch 3646: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4827 - mean_absolute_error: 0.4127 - val_loss: 5.2854 - val_mean_absolute_error: 1.0363\n",
            "Epoch 3647/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5561 - mean_absolute_error: 0.4442\n",
            "Epoch 3647: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5559 - mean_absolute_error: 0.4442 - val_loss: 5.2657 - val_mean_absolute_error: 1.0538\n",
            "Epoch 3648/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5218 - mean_absolute_error: 0.4392\n",
            "Epoch 3648: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5217 - mean_absolute_error: 0.4393 - val_loss: 4.9073 - val_mean_absolute_error: 1.0264\n",
            "Epoch 3649/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8734 - mean_absolute_error: 0.5169\n",
            "Epoch 3649: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8728 - mean_absolute_error: 0.5167 - val_loss: 4.7854 - val_mean_absolute_error: 1.0455\n",
            "Epoch 3650/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7328 - mean_absolute_error: 0.4706\n",
            "Epoch 3650: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7335 - mean_absolute_error: 0.4708 - val_loss: 7.8519 - val_mean_absolute_error: 1.3480\n",
            "Epoch 3651/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 2.9211 - mean_absolute_error: 0.8212\n",
            "Epoch 3651: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 2.9195 - mean_absolute_error: 0.8213 - val_loss: 5.8294 - val_mean_absolute_error: 1.1513\n",
            "Epoch 3652/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7737 - mean_absolute_error: 0.4941\n",
            "Epoch 3652: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7842 - mean_absolute_error: 0.4947 - val_loss: 4.8466 - val_mean_absolute_error: 1.0156\n",
            "Epoch 3653/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7137 - mean_absolute_error: 0.4863\n",
            "Epoch 3653: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7130 - mean_absolute_error: 0.4865 - val_loss: 5.3376 - val_mean_absolute_error: 1.1289\n",
            "Epoch 3654/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5397 - mean_absolute_error: 0.4372\n",
            "Epoch 3654: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5400 - mean_absolute_error: 0.4376 - val_loss: 5.0361 - val_mean_absolute_error: 1.0577\n",
            "Epoch 3655/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5311 - mean_absolute_error: 0.4341\n",
            "Epoch 3655: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5314 - mean_absolute_error: 0.4343 - val_loss: 4.8945 - val_mean_absolute_error: 1.0572\n",
            "Epoch 3656/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4936 - mean_absolute_error: 0.4218\n",
            "Epoch 3656: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4934 - mean_absolute_error: 0.4218 - val_loss: 4.9834 - val_mean_absolute_error: 1.0217\n",
            "Epoch 3657/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5050 - mean_absolute_error: 0.4258\n",
            "Epoch 3657: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5054 - mean_absolute_error: 0.4259 - val_loss: 5.0856 - val_mean_absolute_error: 1.0495\n",
            "Epoch 3658/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5076 - mean_absolute_error: 0.4268\n",
            "Epoch 3658: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5085 - mean_absolute_error: 0.4273 - val_loss: 5.1341 - val_mean_absolute_error: 1.0346\n",
            "Epoch 3659/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5903 - mean_absolute_error: 0.4512\n",
            "Epoch 3659: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5910 - mean_absolute_error: 0.4516 - val_loss: 5.0196 - val_mean_absolute_error: 1.1022\n",
            "Epoch 3660/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5147 - mean_absolute_error: 0.4325\n",
            "Epoch 3660: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5149 - mean_absolute_error: 0.4328 - val_loss: 4.8463 - val_mean_absolute_error: 1.0512\n",
            "Epoch 3661/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4923 - mean_absolute_error: 0.4173\n",
            "Epoch 3661: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4922 - mean_absolute_error: 0.4173 - val_loss: 5.1774 - val_mean_absolute_error: 1.0639\n",
            "Epoch 3662/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5131 - mean_absolute_error: 0.4315\n",
            "Epoch 3662: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5127 - mean_absolute_error: 0.4313 - val_loss: 4.8521 - val_mean_absolute_error: 1.0317\n",
            "Epoch 3663/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5102 - mean_absolute_error: 0.4285\n",
            "Epoch 3663: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5113 - mean_absolute_error: 0.4286 - val_loss: 4.9563 - val_mean_absolute_error: 1.0464\n",
            "Epoch 3664/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6020 - mean_absolute_error: 0.4592\n",
            "Epoch 3664: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6019 - mean_absolute_error: 0.4594 - val_loss: 5.2936 - val_mean_absolute_error: 1.0646\n",
            "Epoch 3665/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5286 - mean_absolute_error: 0.4310\n",
            "Epoch 3665: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5286 - mean_absolute_error: 0.4311 - val_loss: 4.7927 - val_mean_absolute_error: 1.0277\n",
            "Epoch 3666/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6038 - mean_absolute_error: 0.4556\n",
            "Epoch 3666: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6041 - mean_absolute_error: 0.4558 - val_loss: 6.0017 - val_mean_absolute_error: 1.1328\n",
            "Epoch 3667/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.9955 - mean_absolute_error: 0.5439\n",
            "Epoch 3667: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9935 - mean_absolute_error: 0.5435 - val_loss: 5.0138 - val_mean_absolute_error: 1.0481\n",
            "Epoch 3668/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5578 - mean_absolute_error: 0.4441\n",
            "Epoch 3668: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5583 - mean_absolute_error: 0.4444 - val_loss: 5.1096 - val_mean_absolute_error: 1.0702\n",
            "Epoch 3669/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5122 - mean_absolute_error: 0.4321\n",
            "Epoch 3669: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5123 - mean_absolute_error: 0.4322 - val_loss: 4.5402 - val_mean_absolute_error: 1.0066\n",
            "Epoch 3670/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5164 - mean_absolute_error: 0.4322\n",
            "Epoch 3670: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5170 - mean_absolute_error: 0.4325 - val_loss: 6.0063 - val_mean_absolute_error: 1.1813\n",
            "Epoch 3671/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7169 - mean_absolute_error: 0.4827\n",
            "Epoch 3671: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7169 - mean_absolute_error: 0.4827 - val_loss: 5.0836 - val_mean_absolute_error: 1.0648\n",
            "Epoch 3672/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5206 - mean_absolute_error: 0.4311\n",
            "Epoch 3672: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5239 - mean_absolute_error: 0.4316 - val_loss: 5.1021 - val_mean_absolute_error: 1.0302\n",
            "Epoch 3673/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.8988 - mean_absolute_error: 0.7135\n",
            "Epoch 3673: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.8982 - mean_absolute_error: 0.7135 - val_loss: 5.1647 - val_mean_absolute_error: 1.1108\n",
            "Epoch 3674/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0922 - mean_absolute_error: 0.5704\n",
            "Epoch 3674: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0974 - mean_absolute_error: 0.5712 - val_loss: 5.3998 - val_mean_absolute_error: 1.1556\n",
            "Epoch 3675/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6293 - mean_absolute_error: 0.4644\n",
            "Epoch 3675: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6339 - mean_absolute_error: 0.4656 - val_loss: 4.7099 - val_mean_absolute_error: 1.0221\n",
            "Epoch 3676/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5490 - mean_absolute_error: 0.4455\n",
            "Epoch 3676: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5493 - mean_absolute_error: 0.4457 - val_loss: 5.1656 - val_mean_absolute_error: 1.0600\n",
            "Epoch 3677/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.5101 - mean_absolute_error: 0.4269\n",
            "Epoch 3677: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5101 - mean_absolute_error: 0.4269 - val_loss: 5.0881 - val_mean_absolute_error: 1.0535\n",
            "Epoch 3678/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4984 - mean_absolute_error: 0.4194\n",
            "Epoch 3678: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4991 - mean_absolute_error: 0.4197 - val_loss: 5.0283 - val_mean_absolute_error: 1.0320\n",
            "Epoch 3679/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4925 - mean_absolute_error: 0.4229\n",
            "Epoch 3679: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4931 - mean_absolute_error: 0.4233 - val_loss: 4.4866 - val_mean_absolute_error: 0.9896\n",
            "Epoch 3680/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4917 - mean_absolute_error: 0.4223\n",
            "Epoch 3680: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4918 - mean_absolute_error: 0.4224 - val_loss: 4.8648 - val_mean_absolute_error: 1.0549\n",
            "Epoch 3681/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5050 - mean_absolute_error: 0.4244\n",
            "Epoch 3681: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5149 - mean_absolute_error: 0.4261 - val_loss: 4.9080 - val_mean_absolute_error: 1.0351\n",
            "Epoch 3682/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7355 - mean_absolute_error: 0.4804\n",
            "Epoch 3682: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7394 - mean_absolute_error: 0.4811 - val_loss: 5.7011 - val_mean_absolute_error: 1.1033\n",
            "Epoch 3683/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6648 - mean_absolute_error: 0.4666\n",
            "Epoch 3683: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6644 - mean_absolute_error: 0.4664 - val_loss: 4.9538 - val_mean_absolute_error: 1.0721\n",
            "Epoch 3684/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5276 - mean_absolute_error: 0.4290\n",
            "Epoch 3684: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5276 - mean_absolute_error: 0.4290 - val_loss: 4.7408 - val_mean_absolute_error: 1.0827\n",
            "Epoch 3685/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4933 - mean_absolute_error: 0.4267\n",
            "Epoch 3685: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4932 - mean_absolute_error: 0.4267 - val_loss: 4.9973 - val_mean_absolute_error: 1.0490\n",
            "Epoch 3686/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5113 - mean_absolute_error: 0.4272\n",
            "Epoch 3686: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5118 - mean_absolute_error: 0.4275 - val_loss: 5.2753 - val_mean_absolute_error: 1.1052\n",
            "Epoch 3687/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5437 - mean_absolute_error: 0.4392\n",
            "Epoch 3687: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5472 - mean_absolute_error: 0.4403 - val_loss: 4.8223 - val_mean_absolute_error: 1.0204\n",
            "Epoch 3688/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5135 - mean_absolute_error: 0.4352\n",
            "Epoch 3688: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5201 - mean_absolute_error: 0.4361 - val_loss: 4.7592 - val_mean_absolute_error: 1.0024\n",
            "Epoch 3689/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5456 - mean_absolute_error: 0.4407\n",
            "Epoch 3689: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5481 - mean_absolute_error: 0.4414 - val_loss: 5.7527 - val_mean_absolute_error: 1.0993\n",
            "Epoch 3690/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6276 - mean_absolute_error: 0.4630\n",
            "Epoch 3690: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6290 - mean_absolute_error: 0.4633 - val_loss: 4.7397 - val_mean_absolute_error: 1.0158\n",
            "Epoch 3691/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5658 - mean_absolute_error: 0.4549\n",
            "Epoch 3691: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5681 - mean_absolute_error: 0.4556 - val_loss: 5.1907 - val_mean_absolute_error: 1.0667\n",
            "Epoch 3692/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5384 - mean_absolute_error: 0.4382\n",
            "Epoch 3692: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5401 - mean_absolute_error: 0.4387 - val_loss: 5.3051 - val_mean_absolute_error: 1.0694\n",
            "Epoch 3693/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5558 - mean_absolute_error: 0.4465\n",
            "Epoch 3693: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5554 - mean_absolute_error: 0.4463 - val_loss: 5.1469 - val_mean_absolute_error: 1.0844\n",
            "Epoch 3694/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6123 - mean_absolute_error: 0.4602\n",
            "Epoch 3694: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6135 - mean_absolute_error: 0.4606 - val_loss: 5.1194 - val_mean_absolute_error: 1.0325\n",
            "Epoch 3695/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8010 - mean_absolute_error: 0.5149\n",
            "Epoch 3695: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8013 - mean_absolute_error: 0.5151 - val_loss: 5.4045 - val_mean_absolute_error: 1.0980\n",
            "Epoch 3696/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.3454 - mean_absolute_error: 0.5967\n",
            "Epoch 3696: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.3447 - mean_absolute_error: 0.5966 - val_loss: 5.1579 - val_mean_absolute_error: 1.0636\n",
            "Epoch 3697/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8362 - mean_absolute_error: 0.5011\n",
            "Epoch 3697: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8375 - mean_absolute_error: 0.5014 - val_loss: 5.7724 - val_mean_absolute_error: 1.1049\n",
            "Epoch 3698/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6394 - mean_absolute_error: 0.4757\n",
            "Epoch 3698: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6390 - mean_absolute_error: 0.4756 - val_loss: 5.0958 - val_mean_absolute_error: 1.0379\n",
            "Epoch 3699/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5169 - mean_absolute_error: 0.4272\n",
            "Epoch 3699: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5237 - mean_absolute_error: 0.4279 - val_loss: 4.7210 - val_mean_absolute_error: 1.0187\n",
            "Epoch 3700/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5753 - mean_absolute_error: 0.4488\n",
            "Epoch 3700: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5759 - mean_absolute_error: 0.4490 - val_loss: 4.9921 - val_mean_absolute_error: 1.0264\n",
            "Epoch 3701/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5584 - mean_absolute_error: 0.4438\n",
            "Epoch 3701: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5584 - mean_absolute_error: 0.4438 - val_loss: 4.9612 - val_mean_absolute_error: 1.0736\n",
            "Epoch 3702/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5581 - mean_absolute_error: 0.4411\n",
            "Epoch 3702: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5586 - mean_absolute_error: 0.4414 - val_loss: 5.2384 - val_mean_absolute_error: 1.0787\n",
            "Epoch 3703/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5503 - mean_absolute_error: 0.4438\n",
            "Epoch 3703: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5499 - mean_absolute_error: 0.4436 - val_loss: 4.8298 - val_mean_absolute_error: 1.0246\n",
            "Epoch 3704/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5618 - mean_absolute_error: 0.4444\n",
            "Epoch 3704: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5625 - mean_absolute_error: 0.4447 - val_loss: 5.0172 - val_mean_absolute_error: 1.0425\n",
            "Epoch 3705/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2755 - mean_absolute_error: 0.5996\n",
            "Epoch 3705: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2765 - mean_absolute_error: 0.5999 - val_loss: 5.7910 - val_mean_absolute_error: 1.1518\n",
            "Epoch 3706/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7229 - mean_absolute_error: 0.4871\n",
            "Epoch 3706: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7225 - mean_absolute_error: 0.4870 - val_loss: 5.1848 - val_mean_absolute_error: 1.0641\n",
            "Epoch 3707/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5476 - mean_absolute_error: 0.4407\n",
            "Epoch 3707: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5476 - mean_absolute_error: 0.4409 - val_loss: 4.7488 - val_mean_absolute_error: 1.0412\n",
            "Epoch 3708/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5417 - mean_absolute_error: 0.4366\n",
            "Epoch 3708: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5431 - mean_absolute_error: 0.4369 - val_loss: 5.6199 - val_mean_absolute_error: 1.1001\n",
            "Epoch 3709/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5327 - mean_absolute_error: 0.4366\n",
            "Epoch 3709: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5346 - mean_absolute_error: 0.4372 - val_loss: 4.8432 - val_mean_absolute_error: 1.0486\n",
            "Epoch 3710/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5539 - mean_absolute_error: 0.4404\n",
            "Epoch 3710: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5552 - mean_absolute_error: 0.4407 - val_loss: 5.3041 - val_mean_absolute_error: 1.0809\n",
            "Epoch 3711/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8631 - mean_absolute_error: 0.5141\n",
            "Epoch 3711: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8632 - mean_absolute_error: 0.5141 - val_loss: 5.5423 - val_mean_absolute_error: 1.0917\n",
            "Epoch 3712/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5336 - mean_absolute_error: 0.4358\n",
            "Epoch 3712: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5372 - mean_absolute_error: 0.4367 - val_loss: 5.0185 - val_mean_absolute_error: 1.0398\n",
            "Epoch 3713/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6093 - mean_absolute_error: 0.4544\n",
            "Epoch 3713: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6074 - mean_absolute_error: 0.4539 - val_loss: 5.0512 - val_mean_absolute_error: 1.0482\n",
            "Epoch 3714/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5027 - mean_absolute_error: 0.4263\n",
            "Epoch 3714: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5120 - mean_absolute_error: 0.4270 - val_loss: 5.8226 - val_mean_absolute_error: 1.0992\n",
            "Epoch 3715/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.3327 - mean_absolute_error: 0.6023\n",
            "Epoch 3715: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.3325 - mean_absolute_error: 0.6025 - val_loss: 5.1288 - val_mean_absolute_error: 1.0501\n",
            "Epoch 3716/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5632 - mean_absolute_error: 0.4425\n",
            "Epoch 3716: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5633 - mean_absolute_error: 0.4425 - val_loss: 4.9787 - val_mean_absolute_error: 1.0399\n",
            "Epoch 3717/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5094 - mean_absolute_error: 0.4278\n",
            "Epoch 3717: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5108 - mean_absolute_error: 0.4284 - val_loss: 4.9996 - val_mean_absolute_error: 1.0269\n",
            "Epoch 3718/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5476 - mean_absolute_error: 0.4337\n",
            "Epoch 3718: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5474 - mean_absolute_error: 0.4337 - val_loss: 5.2151 - val_mean_absolute_error: 1.0848\n",
            "Epoch 3719/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5636 - mean_absolute_error: 0.4406\n",
            "Epoch 3719: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5658 - mean_absolute_error: 0.4411 - val_loss: 5.5684 - val_mean_absolute_error: 1.0875\n",
            "Epoch 3720/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6023 - mean_absolute_error: 0.4582\n",
            "Epoch 3720: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6025 - mean_absolute_error: 0.4583 - val_loss: 5.3853 - val_mean_absolute_error: 1.0730\n",
            "Epoch 3721/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4828 - mean_absolute_error: 0.4189\n",
            "Epoch 3721: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4836 - mean_absolute_error: 0.4191 - val_loss: 5.7059 - val_mean_absolute_error: 1.1234\n",
            "Epoch 3722/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5555 - mean_absolute_error: 0.4399\n",
            "Epoch 3722: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5552 - mean_absolute_error: 0.4397 - val_loss: 5.1320 - val_mean_absolute_error: 1.0481\n",
            "Epoch 3723/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5416 - mean_absolute_error: 0.4439\n",
            "Epoch 3723: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5432 - mean_absolute_error: 0.4446 - val_loss: 5.0425 - val_mean_absolute_error: 1.0715\n",
            "Epoch 3724/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5263 - mean_absolute_error: 0.4417\n",
            "Epoch 3724: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5260 - mean_absolute_error: 0.4415 - val_loss: 4.9665 - val_mean_absolute_error: 1.0322\n",
            "Epoch 3725/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5034 - mean_absolute_error: 0.4314\n",
            "Epoch 3725: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5036 - mean_absolute_error: 0.4315 - val_loss: 4.9415 - val_mean_absolute_error: 1.0458\n",
            "Epoch 3726/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5052 - mean_absolute_error: 0.4323\n",
            "Epoch 3726: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5051 - mean_absolute_error: 0.4324 - val_loss: 5.0308 - val_mean_absolute_error: 1.0507\n",
            "Epoch 3727/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5350 - mean_absolute_error: 0.4295\n",
            "Epoch 3727: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5359 - mean_absolute_error: 0.4298 - val_loss: 5.1459 - val_mean_absolute_error: 1.0565\n",
            "Epoch 3728/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5652 - mean_absolute_error: 0.4532\n",
            "Epoch 3728: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5655 - mean_absolute_error: 0.4534 - val_loss: 4.9858 - val_mean_absolute_error: 1.0636\n",
            "Epoch 3729/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6016 - mean_absolute_error: 0.4520\n",
            "Epoch 3729: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6043 - mean_absolute_error: 0.4525 - val_loss: 4.9239 - val_mean_absolute_error: 1.0534\n",
            "Epoch 3730/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5883 - mean_absolute_error: 0.4533\n",
            "Epoch 3730: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5883 - mean_absolute_error: 0.4534 - val_loss: 5.0280 - val_mean_absolute_error: 1.0464\n",
            "Epoch 3731/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7947 - mean_absolute_error: 0.5054\n",
            "Epoch 3731: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7988 - mean_absolute_error: 0.5062 - val_loss: 5.0285 - val_mean_absolute_error: 1.0737\n",
            "Epoch 3732/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0991 - mean_absolute_error: 0.5796\n",
            "Epoch 3732: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1001 - mean_absolute_error: 0.5801 - val_loss: 5.3631 - val_mean_absolute_error: 1.0959\n",
            "Epoch 3733/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9334 - mean_absolute_error: 0.5288\n",
            "Epoch 3733: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9330 - mean_absolute_error: 0.5288 - val_loss: 5.6454 - val_mean_absolute_error: 1.0867\n",
            "Epoch 3734/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5706 - mean_absolute_error: 0.4443\n",
            "Epoch 3734: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5704 - mean_absolute_error: 0.4443 - val_loss: 5.0856 - val_mean_absolute_error: 1.0739\n",
            "Epoch 3735/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5233 - mean_absolute_error: 0.4287\n",
            "Epoch 3735: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5245 - mean_absolute_error: 0.4292 - val_loss: 4.7859 - val_mean_absolute_error: 1.0003\n",
            "Epoch 3736/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5629 - mean_absolute_error: 0.4470\n",
            "Epoch 3736: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5625 - mean_absolute_error: 0.4468 - val_loss: 5.0673 - val_mean_absolute_error: 1.0696\n",
            "Epoch 3737/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4950 - mean_absolute_error: 0.4227\n",
            "Epoch 3737: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4992 - mean_absolute_error: 0.4235 - val_loss: 5.4517 - val_mean_absolute_error: 1.0724\n",
            "Epoch 3738/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5670 - mean_absolute_error: 0.4514\n",
            "Epoch 3738: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5709 - mean_absolute_error: 0.4518 - val_loss: 5.3742 - val_mean_absolute_error: 1.0523\n",
            "Epoch 3739/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5118 - mean_absolute_error: 0.4326\n",
            "Epoch 3739: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5136 - mean_absolute_error: 0.4330 - val_loss: 5.0963 - val_mean_absolute_error: 1.1078\n",
            "Epoch 3740/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5346 - mean_absolute_error: 0.4356\n",
            "Epoch 3740: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5353 - mean_absolute_error: 0.4360 - val_loss: 5.0350 - val_mean_absolute_error: 1.0525\n",
            "Epoch 3741/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5170 - mean_absolute_error: 0.4317\n",
            "Epoch 3741: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5171 - mean_absolute_error: 0.4318 - val_loss: 5.3466 - val_mean_absolute_error: 1.0710\n",
            "Epoch 3742/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5030 - mean_absolute_error: 0.4317\n",
            "Epoch 3742: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5040 - mean_absolute_error: 0.4319 - val_loss: 4.9326 - val_mean_absolute_error: 1.0753\n",
            "Epoch 3743/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6308 - mean_absolute_error: 0.4644\n",
            "Epoch 3743: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6306 - mean_absolute_error: 0.4645 - val_loss: 5.0997 - val_mean_absolute_error: 1.0289\n",
            "Epoch 3744/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5575 - mean_absolute_error: 0.4440\n",
            "Epoch 3744: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5573 - mean_absolute_error: 0.4441 - val_loss: 5.5902 - val_mean_absolute_error: 1.1074\n",
            "Epoch 3745/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5438 - mean_absolute_error: 0.4410\n",
            "Epoch 3745: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5463 - mean_absolute_error: 0.4414 - val_loss: 5.4894 - val_mean_absolute_error: 1.0820\n",
            "Epoch 3746/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9935 - mean_absolute_error: 0.5419\n",
            "Epoch 3746: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9935 - mean_absolute_error: 0.5421 - val_loss: 5.1037 - val_mean_absolute_error: 1.0711\n",
            "Epoch 3747/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6327 - mean_absolute_error: 0.4701\n",
            "Epoch 3747: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6322 - mean_absolute_error: 0.4699 - val_loss: 4.9964 - val_mean_absolute_error: 1.0268\n",
            "Epoch 3748/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5335 - mean_absolute_error: 0.4396\n",
            "Epoch 3748: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5339 - mean_absolute_error: 0.4399 - val_loss: 5.2891 - val_mean_absolute_error: 1.0522\n",
            "Epoch 3749/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5372 - mean_absolute_error: 0.4325\n",
            "Epoch 3749: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5375 - mean_absolute_error: 0.4326 - val_loss: 4.9963 - val_mean_absolute_error: 1.0521\n",
            "Epoch 3750/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5160 - mean_absolute_error: 0.4269\n",
            "Epoch 3750: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5157 - mean_absolute_error: 0.4269 - val_loss: 5.0428 - val_mean_absolute_error: 1.0361\n",
            "Epoch 3751/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4886 - mean_absolute_error: 0.4220\n",
            "Epoch 3751: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4891 - mean_absolute_error: 0.4223 - val_loss: 5.0037 - val_mean_absolute_error: 1.0588\n",
            "Epoch 3752/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5346 - mean_absolute_error: 0.4374\n",
            "Epoch 3752: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5345 - mean_absolute_error: 0.4373 - val_loss: 5.4887 - val_mean_absolute_error: 1.1049\n",
            "Epoch 3753/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5082 - mean_absolute_error: 0.4328\n",
            "Epoch 3753: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5080 - mean_absolute_error: 0.4328 - val_loss: 5.1576 - val_mean_absolute_error: 1.0780\n",
            "Epoch 3754/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5560 - mean_absolute_error: 0.4478\n",
            "Epoch 3754: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5558 - mean_absolute_error: 0.4477 - val_loss: 4.9579 - val_mean_absolute_error: 1.0434\n",
            "Epoch 3755/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5465 - mean_absolute_error: 0.4418\n",
            "Epoch 3755: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5470 - mean_absolute_error: 0.4419 - val_loss: 5.4352 - val_mean_absolute_error: 1.0779\n",
            "Epoch 3756/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6225 - mean_absolute_error: 0.4706\n",
            "Epoch 3756: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6243 - mean_absolute_error: 0.4707 - val_loss: 5.2595 - val_mean_absolute_error: 1.0855\n",
            "Epoch 3757/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5759 - mean_absolute_error: 0.4470\n",
            "Epoch 3757: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5767 - mean_absolute_error: 0.4474 - val_loss: 5.0174 - val_mean_absolute_error: 1.0743\n",
            "Epoch 3758/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8082 - mean_absolute_error: 0.4878\n",
            "Epoch 3758: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8077 - mean_absolute_error: 0.4878 - val_loss: 5.1923 - val_mean_absolute_error: 1.0821\n",
            "Epoch 3759/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6524 - mean_absolute_error: 0.4665\n",
            "Epoch 3759: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6526 - mean_absolute_error: 0.4668 - val_loss: 5.0256 - val_mean_absolute_error: 1.0249\n",
            "Epoch 3760/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5015 - mean_absolute_error: 0.4252\n",
            "Epoch 3760: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5013 - mean_absolute_error: 0.4252 - val_loss: 5.0040 - val_mean_absolute_error: 1.0507\n",
            "Epoch 3761/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5091 - mean_absolute_error: 0.4262\n",
            "Epoch 3761: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5092 - mean_absolute_error: 0.4263 - val_loss: 5.0094 - val_mean_absolute_error: 1.0360\n",
            "Epoch 3762/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6040 - mean_absolute_error: 0.4502\n",
            "Epoch 3762: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6035 - mean_absolute_error: 0.4500 - val_loss: 4.8622 - val_mean_absolute_error: 1.1894\n",
            "Epoch 3763/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6959 - mean_absolute_error: 0.4843\n",
            "Epoch 3763: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6999 - mean_absolute_error: 0.4852 - val_loss: 4.8457 - val_mean_absolute_error: 1.0420\n",
            "Epoch 3764/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7198 - mean_absolute_error: 0.4994\n",
            "Epoch 3764: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.7199 - mean_absolute_error: 0.4995 - val_loss: 5.5264 - val_mean_absolute_error: 1.0706\n",
            "Epoch 3765/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5342 - mean_absolute_error: 0.4333\n",
            "Epoch 3765: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5360 - mean_absolute_error: 0.4336 - val_loss: 4.8062 - val_mean_absolute_error: 1.0535\n",
            "Epoch 3766/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5266 - mean_absolute_error: 0.4334\n",
            "Epoch 3766: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5268 - mean_absolute_error: 0.4336 - val_loss: 5.1410 - val_mean_absolute_error: 1.0474\n",
            "Epoch 3767/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5345 - mean_absolute_error: 0.4370\n",
            "Epoch 3767: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5346 - mean_absolute_error: 0.4371 - val_loss: 4.9229 - val_mean_absolute_error: 1.0377\n",
            "Epoch 3768/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5138 - mean_absolute_error: 0.4305\n",
            "Epoch 3768: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5137 - mean_absolute_error: 0.4305 - val_loss: 5.6452 - val_mean_absolute_error: 1.0735\n",
            "Epoch 3769/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4941 - mean_absolute_error: 0.4210\n",
            "Epoch 3769: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4955 - mean_absolute_error: 0.4215 - val_loss: 4.8983 - val_mean_absolute_error: 1.0320\n",
            "Epoch 3770/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5498 - mean_absolute_error: 0.4428\n",
            "Epoch 3770: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5500 - mean_absolute_error: 0.4430 - val_loss: 5.0127 - val_mean_absolute_error: 1.0435\n",
            "Epoch 3771/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5116 - mean_absolute_error: 0.4317\n",
            "Epoch 3771: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5114 - mean_absolute_error: 0.4317 - val_loss: 5.2142 - val_mean_absolute_error: 1.0596\n",
            "Epoch 3772/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5118 - mean_absolute_error: 0.4322\n",
            "Epoch 3772: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5131 - mean_absolute_error: 0.4329 - val_loss: 5.0239 - val_mean_absolute_error: 1.0569\n",
            "Epoch 3773/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5880 - mean_absolute_error: 0.4583\n",
            "Epoch 3773: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5878 - mean_absolute_error: 0.4583 - val_loss: 5.3304 - val_mean_absolute_error: 1.0717\n",
            "Epoch 3774/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5991 - mean_absolute_error: 0.4572\n",
            "Epoch 3774: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5992 - mean_absolute_error: 0.4572 - val_loss: 4.6066 - val_mean_absolute_error: 1.0340\n",
            "Epoch 3775/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5381 - mean_absolute_error: 0.4329\n",
            "Epoch 3775: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5376 - mean_absolute_error: 0.4327 - val_loss: 5.2013 - val_mean_absolute_error: 1.0761\n",
            "Epoch 3776/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5178 - mean_absolute_error: 0.4354\n",
            "Epoch 3776: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5187 - mean_absolute_error: 0.4358 - val_loss: 5.1077 - val_mean_absolute_error: 1.0789\n",
            "Epoch 3777/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5564 - mean_absolute_error: 0.4409\n",
            "Epoch 3777: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5563 - mean_absolute_error: 0.4410 - val_loss: 5.5795 - val_mean_absolute_error: 1.0854\n",
            "Epoch 3778/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5851 - mean_absolute_error: 0.4550\n",
            "Epoch 3778: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5846 - mean_absolute_error: 0.4548 - val_loss: 5.1529 - val_mean_absolute_error: 1.0570\n",
            "Epoch 3779/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6134 - mean_absolute_error: 0.4598\n",
            "Epoch 3779: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6136 - mean_absolute_error: 0.4600 - val_loss: 5.3125 - val_mean_absolute_error: 1.0720\n",
            "Epoch 3780/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5006 - mean_absolute_error: 0.4270\n",
            "Epoch 3780: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5005 - mean_absolute_error: 0.4269 - val_loss: 5.1723 - val_mean_absolute_error: 1.0421\n",
            "Epoch 3781/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5048 - mean_absolute_error: 0.4286\n",
            "Epoch 3781: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5047 - mean_absolute_error: 0.4286 - val_loss: 4.8260 - val_mean_absolute_error: 1.0358\n",
            "Epoch 3782/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5837 - mean_absolute_error: 0.4531\n",
            "Epoch 3782: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5842 - mean_absolute_error: 0.4534 - val_loss: 5.1906 - val_mean_absolute_error: 1.1026\n",
            "Epoch 3783/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6798 - mean_absolute_error: 0.4756\n",
            "Epoch 3783: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6798 - mean_absolute_error: 0.4757 - val_loss: 5.1499 - val_mean_absolute_error: 1.0660\n",
            "Epoch 3784/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5481 - mean_absolute_error: 0.4399\n",
            "Epoch 3784: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5474 - mean_absolute_error: 0.4397 - val_loss: 4.9460 - val_mean_absolute_error: 1.0585\n",
            "Epoch 3785/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5493 - mean_absolute_error: 0.4386\n",
            "Epoch 3785: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5498 - mean_absolute_error: 0.4387 - val_loss: 4.9054 - val_mean_absolute_error: 1.0533\n",
            "Epoch 3786/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5566 - mean_absolute_error: 0.4449\n",
            "Epoch 3786: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5591 - mean_absolute_error: 0.4454 - val_loss: 5.5142 - val_mean_absolute_error: 1.0858\n",
            "Epoch 3787/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5290 - mean_absolute_error: 0.4380\n",
            "Epoch 3787: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5286 - mean_absolute_error: 0.4379 - val_loss: 5.3770 - val_mean_absolute_error: 1.0733\n",
            "Epoch 3788/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5744 - mean_absolute_error: 0.4511\n",
            "Epoch 3788: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5744 - mean_absolute_error: 0.4511 - val_loss: 5.1257 - val_mean_absolute_error: 1.0475\n",
            "Epoch 3789/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5576 - mean_absolute_error: 0.4455\n",
            "Epoch 3789: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5571 - mean_absolute_error: 0.4453 - val_loss: 5.4430 - val_mean_absolute_error: 1.0808\n",
            "Epoch 3790/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8570 - mean_absolute_error: 0.4985\n",
            "Epoch 3790: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.8567 - mean_absolute_error: 0.4985 - val_loss: 6.3632 - val_mean_absolute_error: 1.1912\n",
            "Epoch 3791/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0356 - mean_absolute_error: 0.5634\n",
            "Epoch 3791: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0357 - mean_absolute_error: 0.5633 - val_loss: 5.7050 - val_mean_absolute_error: 1.0966\n",
            "Epoch 3792/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5465 - mean_absolute_error: 0.4396\n",
            "Epoch 3792: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5478 - mean_absolute_error: 0.4402 - val_loss: 5.2322 - val_mean_absolute_error: 1.0807\n",
            "Epoch 3793/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5354 - mean_absolute_error: 0.4415\n",
            "Epoch 3793: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5359 - mean_absolute_error: 0.4419 - val_loss: 4.9367 - val_mean_absolute_error: 1.0384\n",
            "Epoch 3794/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5359 - mean_absolute_error: 0.4364\n",
            "Epoch 3794: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5362 - mean_absolute_error: 0.4366 - val_loss: 5.0203 - val_mean_absolute_error: 1.0458\n",
            "Epoch 3795/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4970 - mean_absolute_error: 0.4211\n",
            "Epoch 3795: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4982 - mean_absolute_error: 0.4214 - val_loss: 5.0696 - val_mean_absolute_error: 1.0553\n",
            "Epoch 3796/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6460 - mean_absolute_error: 0.4646\n",
            "Epoch 3796: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6463 - mean_absolute_error: 0.4647 - val_loss: 5.1732 - val_mean_absolute_error: 1.0627\n",
            "Epoch 3797/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5309 - mean_absolute_error: 0.4357\n",
            "Epoch 3797: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5310 - mean_absolute_error: 0.4359 - val_loss: 5.0019 - val_mean_absolute_error: 1.0752\n",
            "Epoch 3798/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.0509 - mean_absolute_error: 0.5344\n",
            "Epoch 3798: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.0509 - mean_absolute_error: 0.5348 - val_loss: 5.1987 - val_mean_absolute_error: 1.2838\n",
            "Epoch 3799/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7590 - mean_absolute_error: 0.4993\n",
            "Epoch 3799: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7592 - mean_absolute_error: 0.4996 - val_loss: 5.3306 - val_mean_absolute_error: 1.0654\n",
            "Epoch 3800/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5036 - mean_absolute_error: 0.4280\n",
            "Epoch 3800: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5042 - mean_absolute_error: 0.4282 - val_loss: 5.3826 - val_mean_absolute_error: 1.0664\n",
            "Epoch 3801/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4813 - mean_absolute_error: 0.4223\n",
            "Epoch 3801: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4834 - mean_absolute_error: 0.4228 - val_loss: 4.9674 - val_mean_absolute_error: 1.0428\n",
            "Epoch 3802/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4940 - mean_absolute_error: 0.4272\n",
            "Epoch 3802: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4938 - mean_absolute_error: 0.4272 - val_loss: 5.2272 - val_mean_absolute_error: 1.0531\n",
            "Epoch 3803/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4703 - mean_absolute_error: 0.4138\n",
            "Epoch 3803: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4712 - mean_absolute_error: 0.4142 - val_loss: 5.3460 - val_mean_absolute_error: 1.0599\n",
            "Epoch 3804/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5164 - mean_absolute_error: 0.4366\n",
            "Epoch 3804: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5163 - mean_absolute_error: 0.4366 - val_loss: 5.1123 - val_mean_absolute_error: 1.0495\n",
            "Epoch 3805/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5420 - mean_absolute_error: 0.4363\n",
            "Epoch 3805: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5424 - mean_absolute_error: 0.4364 - val_loss: 5.3498 - val_mean_absolute_error: 1.1322\n",
            "Epoch 3806/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4830 - mean_absolute_error: 0.4216\n",
            "Epoch 3806: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4827 - mean_absolute_error: 0.4215 - val_loss: 5.1100 - val_mean_absolute_error: 1.0299\n",
            "Epoch 3807/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5037 - mean_absolute_error: 0.4308\n",
            "Epoch 3807: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5029 - mean_absolute_error: 0.4309 - val_loss: 4.8265 - val_mean_absolute_error: 1.0390\n",
            "Epoch 3808/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5335 - mean_absolute_error: 0.4385\n",
            "Epoch 3808: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5354 - mean_absolute_error: 0.4389 - val_loss: 5.0052 - val_mean_absolute_error: 1.0820\n",
            "Epoch 3809/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7495 - mean_absolute_error: 0.4932\n",
            "Epoch 3809: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7508 - mean_absolute_error: 0.4936 - val_loss: 5.0839 - val_mean_absolute_error: 1.0599\n",
            "Epoch 3810/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5547 - mean_absolute_error: 0.4455\n",
            "Epoch 3810: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5564 - mean_absolute_error: 0.4461 - val_loss: 4.9785 - val_mean_absolute_error: 1.0566\n",
            "Epoch 3811/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8263 - mean_absolute_error: 0.4985\n",
            "Epoch 3811: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8256 - mean_absolute_error: 0.4983 - val_loss: 4.8243 - val_mean_absolute_error: 1.0706\n",
            "Epoch 3812/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5149 - mean_absolute_error: 0.4329\n",
            "Epoch 3812: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5157 - mean_absolute_error: 0.4334 - val_loss: 5.1866 - val_mean_absolute_error: 1.0850\n",
            "Epoch 3813/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5438 - mean_absolute_error: 0.4365\n",
            "Epoch 3813: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5438 - mean_absolute_error: 0.4366 - val_loss: 4.8617 - val_mean_absolute_error: 1.0414\n",
            "Epoch 3814/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5123 - mean_absolute_error: 0.4249\n",
            "Epoch 3814: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5175 - mean_absolute_error: 0.4259 - val_loss: 5.0244 - val_mean_absolute_error: 1.0629\n",
            "Epoch 3815/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6072 - mean_absolute_error: 0.4611\n",
            "Epoch 3815: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6068 - mean_absolute_error: 0.4610 - val_loss: 4.8727 - val_mean_absolute_error: 1.0553\n",
            "Epoch 3816/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4718 - mean_absolute_error: 0.4165\n",
            "Epoch 3816: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4770 - mean_absolute_error: 0.4175 - val_loss: 5.1556 - val_mean_absolute_error: 1.0489\n",
            "Epoch 3817/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.7526 - mean_absolute_error: 0.4979\n",
            "Epoch 3817: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7510 - mean_absolute_error: 0.4975 - val_loss: 5.3307 - val_mean_absolute_error: 1.0916\n",
            "Epoch 3818/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5626 - mean_absolute_error: 0.4459\n",
            "Epoch 3818: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5649 - mean_absolute_error: 0.4465 - val_loss: 5.0567 - val_mean_absolute_error: 1.0462\n",
            "Epoch 3819/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5544 - mean_absolute_error: 0.4436\n",
            "Epoch 3819: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5542 - mean_absolute_error: 0.4436 - val_loss: 5.1717 - val_mean_absolute_error: 1.0476\n",
            "Epoch 3820/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5031 - mean_absolute_error: 0.4312\n",
            "Epoch 3820: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5029 - mean_absolute_error: 0.4312 - val_loss: 5.0719 - val_mean_absolute_error: 1.0717\n",
            "Epoch 3821/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5301 - mean_absolute_error: 0.4329\n",
            "Epoch 3821: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5318 - mean_absolute_error: 0.4334 - val_loss: 4.9037 - val_mean_absolute_error: 1.0754\n",
            "Epoch 3822/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6006 - mean_absolute_error: 0.4635\n",
            "Epoch 3822: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6013 - mean_absolute_error: 0.4636 - val_loss: 4.8447 - val_mean_absolute_error: 1.0238\n",
            "Epoch 3823/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5512 - mean_absolute_error: 0.4355\n",
            "Epoch 3823: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5508 - mean_absolute_error: 0.4353 - val_loss: 5.0642 - val_mean_absolute_error: 1.0970\n",
            "Epoch 3824/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5178 - mean_absolute_error: 0.4342\n",
            "Epoch 3824: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5193 - mean_absolute_error: 0.4349 - val_loss: 4.8682 - val_mean_absolute_error: 1.0375\n",
            "Epoch 3825/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5172 - mean_absolute_error: 0.4336\n",
            "Epoch 3825: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5177 - mean_absolute_error: 0.4340 - val_loss: 5.5490 - val_mean_absolute_error: 1.1021\n",
            "Epoch 3826/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5154 - mean_absolute_error: 0.4328\n",
            "Epoch 3826: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5158 - mean_absolute_error: 0.4330 - val_loss: 5.6327 - val_mean_absolute_error: 1.1353\n",
            "Epoch 3827/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6626 - mean_absolute_error: 0.4619\n",
            "Epoch 3827: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6627 - mean_absolute_error: 0.4621 - val_loss: 6.3278 - val_mean_absolute_error: 1.1695\n",
            "Epoch 3828/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1639 - mean_absolute_error: 0.5752\n",
            "Epoch 3828: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1646 - mean_absolute_error: 0.5757 - val_loss: 6.5399 - val_mean_absolute_error: 1.2300\n",
            "Epoch 3829/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.3293 - mean_absolute_error: 0.5993\n",
            "Epoch 3829: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.3290 - mean_absolute_error: 0.5992 - val_loss: 5.1367 - val_mean_absolute_error: 1.0263\n",
            "Epoch 3830/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6092 - mean_absolute_error: 0.4512\n",
            "Epoch 3830: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6085 - mean_absolute_error: 0.4512 - val_loss: 4.9840 - val_mean_absolute_error: 1.0853\n",
            "Epoch 3831/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4831 - mean_absolute_error: 0.4150\n",
            "Epoch 3831: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4835 - mean_absolute_error: 0.4152 - val_loss: 5.1796 - val_mean_absolute_error: 1.0580\n",
            "Epoch 3832/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4704 - mean_absolute_error: 0.4094\n",
            "Epoch 3832: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4735 - mean_absolute_error: 0.4104 - val_loss: 5.1357 - val_mean_absolute_error: 1.0416\n",
            "Epoch 3833/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4754 - mean_absolute_error: 0.4143\n",
            "Epoch 3833: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4754 - mean_absolute_error: 0.4144 - val_loss: 5.0439 - val_mean_absolute_error: 1.0412\n",
            "Epoch 3834/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4889 - mean_absolute_error: 0.4197\n",
            "Epoch 3834: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4902 - mean_absolute_error: 0.4203 - val_loss: 4.8681 - val_mean_absolute_error: 1.0321\n",
            "Epoch 3835/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5174 - mean_absolute_error: 0.4352\n",
            "Epoch 3835: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5216 - mean_absolute_error: 0.4358 - val_loss: 5.1260 - val_mean_absolute_error: 1.0473\n",
            "Epoch 3836/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5374 - mean_absolute_error: 0.4310\n",
            "Epoch 3836: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5393 - mean_absolute_error: 0.4313 - val_loss: 4.7580 - val_mean_absolute_error: 1.0325\n",
            "Epoch 3837/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5261 - mean_absolute_error: 0.4304\n",
            "Epoch 3837: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5260 - mean_absolute_error: 0.4304 - val_loss: 4.9851 - val_mean_absolute_error: 1.0228\n",
            "Epoch 3838/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4532 - mean_absolute_error: 0.4091\n",
            "Epoch 3838: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4539 - mean_absolute_error: 0.4094 - val_loss: 5.0186 - val_mean_absolute_error: 1.0457\n",
            "Epoch 3839/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5234 - mean_absolute_error: 0.4402\n",
            "Epoch 3839: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5242 - mean_absolute_error: 0.4405 - val_loss: 5.2888 - val_mean_absolute_error: 1.1138\n",
            "Epoch 3840/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5359 - mean_absolute_error: 0.4378\n",
            "Epoch 3840: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5368 - mean_absolute_error: 0.4379 - val_loss: 5.1156 - val_mean_absolute_error: 1.1236\n",
            "Epoch 3841/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5393 - mean_absolute_error: 0.4421\n",
            "Epoch 3841: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5395 - mean_absolute_error: 0.4422 - val_loss: 5.5338 - val_mean_absolute_error: 1.1113\n",
            "Epoch 3842/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5262 - mean_absolute_error: 0.4401\n",
            "Epoch 3842: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5279 - mean_absolute_error: 0.4407 - val_loss: 5.8085 - val_mean_absolute_error: 1.1005\n",
            "Epoch 3843/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5343 - mean_absolute_error: 0.4348\n",
            "Epoch 3843: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5362 - mean_absolute_error: 0.4352 - val_loss: 5.0006 - val_mean_absolute_error: 1.0499\n",
            "Epoch 3844/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5804 - mean_absolute_error: 0.4502\n",
            "Epoch 3844: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5800 - mean_absolute_error: 0.4501 - val_loss: 5.3801 - val_mean_absolute_error: 1.0767\n",
            "Epoch 3845/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5223 - mean_absolute_error: 0.4332\n",
            "Epoch 3845: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5221 - mean_absolute_error: 0.4333 - val_loss: 4.8291 - val_mean_absolute_error: 1.0363\n",
            "Epoch 3846/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6599 - mean_absolute_error: 0.4587\n",
            "Epoch 3846: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6606 - mean_absolute_error: 0.4590 - val_loss: 5.9062 - val_mean_absolute_error: 1.1474\n",
            "Epoch 3847/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5710 - mean_absolute_error: 0.4482\n",
            "Epoch 3847: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5719 - mean_absolute_error: 0.4486 - val_loss: 5.2379 - val_mean_absolute_error: 1.0614\n",
            "Epoch 3848/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5795 - mean_absolute_error: 0.4548\n",
            "Epoch 3848: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5860 - mean_absolute_error: 0.4559 - val_loss: 5.3704 - val_mean_absolute_error: 1.1344\n",
            "Epoch 3849/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7080 - mean_absolute_error: 0.4918\n",
            "Epoch 3849: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7090 - mean_absolute_error: 0.4921 - val_loss: 5.1980 - val_mean_absolute_error: 1.1845\n",
            "Epoch 3850/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5110 - mean_absolute_error: 0.4367\n",
            "Epoch 3850: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5151 - mean_absolute_error: 0.4373 - val_loss: 5.0583 - val_mean_absolute_error: 1.0384\n",
            "Epoch 3851/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5638 - mean_absolute_error: 0.4434\n",
            "Epoch 3851: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5655 - mean_absolute_error: 0.4437 - val_loss: 5.2849 - val_mean_absolute_error: 1.0712\n",
            "Epoch 3852/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5785 - mean_absolute_error: 0.4457\n",
            "Epoch 3852: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5799 - mean_absolute_error: 0.4462 - val_loss: 5.2749 - val_mean_absolute_error: 1.0951\n",
            "Epoch 3853/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5555 - mean_absolute_error: 0.4344\n",
            "Epoch 3853: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5552 - mean_absolute_error: 0.4343 - val_loss: 4.8896 - val_mean_absolute_error: 1.0732\n",
            "Epoch 3854/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5038 - mean_absolute_error: 0.4266\n",
            "Epoch 3854: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5082 - mean_absolute_error: 0.4272 - val_loss: 4.9029 - val_mean_absolute_error: 1.0600\n",
            "Epoch 3855/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5810 - mean_absolute_error: 0.4489\n",
            "Epoch 3855: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5810 - mean_absolute_error: 0.4489 - val_loss: 4.8331 - val_mean_absolute_error: 1.0516\n",
            "Epoch 3856/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4944 - mean_absolute_error: 0.4267\n",
            "Epoch 3856: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4940 - mean_absolute_error: 0.4265 - val_loss: 4.9479 - val_mean_absolute_error: 1.0537\n",
            "Epoch 3857/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5092 - mean_absolute_error: 0.4297\n",
            "Epoch 3857: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5103 - mean_absolute_error: 0.4302 - val_loss: 5.3915 - val_mean_absolute_error: 1.0594\n",
            "Epoch 3858/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9199 - mean_absolute_error: 0.5321\n",
            "Epoch 3858: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.9197 - mean_absolute_error: 0.5320 - val_loss: 5.4691 - val_mean_absolute_error: 1.1035\n",
            "Epoch 3859/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6325 - mean_absolute_error: 0.4595\n",
            "Epoch 3859: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6357 - mean_absolute_error: 0.4600 - val_loss: 4.9385 - val_mean_absolute_error: 1.0687\n",
            "Epoch 3860/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5716 - mean_absolute_error: 0.4424\n",
            "Epoch 3860: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5728 - mean_absolute_error: 0.4432 - val_loss: 5.1172 - val_mean_absolute_error: 1.0246\n",
            "Epoch 3861/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.2652 - mean_absolute_error: 0.5809\n",
            "Epoch 3861: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.2649 - mean_absolute_error: 0.5810 - val_loss: 5.9110 - val_mean_absolute_error: 1.1844\n",
            "Epoch 3862/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6344 - mean_absolute_error: 0.4606\n",
            "Epoch 3862: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6339 - mean_absolute_error: 0.4605 - val_loss: 5.5447 - val_mean_absolute_error: 1.0921\n",
            "Epoch 3863/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5835 - mean_absolute_error: 0.4459\n",
            "Epoch 3863: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5859 - mean_absolute_error: 0.4465 - val_loss: 5.8396 - val_mean_absolute_error: 1.1497\n",
            "Epoch 3864/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5578 - mean_absolute_error: 0.4391\n",
            "Epoch 3864: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5582 - mean_absolute_error: 0.4393 - val_loss: 5.0999 - val_mean_absolute_error: 1.0422\n",
            "Epoch 3865/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5056 - mean_absolute_error: 0.4266\n",
            "Epoch 3865: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5060 - mean_absolute_error: 0.4268 - val_loss: 5.2361 - val_mean_absolute_error: 1.0900\n",
            "Epoch 3866/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5378 - mean_absolute_error: 0.4391\n",
            "Epoch 3866: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5402 - mean_absolute_error: 0.4398 - val_loss: 5.1738 - val_mean_absolute_error: 1.0574\n",
            "Epoch 3867/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8288 - mean_absolute_error: 0.5077\n",
            "Epoch 3867: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.8284 - mean_absolute_error: 0.5076 - val_loss: 5.1258 - val_mean_absolute_error: 1.0525\n",
            "Epoch 3868/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5597 - mean_absolute_error: 0.4412\n",
            "Epoch 3868: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5598 - mean_absolute_error: 0.4412 - val_loss: 5.3744 - val_mean_absolute_error: 1.0672\n",
            "Epoch 3869/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4685 - mean_absolute_error: 0.4155\n",
            "Epoch 3869: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4755 - mean_absolute_error: 0.4166 - val_loss: 5.2084 - val_mean_absolute_error: 1.0435\n",
            "Epoch 3870/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6236 - mean_absolute_error: 0.4760\n",
            "Epoch 3870: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6276 - mean_absolute_error: 0.4768 - val_loss: 4.7495 - val_mean_absolute_error: 1.0145\n",
            "Epoch 3871/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5223 - mean_absolute_error: 0.4362\n",
            "Epoch 3871: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5228 - mean_absolute_error: 0.4363 - val_loss: 4.6457 - val_mean_absolute_error: 1.0512\n",
            "Epoch 3872/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5141 - mean_absolute_error: 0.4298\n",
            "Epoch 3872: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5154 - mean_absolute_error: 0.4300 - val_loss: 4.9380 - val_mean_absolute_error: 1.0287\n",
            "Epoch 3873/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5676 - mean_absolute_error: 0.4485\n",
            "Epoch 3873: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5673 - mean_absolute_error: 0.4485 - val_loss: 5.1912 - val_mean_absolute_error: 1.0559\n",
            "Epoch 3874/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4825 - mean_absolute_error: 0.4192\n",
            "Epoch 3874: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4825 - mean_absolute_error: 0.4193 - val_loss: 5.4524 - val_mean_absolute_error: 1.0764\n",
            "Epoch 3875/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4916 - mean_absolute_error: 0.4266\n",
            "Epoch 3875: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4918 - mean_absolute_error: 0.4267 - val_loss: 4.9805 - val_mean_absolute_error: 1.0658\n",
            "Epoch 3876/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5446 - mean_absolute_error: 0.4362\n",
            "Epoch 3876: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5445 - mean_absolute_error: 0.4362 - val_loss: 5.4829 - val_mean_absolute_error: 1.0904\n",
            "Epoch 3877/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5437 - mean_absolute_error: 0.4400\n",
            "Epoch 3877: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5442 - mean_absolute_error: 0.4403 - val_loss: 4.9320 - val_mean_absolute_error: 1.0683\n",
            "Epoch 3878/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5078 - mean_absolute_error: 0.4287\n",
            "Epoch 3878: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5075 - mean_absolute_error: 0.4287 - val_loss: 4.8724 - val_mean_absolute_error: 1.0258\n",
            "Epoch 3879/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4989 - mean_absolute_error: 0.4275\n",
            "Epoch 3879: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4986 - mean_absolute_error: 0.4275 - val_loss: 5.1401 - val_mean_absolute_error: 1.0785\n",
            "Epoch 3880/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5154 - mean_absolute_error: 0.4335\n",
            "Epoch 3880: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5157 - mean_absolute_error: 0.4334 - val_loss: 5.5149 - val_mean_absolute_error: 1.1405\n",
            "Epoch 3881/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5797 - mean_absolute_error: 0.4470\n",
            "Epoch 3881: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5793 - mean_absolute_error: 0.4468 - val_loss: 4.9217 - val_mean_absolute_error: 1.0623\n",
            "Epoch 3882/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5118 - mean_absolute_error: 0.4307\n",
            "Epoch 3882: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5121 - mean_absolute_error: 0.4307 - val_loss: 5.3237 - val_mean_absolute_error: 1.0833\n",
            "Epoch 3883/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4904 - mean_absolute_error: 0.4176\n",
            "Epoch 3883: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4914 - mean_absolute_error: 0.4179 - val_loss: 5.1882 - val_mean_absolute_error: 1.0620\n",
            "Epoch 3884/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5486 - mean_absolute_error: 0.4429\n",
            "Epoch 3884: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5497 - mean_absolute_error: 0.4438 - val_loss: 5.0034 - val_mean_absolute_error: 1.0402\n",
            "Epoch 3885/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6210 - mean_absolute_error: 0.4636\n",
            "Epoch 3885: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6215 - mean_absolute_error: 0.4638 - val_loss: 5.2433 - val_mean_absolute_error: 1.0658\n",
            "Epoch 3886/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5677 - mean_absolute_error: 0.4481\n",
            "Epoch 3886: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5738 - mean_absolute_error: 0.4489 - val_loss: 5.2166 - val_mean_absolute_error: 1.0691\n",
            "Epoch 3887/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6489 - mean_absolute_error: 0.4723\n",
            "Epoch 3887: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6487 - mean_absolute_error: 0.4723 - val_loss: 5.5196 - val_mean_absolute_error: 1.1124\n",
            "Epoch 3888/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5270 - mean_absolute_error: 0.4322\n",
            "Epoch 3888: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5315 - mean_absolute_error: 0.4330 - val_loss: 5.3097 - val_mean_absolute_error: 1.0855\n",
            "Epoch 3889/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7046 - mean_absolute_error: 0.4836\n",
            "Epoch 3889: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7044 - mean_absolute_error: 0.4835 - val_loss: 5.2664 - val_mean_absolute_error: 1.0829\n",
            "Epoch 3890/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4931 - mean_absolute_error: 0.4223\n",
            "Epoch 3890: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4930 - mean_absolute_error: 0.4224 - val_loss: 5.4386 - val_mean_absolute_error: 1.0968\n",
            "Epoch 3891/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4870 - mean_absolute_error: 0.4281\n",
            "Epoch 3891: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4899 - mean_absolute_error: 0.4286 - val_loss: 5.2062 - val_mean_absolute_error: 1.1026\n",
            "Epoch 3892/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5500 - mean_absolute_error: 0.4447\n",
            "Epoch 3892: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5508 - mean_absolute_error: 0.4451 - val_loss: 5.3934 - val_mean_absolute_error: 1.0788\n",
            "Epoch 3893/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5360 - mean_absolute_error: 0.4361\n",
            "Epoch 3893: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5372 - mean_absolute_error: 0.4365 - val_loss: 5.6043 - val_mean_absolute_error: 1.1065\n",
            "Epoch 3894/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5717 - mean_absolute_error: 0.4527\n",
            "Epoch 3894: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5799 - mean_absolute_error: 0.4538 - val_loss: 5.4385 - val_mean_absolute_error: 1.1614\n",
            "Epoch 3895/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7242 - mean_absolute_error: 0.4878\n",
            "Epoch 3895: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7239 - mean_absolute_error: 0.4877 - val_loss: 5.6548 - val_mean_absolute_error: 1.1169\n",
            "Epoch 3896/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5362 - mean_absolute_error: 0.4305\n",
            "Epoch 3896: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5366 - mean_absolute_error: 0.4306 - val_loss: 5.2693 - val_mean_absolute_error: 1.0373\n",
            "Epoch 3897/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5346 - mean_absolute_error: 0.4410\n",
            "Epoch 3897: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5345 - mean_absolute_error: 0.4411 - val_loss: 5.7583 - val_mean_absolute_error: 1.1104\n",
            "Epoch 3898/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5482 - mean_absolute_error: 0.4416\n",
            "Epoch 3898: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5492 - mean_absolute_error: 0.4420 - val_loss: 5.8085 - val_mean_absolute_error: 1.1072\n",
            "Epoch 3899/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6330 - mean_absolute_error: 0.4648\n",
            "Epoch 3899: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6328 - mean_absolute_error: 0.4648 - val_loss: 5.6354 - val_mean_absolute_error: 1.0754\n",
            "Epoch 3900/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4584 - mean_absolute_error: 0.4143\n",
            "Epoch 3900: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4583 - mean_absolute_error: 0.4142 - val_loss: 5.0795 - val_mean_absolute_error: 1.0352\n",
            "Epoch 3901/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4775 - mean_absolute_error: 0.4191\n",
            "Epoch 3901: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4781 - mean_absolute_error: 0.4194 - val_loss: 5.4937 - val_mean_absolute_error: 1.0924\n",
            "Epoch 3902/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4887 - mean_absolute_error: 0.4185\n",
            "Epoch 3902: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4887 - mean_absolute_error: 0.4185 - val_loss: 5.5834 - val_mean_absolute_error: 1.0963\n",
            "Epoch 3903/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4873 - mean_absolute_error: 0.4181\n",
            "Epoch 3903: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4880 - mean_absolute_error: 0.4185 - val_loss: 5.5306 - val_mean_absolute_error: 1.0970\n",
            "Epoch 3904/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5138 - mean_absolute_error: 0.4364\n",
            "Epoch 3904: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5136 - mean_absolute_error: 0.4363 - val_loss: 5.6173 - val_mean_absolute_error: 1.0808\n",
            "Epoch 3905/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5438 - mean_absolute_error: 0.4410\n",
            "Epoch 3905: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5523 - mean_absolute_error: 0.4423 - val_loss: 5.3809 - val_mean_absolute_error: 1.0571\n",
            "Epoch 3906/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 2.2412 - mean_absolute_error: 0.7671\n",
            "Epoch 3906: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 2.2404 - mean_absolute_error: 0.7673 - val_loss: 6.0047 - val_mean_absolute_error: 1.1792\n",
            "Epoch 3907/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6765 - mean_absolute_error: 0.4812\n",
            "Epoch 3907: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6763 - mean_absolute_error: 0.4813 - val_loss: 5.3241 - val_mean_absolute_error: 1.1245\n",
            "Epoch 3908/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5256 - mean_absolute_error: 0.4347\n",
            "Epoch 3908: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5258 - mean_absolute_error: 0.4349 - val_loss: 5.2628 - val_mean_absolute_error: 1.0614\n",
            "Epoch 3909/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4529 - mean_absolute_error: 0.4098\n",
            "Epoch 3909: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4530 - mean_absolute_error: 0.4099 - val_loss: 5.1396 - val_mean_absolute_error: 1.0648\n",
            "Epoch 3910/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4698 - mean_absolute_error: 0.4167\n",
            "Epoch 3910: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4707 - mean_absolute_error: 0.4172 - val_loss: 5.1182 - val_mean_absolute_error: 1.0436\n",
            "Epoch 3911/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4728 - mean_absolute_error: 0.4148\n",
            "Epoch 3911: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4725 - mean_absolute_error: 0.4147 - val_loss: 4.8866 - val_mean_absolute_error: 1.0641\n",
            "Epoch 3912/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4619 - mean_absolute_error: 0.4106\n",
            "Epoch 3912: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4619 - mean_absolute_error: 0.4107 - val_loss: 5.5061 - val_mean_absolute_error: 1.0706\n",
            "Epoch 3913/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4899 - mean_absolute_error: 0.4216\n",
            "Epoch 3913: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4910 - mean_absolute_error: 0.4219 - val_loss: 5.5229 - val_mean_absolute_error: 1.0847\n",
            "Epoch 3914/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.4078 - mean_absolute_error: 0.6074\n",
            "Epoch 3914: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.4069 - mean_absolute_error: 0.6073 - val_loss: 5.4553 - val_mean_absolute_error: 1.0830\n",
            "Epoch 3915/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5982 - mean_absolute_error: 0.4596\n",
            "Epoch 3915: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5984 - mean_absolute_error: 0.4598 - val_loss: 5.2025 - val_mean_absolute_error: 1.1426\n",
            "Epoch 3916/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5095 - mean_absolute_error: 0.4282\n",
            "Epoch 3916: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5095 - mean_absolute_error: 0.4284 - val_loss: 5.1523 - val_mean_absolute_error: 1.0531\n",
            "Epoch 3917/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4608 - mean_absolute_error: 0.4101\n",
            "Epoch 3917: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4606 - mean_absolute_error: 0.4101 - val_loss: 5.2924 - val_mean_absolute_error: 1.1092\n",
            "Epoch 3918/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4625 - mean_absolute_error: 0.4144\n",
            "Epoch 3918: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4626 - mean_absolute_error: 0.4146 - val_loss: 5.0456 - val_mean_absolute_error: 1.0684\n",
            "Epoch 3919/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4899 - mean_absolute_error: 0.4151\n",
            "Epoch 3919: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4901 - mean_absolute_error: 0.4152 - val_loss: 4.9902 - val_mean_absolute_error: 1.0509\n",
            "Epoch 3920/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4572 - mean_absolute_error: 0.4136\n",
            "Epoch 3920: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4607 - mean_absolute_error: 0.4143 - val_loss: 4.9225 - val_mean_absolute_error: 1.0361\n",
            "Epoch 3921/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5186 - mean_absolute_error: 0.4364\n",
            "Epoch 3921: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5196 - mean_absolute_error: 0.4367 - val_loss: 4.9672 - val_mean_absolute_error: 1.0376\n",
            "Epoch 3922/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5746 - mean_absolute_error: 0.4442\n",
            "Epoch 3922: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5763 - mean_absolute_error: 0.4447 - val_loss: 5.6130 - val_mean_absolute_error: 1.0878\n",
            "Epoch 3923/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4878 - mean_absolute_error: 0.4231\n",
            "Epoch 3923: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4879 - mean_absolute_error: 0.4234 - val_loss: 5.2312 - val_mean_absolute_error: 1.0720\n",
            "Epoch 3924/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5180 - mean_absolute_error: 0.4327\n",
            "Epoch 3924: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5222 - mean_absolute_error: 0.4331 - val_loss: 5.5520 - val_mean_absolute_error: 1.0936\n",
            "Epoch 3925/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 2.0639 - mean_absolute_error: 0.7257\n",
            "Epoch 3925: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 2.0625 - mean_absolute_error: 0.7256 - val_loss: 5.6730 - val_mean_absolute_error: 1.1394\n",
            "Epoch 3926/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7975 - mean_absolute_error: 0.5030\n",
            "Epoch 3926: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8077 - mean_absolute_error: 0.5045 - val_loss: 5.4832 - val_mean_absolute_error: 1.1329\n",
            "Epoch 3927/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.8111 - mean_absolute_error: 0.5052\n",
            "Epoch 3927: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.8107 - mean_absolute_error: 0.5051 - val_loss: 4.8045 - val_mean_absolute_error: 1.0333\n",
            "Epoch 3928/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5610 - mean_absolute_error: 0.4419\n",
            "Epoch 3928: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5615 - mean_absolute_error: 0.4422 - val_loss: 5.2767 - val_mean_absolute_error: 1.0673\n",
            "Epoch 3929/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5112 - mean_absolute_error: 0.4224\n",
            "Epoch 3929: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5109 - mean_absolute_error: 0.4223 - val_loss: 5.3195 - val_mean_absolute_error: 1.0807\n",
            "Epoch 3930/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5325 - mean_absolute_error: 0.4251\n",
            "Epoch 3930: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5415 - mean_absolute_error: 0.4260 - val_loss: 5.5970 - val_mean_absolute_error: 1.0972\n",
            "Epoch 3931/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7685 - mean_absolute_error: 0.4919\n",
            "Epoch 3931: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7679 - mean_absolute_error: 0.4918 - val_loss: 5.4084 - val_mean_absolute_error: 1.0774\n",
            "Epoch 3932/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5201 - mean_absolute_error: 0.4263\n",
            "Epoch 3932: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5206 - mean_absolute_error: 0.4264 - val_loss: 5.0886 - val_mean_absolute_error: 1.0260\n",
            "Epoch 3933/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5030 - mean_absolute_error: 0.4229\n",
            "Epoch 3933: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5031 - mean_absolute_error: 0.4231 - val_loss: 5.2007 - val_mean_absolute_error: 1.0368\n",
            "Epoch 3934/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4918 - mean_absolute_error: 0.4164\n",
            "Epoch 3934: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4918 - mean_absolute_error: 0.4165 - val_loss: 5.1416 - val_mean_absolute_error: 1.0425\n",
            "Epoch 3935/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4890 - mean_absolute_error: 0.4183\n",
            "Epoch 3935: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4887 - mean_absolute_error: 0.4183 - val_loss: 5.1290 - val_mean_absolute_error: 1.0773\n",
            "Epoch 3936/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4598 - mean_absolute_error: 0.4106\n",
            "Epoch 3936: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4633 - mean_absolute_error: 0.4113 - val_loss: 5.2469 - val_mean_absolute_error: 1.0557\n",
            "Epoch 3937/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6072 - mean_absolute_error: 0.4579\n",
            "Epoch 3937: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6052 - mean_absolute_error: 0.4574 - val_loss: 5.4747 - val_mean_absolute_error: 1.0868\n",
            "Epoch 3938/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6329 - mean_absolute_error: 0.4660\n",
            "Epoch 3938: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6326 - mean_absolute_error: 0.4659 - val_loss: 5.3385 - val_mean_absolute_error: 1.0587\n",
            "Epoch 3939/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4882 - mean_absolute_error: 0.4204\n",
            "Epoch 3939: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4882 - mean_absolute_error: 0.4206 - val_loss: 5.1751 - val_mean_absolute_error: 1.0773\n",
            "Epoch 3940/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4564 - mean_absolute_error: 0.4121\n",
            "Epoch 3940: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4576 - mean_absolute_error: 0.4125 - val_loss: 4.9361 - val_mean_absolute_error: 1.0302\n",
            "Epoch 3941/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5466 - mean_absolute_error: 0.4362\n",
            "Epoch 3941: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5476 - mean_absolute_error: 0.4364 - val_loss: 5.1500 - val_mean_absolute_error: 1.0966\n",
            "Epoch 3942/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5223 - mean_absolute_error: 0.4310\n",
            "Epoch 3942: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5221 - mean_absolute_error: 0.4309 - val_loss: 5.2730 - val_mean_absolute_error: 1.0723\n",
            "Epoch 3943/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4677 - mean_absolute_error: 0.4163\n",
            "Epoch 3943: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4681 - mean_absolute_error: 0.4166 - val_loss: 4.9368 - val_mean_absolute_error: 1.0742\n",
            "Epoch 3944/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5181 - mean_absolute_error: 0.4356\n",
            "Epoch 3944: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5188 - mean_absolute_error: 0.4358 - val_loss: 5.6785 - val_mean_absolute_error: 1.0949\n",
            "Epoch 3945/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5299 - mean_absolute_error: 0.4356\n",
            "Epoch 3945: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5338 - mean_absolute_error: 0.4367 - val_loss: 5.1224 - val_mean_absolute_error: 1.0715\n",
            "Epoch 3946/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5696 - mean_absolute_error: 0.4481\n",
            "Epoch 3946: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5703 - mean_absolute_error: 0.4484 - val_loss: 5.5267 - val_mean_absolute_error: 1.1130\n",
            "Epoch 3947/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5388 - mean_absolute_error: 0.4346\n",
            "Epoch 3947: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5387 - mean_absolute_error: 0.4346 - val_loss: 5.7389 - val_mean_absolute_error: 1.0927\n",
            "Epoch 3948/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4679 - mean_absolute_error: 0.4202\n",
            "Epoch 3948: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4686 - mean_absolute_error: 0.4206 - val_loss: 4.7904 - val_mean_absolute_error: 1.0243\n",
            "Epoch 3949/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7353 - mean_absolute_error: 0.4896\n",
            "Epoch 3949: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7371 - mean_absolute_error: 0.4903 - val_loss: 5.5221 - val_mean_absolute_error: 1.0565\n",
            "Epoch 3950/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5756 - mean_absolute_error: 0.4499\n",
            "Epoch 3950: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5753 - mean_absolute_error: 0.4498 - val_loss: 5.1940 - val_mean_absolute_error: 1.0440\n",
            "Epoch 3951/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4642 - mean_absolute_error: 0.4097\n",
            "Epoch 3951: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4646 - mean_absolute_error: 0.4098 - val_loss: 5.2371 - val_mean_absolute_error: 1.0735\n",
            "Epoch 3952/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5077 - mean_absolute_error: 0.4266\n",
            "Epoch 3952: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5104 - mean_absolute_error: 0.4272 - val_loss: 5.5283 - val_mean_absolute_error: 1.0807\n",
            "Epoch 3953/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5686 - mean_absolute_error: 0.4489\n",
            "Epoch 3953: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5687 - mean_absolute_error: 0.4490 - val_loss: 4.8698 - val_mean_absolute_error: 1.0291\n",
            "Epoch 3954/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4989 - mean_absolute_error: 0.4239\n",
            "Epoch 3954: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5025 - mean_absolute_error: 0.4246 - val_loss: 5.2412 - val_mean_absolute_error: 1.0707\n",
            "Epoch 3955/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5194 - mean_absolute_error: 0.4340\n",
            "Epoch 3955: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5230 - mean_absolute_error: 0.4349 - val_loss: 5.2993 - val_mean_absolute_error: 1.0841\n",
            "Epoch 3956/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6299 - mean_absolute_error: 0.4624\n",
            "Epoch 3956: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6296 - mean_absolute_error: 0.4624 - val_loss: 5.4654 - val_mean_absolute_error: 1.1951\n",
            "Epoch 3957/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5347 - mean_absolute_error: 0.4399\n",
            "Epoch 3957: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5361 - mean_absolute_error: 0.4404 - val_loss: 5.4310 - val_mean_absolute_error: 1.0700\n",
            "Epoch 3958/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5632 - mean_absolute_error: 0.4465\n",
            "Epoch 3958: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5638 - mean_absolute_error: 0.4468 - val_loss: 5.3151 - val_mean_absolute_error: 1.0679\n",
            "Epoch 3959/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7133 - mean_absolute_error: 0.4812\n",
            "Epoch 3959: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7140 - mean_absolute_error: 0.4813 - val_loss: 5.0047 - val_mean_absolute_error: 1.0362\n",
            "Epoch 3960/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5025 - mean_absolute_error: 0.4227\n",
            "Epoch 3960: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5025 - mean_absolute_error: 0.4228 - val_loss: 5.2339 - val_mean_absolute_error: 1.0807\n",
            "Epoch 3961/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1792 - mean_absolute_error: 0.5439\n",
            "Epoch 3961: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1788 - mean_absolute_error: 0.5440 - val_loss: 4.8989 - val_mean_absolute_error: 1.1010\n",
            "Epoch 3962/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7426 - mean_absolute_error: 0.4905\n",
            "Epoch 3962: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7427 - mean_absolute_error: 0.4906 - val_loss: 4.9960 - val_mean_absolute_error: 1.0283\n",
            "Epoch 3963/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4981 - mean_absolute_error: 0.4225\n",
            "Epoch 3963: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4992 - mean_absolute_error: 0.4230 - val_loss: 5.0985 - val_mean_absolute_error: 1.0458\n",
            "Epoch 3964/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4825 - mean_absolute_error: 0.4184\n",
            "Epoch 3964: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4826 - mean_absolute_error: 0.4185 - val_loss: 5.2981 - val_mean_absolute_error: 1.1016\n",
            "Epoch 3965/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4895 - mean_absolute_error: 0.4177\n",
            "Epoch 3965: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4901 - mean_absolute_error: 0.4177 - val_loss: 5.5588 - val_mean_absolute_error: 1.0951\n",
            "Epoch 3966/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6757 - mean_absolute_error: 0.4721\n",
            "Epoch 3966: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6756 - mean_absolute_error: 0.4721 - val_loss: 5.2660 - val_mean_absolute_error: 1.1373\n",
            "Epoch 3967/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5525 - mean_absolute_error: 0.4419\n",
            "Epoch 3967: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5724 - mean_absolute_error: 0.4437 - val_loss: 6.0421 - val_mean_absolute_error: 1.1929\n",
            "Epoch 3968/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.6300 - mean_absolute_error: 0.6586\n",
            "Epoch 3968: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.6285 - mean_absolute_error: 0.6583 - val_loss: 5.6362 - val_mean_absolute_error: 1.1279\n",
            "Epoch 3969/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5769 - mean_absolute_error: 0.4523\n",
            "Epoch 3969: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5765 - mean_absolute_error: 0.4522 - val_loss: 5.1275 - val_mean_absolute_error: 1.0511\n",
            "Epoch 3970/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5052 - mean_absolute_error: 0.4197\n",
            "Epoch 3970: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5082 - mean_absolute_error: 0.4203 - val_loss: 4.8254 - val_mean_absolute_error: 1.0653\n",
            "Epoch 3971/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4673 - mean_absolute_error: 0.4148\n",
            "Epoch 3971: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4675 - mean_absolute_error: 0.4148 - val_loss: 5.1012 - val_mean_absolute_error: 1.0322\n",
            "Epoch 3972/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4405 - mean_absolute_error: 0.4055\n",
            "Epoch 3972: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4431 - mean_absolute_error: 0.4061 - val_loss: 5.1609 - val_mean_absolute_error: 1.0567\n",
            "Epoch 3973/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5124 - mean_absolute_error: 0.4266\n",
            "Epoch 3973: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5125 - mean_absolute_error: 0.4266 - val_loss: 5.2879 - val_mean_absolute_error: 1.0816\n",
            "Epoch 3974/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4319 - mean_absolute_error: 0.4067\n",
            "Epoch 3974: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4320 - mean_absolute_error: 0.4068 - val_loss: 5.0554 - val_mean_absolute_error: 1.0384\n",
            "Epoch 3975/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4478 - mean_absolute_error: 0.4111\n",
            "Epoch 3975: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4484 - mean_absolute_error: 0.4112 - val_loss: 5.2857 - val_mean_absolute_error: 1.0724\n",
            "Epoch 3976/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4721 - mean_absolute_error: 0.4179\n",
            "Epoch 3976: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4739 - mean_absolute_error: 0.4186 - val_loss: 5.2013 - val_mean_absolute_error: 1.0392\n",
            "Epoch 3977/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4674 - mean_absolute_error: 0.4127\n",
            "Epoch 3977: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4701 - mean_absolute_error: 0.4133 - val_loss: 5.4155 - val_mean_absolute_error: 1.0596\n",
            "Epoch 3978/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4755 - mean_absolute_error: 0.4146\n",
            "Epoch 3978: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4776 - mean_absolute_error: 0.4151 - val_loss: 5.7640 - val_mean_absolute_error: 1.1370\n",
            "Epoch 3979/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6103 - mean_absolute_error: 0.4565\n",
            "Epoch 3979: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6110 - mean_absolute_error: 0.4567 - val_loss: 5.6935 - val_mean_absolute_error: 1.0829\n",
            "Epoch 3980/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.6736 - mean_absolute_error: 0.4719\n",
            "Epoch 3980: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.6720 - mean_absolute_error: 0.4716 - val_loss: 5.5174 - val_mean_absolute_error: 1.0903\n",
            "Epoch 3981/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4914 - mean_absolute_error: 0.4239\n",
            "Epoch 3981: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4915 - mean_absolute_error: 0.4240 - val_loss: 5.3455 - val_mean_absolute_error: 1.0792\n",
            "Epoch 3982/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4673 - mean_absolute_error: 0.4166\n",
            "Epoch 3982: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4671 - mean_absolute_error: 0.4166 - val_loss: 5.5117 - val_mean_absolute_error: 1.0692\n",
            "Epoch 3983/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4689 - mean_absolute_error: 0.4135\n",
            "Epoch 3983: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4691 - mean_absolute_error: 0.4138 - val_loss: 5.1713 - val_mean_absolute_error: 1.0582\n",
            "Epoch 3984/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4804 - mean_absolute_error: 0.4158\n",
            "Epoch 3984: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4807 - mean_absolute_error: 0.4160 - val_loss: 5.2720 - val_mean_absolute_error: 1.0507\n",
            "Epoch 3985/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5123 - mean_absolute_error: 0.4286\n",
            "Epoch 3985: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5124 - mean_absolute_error: 0.4288 - val_loss: 5.0685 - val_mean_absolute_error: 1.0517\n",
            "Epoch 3986/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5043 - mean_absolute_error: 0.4246\n",
            "Epoch 3986: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5060 - mean_absolute_error: 0.4254 - val_loss: 5.0555 - val_mean_absolute_error: 1.0388\n",
            "Epoch 3987/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4662 - mean_absolute_error: 0.4132\n",
            "Epoch 3987: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4677 - mean_absolute_error: 0.4137 - val_loss: 5.5060 - val_mean_absolute_error: 1.0824\n",
            "Epoch 3988/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.3303 - mean_absolute_error: 0.5985\n",
            "Epoch 3988: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.3293 - mean_absolute_error: 0.5984 - val_loss: 5.4183 - val_mean_absolute_error: 1.0691\n",
            "Epoch 3989/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5668 - mean_absolute_error: 0.4437\n",
            "Epoch 3989: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5674 - mean_absolute_error: 0.4438 - val_loss: 4.9879 - val_mean_absolute_error: 1.0553\n",
            "Epoch 3990/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4928 - mean_absolute_error: 0.4229\n",
            "Epoch 3990: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4932 - mean_absolute_error: 0.4230 - val_loss: 5.0087 - val_mean_absolute_error: 1.0218\n",
            "Epoch 3991/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4870 - mean_absolute_error: 0.4185\n",
            "Epoch 3991: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4868 - mean_absolute_error: 0.4185 - val_loss: 4.9622 - val_mean_absolute_error: 1.0412\n",
            "Epoch 3992/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4601 - mean_absolute_error: 0.4163\n",
            "Epoch 3992: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4600 - mean_absolute_error: 0.4164 - val_loss: 5.1391 - val_mean_absolute_error: 1.0571\n",
            "Epoch 3993/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4518 - mean_absolute_error: 0.4087\n",
            "Epoch 3993: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4522 - mean_absolute_error: 0.4090 - val_loss: 5.0187 - val_mean_absolute_error: 1.0455\n",
            "Epoch 3994/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5607 - mean_absolute_error: 0.4489\n",
            "Epoch 3994: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5610 - mean_absolute_error: 0.4491 - val_loss: 5.1346 - val_mean_absolute_error: 1.0508\n",
            "Epoch 3995/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4532 - mean_absolute_error: 0.4133\n",
            "Epoch 3995: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4538 - mean_absolute_error: 0.4136 - val_loss: 6.3347 - val_mean_absolute_error: 1.1400\n",
            "Epoch 3996/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4997 - mean_absolute_error: 0.4271\n",
            "Epoch 3996: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5029 - mean_absolute_error: 0.4276 - val_loss: 4.9782 - val_mean_absolute_error: 1.0417\n",
            "Epoch 3997/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5544 - mean_absolute_error: 0.4431\n",
            "Epoch 3997: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5543 - mean_absolute_error: 0.4431 - val_loss: 4.9701 - val_mean_absolute_error: 1.0357\n",
            "Epoch 3998/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4882 - mean_absolute_error: 0.4188\n",
            "Epoch 3998: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4898 - mean_absolute_error: 0.4191 - val_loss: 5.2307 - val_mean_absolute_error: 1.1425\n",
            "Epoch 3999/10000\n",
            "188/188 [==============================] - ETA: 0s - loss: 0.6300 - mean_absolute_error: 0.4643\n",
            "Epoch 3999: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6300 - mean_absolute_error: 0.4643 - val_loss: 5.5884 - val_mean_absolute_error: 1.1224\n",
            "Epoch 4000/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5713 - mean_absolute_error: 0.4462\n",
            "Epoch 4000: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5728 - mean_absolute_error: 0.4465 - val_loss: 6.0206 - val_mean_absolute_error: 1.0991\n",
            "Epoch 4001/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5408 - mean_absolute_error: 0.4361\n",
            "Epoch 4001: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5414 - mean_absolute_error: 0.4363 - val_loss: 5.5821 - val_mean_absolute_error: 1.1364\n",
            "Epoch 4002/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5019 - mean_absolute_error: 0.4248\n",
            "Epoch 4002: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5029 - mean_absolute_error: 0.4253 - val_loss: 5.7135 - val_mean_absolute_error: 1.1037\n",
            "Epoch 4003/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5945 - mean_absolute_error: 0.4593\n",
            "Epoch 4003: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5950 - mean_absolute_error: 0.4595 - val_loss: 5.0592 - val_mean_absolute_error: 1.0367\n",
            "Epoch 4004/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4939 - mean_absolute_error: 0.4280\n",
            "Epoch 4004: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4936 - mean_absolute_error: 0.4278 - val_loss: 4.7634 - val_mean_absolute_error: 1.0393\n",
            "Epoch 4005/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4585 - mean_absolute_error: 0.4162\n",
            "Epoch 4005: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4636 - mean_absolute_error: 0.4168 - val_loss: 4.9909 - val_mean_absolute_error: 1.0298\n",
            "Epoch 4006/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4785 - mean_absolute_error: 0.4191\n",
            "Epoch 4006: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4801 - mean_absolute_error: 0.4195 - val_loss: 5.0717 - val_mean_absolute_error: 1.0190\n",
            "Epoch 4007/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5055 - mean_absolute_error: 0.4288\n",
            "Epoch 4007: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5054 - mean_absolute_error: 0.4290 - val_loss: 4.9279 - val_mean_absolute_error: 1.0479\n",
            "Epoch 4008/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5370 - mean_absolute_error: 0.4361\n",
            "Epoch 4008: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5370 - mean_absolute_error: 0.4362 - val_loss: 5.2781 - val_mean_absolute_error: 1.1415\n",
            "Epoch 4009/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5281 - mean_absolute_error: 0.4360\n",
            "Epoch 4009: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5280 - mean_absolute_error: 0.4361 - val_loss: 5.1492 - val_mean_absolute_error: 1.0577\n",
            "Epoch 4010/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5074 - mean_absolute_error: 0.4311\n",
            "Epoch 4010: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5078 - mean_absolute_error: 0.4314 - val_loss: 5.3823 - val_mean_absolute_error: 1.0827\n",
            "Epoch 4011/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5666 - mean_absolute_error: 0.4543\n",
            "Epoch 4011: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5664 - mean_absolute_error: 0.4543 - val_loss: 5.5187 - val_mean_absolute_error: 1.1054\n",
            "Epoch 4012/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5559 - mean_absolute_error: 0.4427\n",
            "Epoch 4012: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5562 - mean_absolute_error: 0.4428 - val_loss: 5.1146 - val_mean_absolute_error: 1.0597\n",
            "Epoch 4013/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4691 - mean_absolute_error: 0.4207\n",
            "Epoch 4013: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4708 - mean_absolute_error: 0.4211 - val_loss: 5.4699 - val_mean_absolute_error: 1.0875\n",
            "Epoch 4014/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 3.6021 - mean_absolute_error: 0.9234\n",
            "Epoch 4014: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 3.6005 - mean_absolute_error: 0.9231 - val_loss: 4.7823 - val_mean_absolute_error: 1.0770\n",
            "Epoch 4015/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 1.1021 - mean_absolute_error: 0.5663\n",
            "Epoch 4015: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 1.1013 - mean_absolute_error: 0.5662 - val_loss: 4.6911 - val_mean_absolute_error: 1.0694\n",
            "Epoch 4016/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6724 - mean_absolute_error: 0.4757\n",
            "Epoch 4016: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6744 - mean_absolute_error: 0.4761 - val_loss: 4.6746 - val_mean_absolute_error: 1.0318\n",
            "Epoch 4017/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5705 - mean_absolute_error: 0.4445\n",
            "Epoch 4017: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5712 - mean_absolute_error: 0.4446 - val_loss: 4.4554 - val_mean_absolute_error: 0.9915\n",
            "Epoch 4018/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5337 - mean_absolute_error: 0.4358\n",
            "Epoch 4018: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5333 - mean_absolute_error: 0.4356 - val_loss: 4.5235 - val_mean_absolute_error: 1.0111\n",
            "Epoch 4019/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4772 - mean_absolute_error: 0.4171\n",
            "Epoch 4019: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4773 - mean_absolute_error: 0.4172 - val_loss: 4.7166 - val_mean_absolute_error: 1.0463\n",
            "Epoch 4020/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4700 - mean_absolute_error: 0.4144\n",
            "Epoch 4020: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4706 - mean_absolute_error: 0.4146 - val_loss: 4.9861 - val_mean_absolute_error: 1.0254\n",
            "Epoch 4021/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4813 - mean_absolute_error: 0.4174\n",
            "Epoch 4021: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4828 - mean_absolute_error: 0.4177 - val_loss: 4.8021 - val_mean_absolute_error: 1.0330\n",
            "Epoch 4022/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4467 - mean_absolute_error: 0.4041\n",
            "Epoch 4022: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4466 - mean_absolute_error: 0.4041 - val_loss: 4.8864 - val_mean_absolute_error: 1.0397\n",
            "Epoch 4023/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4377 - mean_absolute_error: 0.3994\n",
            "Epoch 4023: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4380 - mean_absolute_error: 0.3995 - val_loss: 5.0157 - val_mean_absolute_error: 1.0370\n",
            "Epoch 4024/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4364 - mean_absolute_error: 0.4001\n",
            "Epoch 4024: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4374 - mean_absolute_error: 0.4005 - val_loss: 5.1169 - val_mean_absolute_error: 1.0737\n",
            "Epoch 4025/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4515 - mean_absolute_error: 0.4084\n",
            "Epoch 4025: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4525 - mean_absolute_error: 0.4088 - val_loss: 5.1564 - val_mean_absolute_error: 1.0955\n",
            "Epoch 4026/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4695 - mean_absolute_error: 0.4156\n",
            "Epoch 4026: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4728 - mean_absolute_error: 0.4162 - val_loss: 4.7475 - val_mean_absolute_error: 1.0155\n",
            "Epoch 4027/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5255 - mean_absolute_error: 0.4348\n",
            "Epoch 4027: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5254 - mean_absolute_error: 0.4349 - val_loss: 4.9906 - val_mean_absolute_error: 1.0378\n",
            "Epoch 4028/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4841 - mean_absolute_error: 0.4211\n",
            "Epoch 4028: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4887 - mean_absolute_error: 0.4219 - val_loss: 5.4760 - val_mean_absolute_error: 1.0576\n",
            "Epoch 4029/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4987 - mean_absolute_error: 0.4246\n",
            "Epoch 4029: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5011 - mean_absolute_error: 0.4253 - val_loss: 5.2523 - val_mean_absolute_error: 1.0488\n",
            "Epoch 4030/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4825 - mean_absolute_error: 0.4222\n",
            "Epoch 4030: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4815 - mean_absolute_error: 0.4220 - val_loss: 5.3126 - val_mean_absolute_error: 1.0646\n",
            "Epoch 4031/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4590 - mean_absolute_error: 0.4127\n",
            "Epoch 4031: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4618 - mean_absolute_error: 0.4132 - val_loss: 5.0670 - val_mean_absolute_error: 1.0436\n",
            "Epoch 4032/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4948 - mean_absolute_error: 0.4262\n",
            "Epoch 4032: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4951 - mean_absolute_error: 0.4264 - val_loss: 4.8081 - val_mean_absolute_error: 1.0655\n",
            "Epoch 4033/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4830 - mean_absolute_error: 0.4206\n",
            "Epoch 4033: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4842 - mean_absolute_error: 0.4210 - val_loss: 5.3687 - val_mean_absolute_error: 1.0624\n",
            "Epoch 4034/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5341 - mean_absolute_error: 0.4409\n",
            "Epoch 4034: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5341 - mean_absolute_error: 0.4409 - val_loss: 4.6355 - val_mean_absolute_error: 1.0086\n",
            "Epoch 4035/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4576 - mean_absolute_error: 0.4122\n",
            "Epoch 4035: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4575 - mean_absolute_error: 0.4122 - val_loss: 5.1932 - val_mean_absolute_error: 1.0577\n",
            "Epoch 4036/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5036 - mean_absolute_error: 0.4182\n",
            "Epoch 4036: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5053 - mean_absolute_error: 0.4185 - val_loss: 5.2922 - val_mean_absolute_error: 1.0816\n",
            "Epoch 4037/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.5185 - mean_absolute_error: 0.4308\n",
            "Epoch 4037: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5200 - mean_absolute_error: 0.4311 - val_loss: 4.9153 - val_mean_absolute_error: 1.0867\n",
            "Epoch 4038/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4975 - mean_absolute_error: 0.4323\n",
            "Epoch 4038: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4980 - mean_absolute_error: 0.4327 - val_loss: 5.2322 - val_mean_absolute_error: 1.0754\n",
            "Epoch 4039/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4941 - mean_absolute_error: 0.4257\n",
            "Epoch 4039: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4944 - mean_absolute_error: 0.4259 - val_loss: 5.0942 - val_mean_absolute_error: 1.1361\n",
            "Epoch 4040/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4795 - mean_absolute_error: 0.4188\n",
            "Epoch 4040: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4846 - mean_absolute_error: 0.4199 - val_loss: 5.4329 - val_mean_absolute_error: 1.1000\n",
            "Epoch 4041/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.9773 - mean_absolute_error: 0.5418\n",
            "Epoch 4041: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.9832 - mean_absolute_error: 0.5427 - val_loss: 5.6634 - val_mean_absolute_error: 1.0822\n",
            "Epoch 4042/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.6533 - mean_absolute_error: 0.4689\n",
            "Epoch 4042: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.6532 - mean_absolute_error: 0.4689 - val_loss: 5.4275 - val_mean_absolute_error: 1.0624\n",
            "Epoch 4043/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.4743 - mean_absolute_error: 0.4163\n",
            "Epoch 4043: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4736 - mean_absolute_error: 0.4162 - val_loss: 5.2523 - val_mean_absolute_error: 1.0629\n",
            "Epoch 4044/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4646 - mean_absolute_error: 0.4105\n",
            "Epoch 4044: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4654 - mean_absolute_error: 0.4107 - val_loss: 5.3094 - val_mean_absolute_error: 1.0840\n",
            "Epoch 4045/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4970 - mean_absolute_error: 0.4209\n",
            "Epoch 4045: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5038 - mean_absolute_error: 0.4219 - val_loss: 5.0971 - val_mean_absolute_error: 1.0462\n",
            "Epoch 4046/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4636 - mean_absolute_error: 0.4201\n",
            "Epoch 4046: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4652 - mean_absolute_error: 0.4205 - val_loss: 5.3731 - val_mean_absolute_error: 1.0604\n",
            "Epoch 4047/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4902 - mean_absolute_error: 0.4264\n",
            "Epoch 4047: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4908 - mean_absolute_error: 0.4265 - val_loss: 5.2957 - val_mean_absolute_error: 1.0970\n",
            "Epoch 4048/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5483 - mean_absolute_error: 0.4394\n",
            "Epoch 4048: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5480 - mean_absolute_error: 0.4393 - val_loss: 4.7883 - val_mean_absolute_error: 1.0343\n",
            "Epoch 4049/10000\n",
            "186/188 [============================>.] - ETA: 0s - loss: 0.8291 - mean_absolute_error: 0.5070\n",
            "Epoch 4049: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.8322 - mean_absolute_error: 0.5072 - val_loss: 6.1825 - val_mean_absolute_error: 1.1820\n",
            "Epoch 4050/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7398 - mean_absolute_error: 0.4862\n",
            "Epoch 4050: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7437 - mean_absolute_error: 0.4873 - val_loss: 5.6956 - val_mean_absolute_error: 1.1968\n",
            "Epoch 4051/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7785 - mean_absolute_error: 0.4963\n",
            "Epoch 4051: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7786 - mean_absolute_error: 0.4965 - val_loss: 4.9552 - val_mean_absolute_error: 1.0424\n",
            "Epoch 4052/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5035 - mean_absolute_error: 0.4249\n",
            "Epoch 4052: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5041 - mean_absolute_error: 0.4251 - val_loss: 5.4581 - val_mean_absolute_error: 1.1156\n",
            "Epoch 4053/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4561 - mean_absolute_error: 0.4115\n",
            "Epoch 4053: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4559 - mean_absolute_error: 0.4115 - val_loss: 4.7631 - val_mean_absolute_error: 1.0436\n",
            "Epoch 4054/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4616 - mean_absolute_error: 0.4094\n",
            "Epoch 4054: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4622 - mean_absolute_error: 0.4098 - val_loss: 5.0460 - val_mean_absolute_error: 1.0794\n",
            "Epoch 4055/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5641 - mean_absolute_error: 0.4382\n",
            "Epoch 4055: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5680 - mean_absolute_error: 0.4387 - val_loss: 5.2059 - val_mean_absolute_error: 1.0902\n",
            "Epoch 4056/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.7770 - mean_absolute_error: 0.5007\n",
            "Epoch 4056: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.7766 - mean_absolute_error: 0.5006 - val_loss: 5.2495 - val_mean_absolute_error: 1.0512\n",
            "Epoch 4057/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5017 - mean_absolute_error: 0.4252\n",
            "Epoch 4057: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5013 - mean_absolute_error: 0.4250 - val_loss: 5.2761 - val_mean_absolute_error: 1.0513\n",
            "Epoch 4058/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4478 - mean_absolute_error: 0.4071\n",
            "Epoch 4058: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4480 - mean_absolute_error: 0.4073 - val_loss: 4.8603 - val_mean_absolute_error: 1.0335\n",
            "Epoch 4059/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4867 - mean_absolute_error: 0.4162\n",
            "Epoch 4059: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4865 - mean_absolute_error: 0.4162 - val_loss: 5.1457 - val_mean_absolute_error: 1.0560\n",
            "Epoch 4060/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4955 - mean_absolute_error: 0.4185\n",
            "Epoch 4060: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4956 - mean_absolute_error: 0.4186 - val_loss: 5.0642 - val_mean_absolute_error: 1.0435\n",
            "Epoch 4061/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4749 - mean_absolute_error: 0.4153\n",
            "Epoch 4061: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4755 - mean_absolute_error: 0.4156 - val_loss: 5.0322 - val_mean_absolute_error: 1.0353\n",
            "Epoch 4062/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4675 - mean_absolute_error: 0.4193\n",
            "Epoch 4062: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4674 - mean_absolute_error: 0.4193 - val_loss: 4.9545 - val_mean_absolute_error: 1.0283\n",
            "Epoch 4063/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4779 - mean_absolute_error: 0.4179\n",
            "Epoch 4063: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4790 - mean_absolute_error: 0.4184 - val_loss: 5.7789 - val_mean_absolute_error: 1.1108\n",
            "Epoch 4064/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4916 - mean_absolute_error: 0.4252\n",
            "Epoch 4064: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.4925 - mean_absolute_error: 0.4254 - val_loss: 5.3363 - val_mean_absolute_error: 1.0773\n",
            "Epoch 4065/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5084 - mean_absolute_error: 0.4284\n",
            "Epoch 4065: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5102 - mean_absolute_error: 0.4289 - val_loss: 5.0492 - val_mean_absolute_error: 1.0353\n",
            "Epoch 4066/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5115 - mean_absolute_error: 0.4240\n",
            "Epoch 4066: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5117 - mean_absolute_error: 0.4242 - val_loss: 5.1559 - val_mean_absolute_error: 1.0576\n",
            "Epoch 4067/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5325 - mean_absolute_error: 0.4292\n",
            "Epoch 4067: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5327 - mean_absolute_error: 0.4294 - val_loss: 5.1717 - val_mean_absolute_error: 1.1628\n",
            "Epoch 4068/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5181 - mean_absolute_error: 0.4276\n",
            "Epoch 4068: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5181 - mean_absolute_error: 0.4277 - val_loss: 5.6170 - val_mean_absolute_error: 1.1037\n",
            "Epoch 4069/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4891 - mean_absolute_error: 0.4276\n",
            "Epoch 4069: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4893 - mean_absolute_error: 0.4276 - val_loss: 4.9670 - val_mean_absolute_error: 1.0301\n",
            "Epoch 4070/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5133 - mean_absolute_error: 0.4264\n",
            "Epoch 4070: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5133 - mean_absolute_error: 0.4265 - val_loss: 4.8648 - val_mean_absolute_error: 1.0588\n",
            "Epoch 4071/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5294 - mean_absolute_error: 0.4323\n",
            "Epoch 4071: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5294 - mean_absolute_error: 0.4323 - val_loss: 5.1963 - val_mean_absolute_error: 1.1071\n",
            "Epoch 4072/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5854 - mean_absolute_error: 0.4478\n",
            "Epoch 4072: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5887 - mean_absolute_error: 0.4487 - val_loss: 5.2175 - val_mean_absolute_error: 1.0595\n",
            "Epoch 4073/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5607 - mean_absolute_error: 0.4456\n",
            "Epoch 4073: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5617 - mean_absolute_error: 0.4460 - val_loss: 5.1980 - val_mean_absolute_error: 1.1152\n",
            "Epoch 4074/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4775 - mean_absolute_error: 0.4186\n",
            "Epoch 4074: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4772 - mean_absolute_error: 0.4186 - val_loss: 4.7806 - val_mean_absolute_error: 1.0422\n",
            "Epoch 4075/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.4785 - mean_absolute_error: 0.4139\n",
            "Epoch 4075: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.4827 - mean_absolute_error: 0.4148 - val_loss: 4.6279 - val_mean_absolute_error: 1.0143\n",
            "Epoch 4076/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5360 - mean_absolute_error: 0.4381\n",
            "Epoch 4076: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5363 - mean_absolute_error: 0.4383 - val_loss: 4.8800 - val_mean_absolute_error: 1.0650\n",
            "Epoch 4077/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5102 - mean_absolute_error: 0.4189\n",
            "Epoch 4077: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5111 - mean_absolute_error: 0.4191 - val_loss: 5.3269 - val_mean_absolute_error: 1.1450\n",
            "Epoch 4078/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5480 - mean_absolute_error: 0.4389\n",
            "Epoch 4078: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5478 - mean_absolute_error: 0.4388 - val_loss: 5.4362 - val_mean_absolute_error: 1.0951\n",
            "Epoch 4079/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5081 - mean_absolute_error: 0.4290\n",
            "Epoch 4079: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 22ms/step - loss: 0.5082 - mean_absolute_error: 0.4290 - val_loss: 5.6089 - val_mean_absolute_error: 1.1404\n",
            "Epoch 4080/10000\n",
            "187/188 [============================>.] - ETA: 0s - loss: 0.5424 - mean_absolute_error: 0.4441\n",
            "Epoch 4080: val_loss did not improve from 3.88748\n",
            "188/188 [==============================] - 4s 23ms/step - loss: 0.5433 - mean_absolute_error: 0.4445 - val_loss: 5.2423 - val_mean_absolute_error: 1.1780\n",
            "Epoch 4081/10000\n",
            "154/188 [=======================>......] - ETA: 0s - loss: 0.5303 - mean_absolute_error: 0.4315"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6fbd05c0e1d8>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# After training, you can load the best weights using the following:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m combined_model.fit([X_Train3, X_Trainnum], YData.values,\n\u001b[0m\u001b[1;32m     30\u001b[0m                              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                              validation_data=([X_val3, X_valnum], YDataval.values),callbacks=[checkpoint_callback],batch_size=100,shuffle=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# Example usage\n",
        "sequence_length = 500  # Adjust according to your data\n",
        "num_variables = 3  # Adjust according to your data\n",
        "num_numeric_features = 1  # Adjust according to your data\n",
        "\n",
        "input_shape_time_series = (sequence_length, num_variables)\n",
        "input_shape_numeric = (num_numeric_features,)\n",
        "\n",
        "combined_model = CombinedModelWithLSTM(input_shape_time_series, num_numeric_features)\n",
        "\n",
        "# Compile the model for regression\n",
        "combined_model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "\n",
        "\n",
        "\n",
        "# Define ModelCheckpoint callback to save the model weights\n",
        "checkpoint_callback = ModelCheckpoint(filepath='lstm_IA_fi_2.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      save_best_only=True,\n",
        "                                      save_weights_only=True,\n",
        "                                      mode='min',  # or 'max' depending on your metric\n",
        "                                      verbose=1)\n",
        "\n",
        "# Train the model with your data and use the ModelCheckpoint callback\n",
        "# Assuming you have 'train_data', 'train_labels', 'val_data', and 'val_labels'\n",
        "\n",
        "\n",
        "# After training, you can load the best weights using the following:\n",
        "combined_model.fit([X_Train3, X_Trainnum], YData.values,\n",
        "                             epochs=10000,\n",
        "                             validation_data=([X_val3, X_valnum], YDataval.values),callbacks=[checkpoint_callback],batch_size=100,shuffle=True)\n",
        "\n",
        "y_pred = combined_model.predict([X_Test3,X_Testnum])\n",
        "np.sum(np.square(YDatatest.values - y_pred.flatten()))/len(y_pred)\n",
        "\n",
        "model2 = CombinedModelWithLSTM(input_shape_time_series=(input_shape_time_series), num_numeric_features=(num_numeric_features))\n",
        "dummy_input = (tf.zeros((1,) + input_shape_time_series), tf.zeros((1, num_numeric_features)))\n",
        "model2(dummy_input)\n",
        "\n",
        "# Load the weights\n",
        "\n",
        "model2.load_weights('lstm_IA_fi_3.h5')\n",
        "\n",
        "\n",
        "y_pred = model2.predict([X_Test3,X_Testnum])\n",
        "np.sum(np.square(YDatatest.values - y_pred.flatten()))/len(y_pred)\n",
        "\n",
        "\n",
        "plt.grid( which='major', color='k', linestyle='--')\n",
        "plt.grid(which='minor', color='r', linestyle='-', alpha=0.2)\n",
        "plt.plot(YDatatest.values)\n",
        "plt.plot(y_pred)\n",
        "plt.minorticks_on()\n",
        "plt.xlabel(\"Cycles\")\n",
        "plt.ylabel(\"CA50-Inj\")\n",
        "plt.legend(['actual','predicted'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.grid( which='major', color='k', linestyle='--')\n",
        "plt.grid(which='minor', color='r', linestyle='-', alpha=0.2)\n",
        "plt.plot((np.abs(YDatatest.values - y_pred.flatten())))\n",
        "plt.plot((np.abs(YDatatest.values - y_pred.flatten())),'+')\n",
        "plt.axhline(y=1, color='red', linestyle='--', label='y=5')\n",
        "\n",
        "plt.ylabel(\"Error(degree)\")\n",
        "plt.xlabel(\"Cycles\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.grid( which='major', color='k', linestyle='--')\n",
        "plt.grid(which='minor', color='r', linestyle='-', alpha=0.2)\n",
        "plt.plot((np.abs(YDatatest.values - y_pred.flatten()))*100/np.abs(YDatatest.values))\n",
        "plt.plot((np.abs(YDatatest.values - y_pred.flatten()))*100/np.abs(YDatatest.values),'+')\n",
        "plt.axhline(y=5, color='red', linestyle='--', label='y=5')\n",
        "\n",
        "plt.ylabel(\"Error %\")\n",
        "plt.xlabel(\"Cycles\")\n",
        "\n",
        "\n",
        "# Calculate RMSE for every 9 samples\n",
        "window_size = 97\n",
        "window_size2 = 7760/97\n",
        "\n",
        "rmse_list = []\n",
        "\n",
        "for i in range(0, len(y_pred), window_size):\n",
        "    # Extract a window of 9 samples\n",
        "    window = y_pred[i:i + window_size]\n",
        "    value =  YDatatest[i:i + window_size]\n",
        "    # Calculate the RMSE for the window\n",
        "    rmse = sqrt(mean_squared_error(window,value))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "# Print the RMSE values for each window\n",
        "for i, rmse in enumerate(rmse_list):\n",
        "    print(f\"RMSE for samples {i * window_size + 1}-{(i + 1) * window_size}: {rmse}\")\n",
        "plt.plot(np.arange(1,window_size2+1),rmse_list)\n",
        "plt.scatter(np.arange(1,window_size2+1),rmse_list)\n",
        "\n",
        "plt.xlabel(\"Operating Conditions\")\n",
        "plt.ylabel(\"RMSE(deg)\")\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(YDatatest)\n",
        "plt.minorticks_on()\n",
        "plt.xlabel(\"Cycles\")\n",
        "plt.ylabel(\"CA50-Inj(deg)\")\n",
        "plt.legend(['actual'])\n",
        "\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot(YDatatest)\n",
        "plt.plot(y_pred)\n",
        "plt.minorticks_on()\n",
        "plt.xlabel(\"Cycles\")\n",
        "plt.ylabel(\"CA50-Inj(deg)\")\n",
        "plt.legend(['actual','predicted'])\n",
        "\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.plot(np.arange(1,window_size2+1),rmse_list)\n",
        "plt.scatter(np.arange(1,window_size2+1),rmse_list)\n",
        "plt.axhline(y=1, color='red', linestyle='--', label='y=5')\n",
        "\n",
        "plt.xlabel(\"Operating Conditions\")\n",
        "plt.ylabel(\"RMSE(deg)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "yWhF9-SNl133",
        "outputId": "c76edbf8-8f8e-4b13-cce2-7f29d3b8acf2"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f1220914ed98>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mYData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXData3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mXData3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0mYDatatest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXData3test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mXData3test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0mYDataval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXData3val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mXData3val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m         \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_tuple_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1464\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1557\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ],
      "source": [
        "\n",
        "path1 ='/content/sample_data/TimeS/TimeS301LMS/'\n",
        "path2 ='/content/sample_data/TimeS/TimeS301RMS/'\n",
        "path3 ='/content/sample_data/TimeS/TimeS301D/'\n",
        "path4 ='/content/sample_data/TimeS/CA5301/'\n",
        "\n",
        "fnames = glob.glob(path1+'*.csv')\n",
        "\n",
        "\n",
        "\n",
        "fnames2 = []\n",
        "i = 0\n",
        "for f in fnames:\n",
        " fnames2.append(os.path.splitext(os.path.split(f)[-1])[-2])\n",
        " i = i + 1\n",
        "\n",
        "operation_condition = [int(s) for s in fnames2 if s.isdigit()]\n",
        "operation_condition = sorted(operation_condition)\n",
        "#operation_condition = list(range(46, 91)) + list(range(99, 114))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data3 = pd.read_excel('/content/sample_data/DLdata/dataLMS/RunTable.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "count1 = 0\n",
        "for i  in operation_condition: #len(fnames2)\n",
        "\n",
        "  filename3 = os.path.join(path1,str(i)+'.csv')\n",
        "  data3 = pd.read_csv(filename3,header = None)\n",
        "  if(min(data3.iloc[:,0]-data3.iloc[:,16])>casoid):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   df3.loc[count1,'filenames'] = filename3\n",
        "   df3.loc[count1,'class'] = str(i)\n",
        "   count1 = count1 + 1\n",
        "   XData3test = pd.concat([XData3test,data3],ignore_index=True,axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################################################\n",
        "\n",
        "path1 ='/content/sample_data/TimeS/TimeS481/LMS/'\n",
        "path2 ='/content/sample_data/TimeS/TimeS481/RMS/'\n",
        "path3 ='/content/sample_data/TimeS/TimeS481/D/'\n",
        "path4 ='/content/sample_data/TimeS/CA50481/'\n",
        "\n",
        "fnames = glob.glob(path1+'*.csv')\n",
        "\n",
        "\n",
        "\n",
        "fnames2 = []\n",
        "i = 0\n",
        "for f in fnames:\n",
        " fnames2.append(os.path.splitext(os.path.split(f)[-1])[-2])\n",
        " i = i + 1\n",
        "\n",
        "operation_condition = [int(s) for s in fnames2 if s.isdigit()]\n",
        "operation_condition = sorted(operation_condition)\n",
        "#operation_condition = list(range(46, 91)) + list(range(99, 114))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#data3 = pd.read_excel('/content/sample_data/DLdata/dataLMS/RunTable.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i  in operation_condition: #len(fnames2)\n",
        "\n",
        "  filename3 = os.path.join(path1,str(i)+'.csv')\n",
        "  data3 = pd.read_csv(filename3,header = None)\n",
        "  if(min(data3.iloc[:,0]-data3.iloc[:,16])>casoid):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    df3.loc[count1,'filenames'] = filename3\n",
        "    df3.loc[count1,'class'] = str(i)\n",
        "    count1 = count1 + 1\n",
        "    XData3test = pd.concat([XData3test,data3],ignore_index=True,axis=0)\n",
        "\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "YData = XData3.iloc[:,0]-XData3.iloc[:,16]\n",
        "YDatatest = XData3test.iloc[:,0]-XData3test.iloc[:,16]\n",
        "YDataval = XData3val.iloc[:,0]-XData3val.iloc[:,16]\n",
        "\n",
        "X_Trainnum = XData3.iloc[:,16]\n",
        "X_valnum = XData3val.iloc[:,16]\n",
        "X_Testnum = XData3test.iloc[:,16]\n",
        "\n",
        "###############################################################################\n",
        "###############################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "sh5nNMcaoEjm",
        "outputId": "304adf1c-378a-4950-dfa7-302c2463e4d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-48266130-008c-4b63-973e-1ab724127080\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48266130-008c-4b63-973e-1ab724127080')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-48266130-008c-4b63-973e-1ab724127080 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-48266130-008c-4b63-973e-1ab724127080');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "XData3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIZoAcsfl6tR"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_samples = len(df1)\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "\n",
        "      # Initialize empty arrays to store batch data\n",
        "batch_LMS = pd.DataFrame()\n",
        "batch_RMS = pd.DataFrame()\n",
        "batch_CD = pd.DataFrame()\n",
        "\n",
        "for idx in indices:\n",
        "          # Load and preprocess the images\n",
        "          fpath1 = os.path.split(df1.iloc[idx]['filenames'])[-2] + '/LMS/' + df1.iloc[idx]['class'] + '.csv'\n",
        "          fpath2 = os.path.split(df1.iloc[idx]['filenames'])[-2] + '/RMS/' + df1.iloc[idx]['class'] + '.csv'\n",
        "          fpath3 = os.path.split(df1.iloc[idx]['filenames'])[-2] + '/D/' + df1.iloc[idx]['class'] + '.csv'\n",
        "\n",
        "          data1 = pd.read_csv(fpath1,header=None)\n",
        "          data2 = pd.read_csv(fpath2,header=None)\n",
        "          data3 = pd.read_csv(fpath3,header=None)/100\n",
        "        #  data1 = data1/np.max(data1,axis = 0)\n",
        "        #  data2 = data2/np.max(data2,axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          batch_LMS = pd.concat([batch_LMS,data1],ignore_index=True,axis = 1)\n",
        "          batch_RMS = pd.concat([batch_RMS,data2],axis = 1)\n",
        "          batch_CD = pd.concat([batch_CD,data3],axis = 1)\n",
        "\n",
        "batch_LMS = np.array(batch_LMS.T)\n",
        "batch_RMS = np.array(batch_RMS.T)\n",
        "batch_CD = np.array(batch_CD.T)\n",
        "\n",
        "X_Train = np.stack((batch_LMS, batch_RMS, batch_CD), axis=2)\n",
        "\n",
        "num_samples = len(df2)\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "\n",
        "      # Initialize empty arrays to store batch data\n",
        "batch_LMS = pd.DataFrame()\n",
        "batch_RMS = pd.DataFrame()\n",
        "batch_CD = pd.DataFrame()\n",
        "\n",
        "for idx in indices:\n",
        "          # Load and preprocess the images\n",
        "          fpath1 = os.path.split(df2.iloc[idx]['filenames'])[-2] + '/LMS/' + df2.iloc[idx]['class'] + '.csv'\n",
        "          fpath2 = os.path.split(df2.iloc[idx]['filenames'])[-2] + '/RMS/' + df2.iloc[idx]['class'] + '.csv'\n",
        "          fpath3 = os.path.split(df2.iloc[idx]['filenames'])[-2] + '/D/' + df2.iloc[idx]['class'] + '.csv'\n",
        "\n",
        "          data1 = pd.read_csv(fpath1,header=None)\n",
        "          data2 = pd.read_csv(fpath2,header=None)\n",
        "          data3 = pd.read_csv(fpath3,header=None)/100\n",
        "          #data1 = data1/np.max(data1,axis = 0)\n",
        "          #data2 = data2/np.max(data2,axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          batch_LMS = pd.concat([batch_LMS,data1],ignore_index=True,axis = 1)\n",
        "          batch_RMS = pd.concat([batch_RMS,data2],axis = 1)\n",
        "          batch_CD = pd.concat([batch_CD,data3],axis = 1)\n",
        "\n",
        "batch_LMS = np.array(batch_LMS.T)\n",
        "batch_RMS = np.array(batch_RMS.T)\n",
        "batch_CD = np.array(batch_CD.T)\n",
        "\n",
        "X_val = np.stack((batch_LMS, batch_RMS, batch_CD), axis=2)\n",
        "\n",
        "num_samples = len(df3)\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "\n",
        "      # Initialize empty arrays to store batch data\n",
        "batch_LMS = pd.DataFrame()\n",
        "batch_RMS = pd.DataFrame()\n",
        "batch_CD = pd.DataFrame()\n",
        "\n",
        "for idx in indices:\n",
        "          # Load and preprocess the images\n",
        "          fpath1 = os.path.split(df3.iloc[idx]['filenames'])[-2] + '/LMS/' + df3.iloc[idx]['class'] + '.csv'\n",
        "          fpath2 = os.path.split(df3.iloc[idx]['filenames'])[-2] + '/RMS/' + df3.iloc[idx]['class'] + '.csv'\n",
        "          fpath3 = os.path.split(df3.iloc[idx]['filenames'])[-2] + '/D/' + df3.iloc[idx]['class'] + '.csv'\n",
        "\n",
        "          data1 = pd.read_csv(fpath1,header=None)\n",
        "          data2 = pd.read_csv(fpath2,header=None)\n",
        "          data3 = pd.read_csv(fpath3,header=None)/100\n",
        "         # data1 = data1/np.max(data1,axis = 0)\n",
        "          #data2 = data2/np.max(data2,axis = 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          batch_LMS = pd.concat([batch_LMS,data1],ignore_index=True,axis = 1)\n",
        "          batch_RMS = pd.concat([batch_RMS,data2],axis = 1)\n",
        "          batch_CD = pd.concat([batch_CD,data3],axis = 1)\n",
        "\n",
        "batch_LMS = np.array(batch_LMS.T)\n",
        "batch_RMS = np.array(batch_RMS.T)\n",
        "batch_CD = np.array(batch_CD.T)\n",
        "\n",
        "X_Test = np.stack((batch_LMS, batch_RMS, batch_CD), axis=2)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Assuming your original matrix is named 'original_matrix'\n",
        "# and it has dimensions n x 1510 x 3\n",
        "\n",
        "# Create a sample matrix with dimensions n x 1510 x 3\n",
        "def interp_data(data,new_columns):\n",
        "\n",
        "\n",
        "  original_matrix = data\n",
        "\n",
        "\n",
        "# Create linear interpolation functions for each channel (3 channels)\n",
        "  interpolated_channels = [interp1d(np.arange(data.shape[1]), original_matrix[:, :, i], kind='linear', axis=1, fill_value=\"extrapolate\") for i in range(3)]\n",
        "\n",
        "# Define the new number of columns (500)\n",
        "\n",
        "# Interpolate each channel to get the new matrix with dimensions n x 500 x 3\n",
        "\n",
        "  interpolated_matrix = np.stack([interp(np.linspace(0, 1509, new_columns)) for interp in interpolated_channels], axis=-1)\n",
        "  return interpolated_matrix\n",
        "# 'interpolated_matrix' now has dimensions n x 500 x 3\n",
        "\n",
        "\n",
        "\n",
        "X_Train2 = X_Train[:,0:1000,:]\n",
        "X_Test2 = X_Test[:,0:1000,:]\n",
        "X_val2 = X_val[:,0:1000,:]\n",
        "\n",
        "\n",
        "X_Train3 = interp_data(X_Train2,500)\n",
        "X_Test3 = interp_data(X_Test2,500)\n",
        "X_val3 = interp_data(X_val2,500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcWMHV3amB51",
        "outputId": "8fb2f406-884a-4775-ead9-73e3babcb468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Sequential\n",
        "\n",
        "# 1D ResNet Block\n",
        "class ResidualBlock1D(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides=1, padding='same'):\n",
        "        super(ResidualBlock1D, self).__init__()\n",
        "\n",
        "        self.conv1 = layers.Conv1D(filters, kernel_size, strides=strides, padding=padding)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.relu = layers.ReLU()\n",
        "\n",
        "        self.conv2 = layers.Conv1D(filters, kernel_size, strides=strides, padding=padding)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        self.shortcut = layers.Conv1D(filters, kernel_size=1, strides=strides, padding='same')\n",
        "        self.shortcut_bn = layers.BatchNormalization()\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        shortcut = self.shortcut(inputs)\n",
        "        shortcut = self.shortcut_bn(shortcut, training=training)\n",
        "\n",
        "        x += shortcut\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 1D ResNet Model\n",
        "class ResNet1D(tf.keras.Model):\n",
        "    def __init__(self, input_shape, num_blocks=1, filters=36):\n",
        "        super(ResNet1D, self).__init__()\n",
        "\n",
        "        self.in_channels = filters\n",
        "\n",
        "        self.conv1 = layers.Conv1D(self.in_channels, kernel_size=3, strides=2, padding='same', input_shape=input_shape)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.relu = layers.ReLU()\n",
        "\n",
        "        self.res_blocks = [ResidualBlock1D(filters, kernel_size=3) for _ in range(num_blocks)]\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x, training=training)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Numeric Input Model\n",
        "class NumericModel(tf.keras.Model):\n",
        "    def __init__(self, num_numeric_features):\n",
        "        super(NumericModel, self).__init__()\n",
        "\n",
        "        self.dense2 = layers.Dense(2, activation='relu')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.dense2(inputs)\n",
        "        return x\n",
        "\n",
        "# Combined Model with LSTM\n",
        "class CombinedModelWithLSTM(tf.keras.Model):\n",
        "    def __init__(self, input_shape_time_series, num_numeric_features):\n",
        "        super(CombinedModelWithLSTM, self).__init__()\n",
        "\n",
        "        self.resnet = ResNet1D(input_shape_time_series)\n",
        "        self.numeric_model = NumericModel(num_numeric_features)\n",
        "\n",
        "        # Adjust the number of units in the dense layer to match the ResNet output\n",
        "\n",
        "        # Concatenate the output of the ResNet and Numeric branches\n",
        "        self.lstm = layers.Bidirectional(layers.LSTM(5))  # You can adjust the number of LSTM units\n",
        "        self.concat = layers.Concatenate()\n",
        "        self.dense = layers.Dense(2, activation='relu')  # For regression\n",
        "\n",
        "\n",
        "        self.output_layer = layers.Dense(1, activation='linear')  # For regression\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        time_series_data, numeric_data = inputs\n",
        "\n",
        "        # Process time series data through ResNet branch\n",
        "        x_time_series = self.resnet(time_series_data, training=training)\n",
        "\n",
        "        # Process numeric data through Numeric branch\n",
        "        x_numeric = self.numeric_model(numeric_data, training=training)\n",
        "\n",
        "\n",
        "        # Process the  features through LSTM layer\n",
        "        x = self.lstm(x_time_series)  # LSTM expects 3D input, add an extra dimension\n",
        "\n",
        "        # Concatenate the outputs of both branches\n",
        "        x = self.concat([x, x_numeric])\n",
        "\n",
        "        x = self.dense(x)\n",
        "\n",
        "\n",
        "\n",
        "        # Output layer\n",
        "        output = self.output_layer(x)\n",
        "\n",
        "        return output\n",
        "# Example usage\n",
        "sequence_length = 500  # Adjust according to your data\n",
        "num_variables = 3  # Adjust according to your data\n",
        "num_numeric_features = 1  # Adjust according to your data\n",
        "\n",
        "input_shape_time_series = (sequence_length, num_variables)\n",
        "input_shape_numeric = (num_numeric_features,)\n",
        "\n",
        "combined_model = CombinedModelWithLSTM(input_shape_time_series, num_numeric_features)\n",
        "\n",
        "# Compile the model for regression\n",
        "combined_model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvIO1LQemHjQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Define ModelCheckpoint callback to save the model weights\n",
        "checkpoint_callback = ModelCheckpoint(filepath='lstm_IA_fi_2.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      save_best_only=True,\n",
        "                                      save_weights_only=True,\n",
        "                                      mode='min',  # or 'max' depending on your metric\n",
        "                                      verbose=1)\n",
        "\n",
        "# Train the model with your data and use the ModelCheckpoint callback\n",
        "# Assuming you have 'train_data', 'train_labels', 'val_data', and 'val_labels'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku3HAt6oN9Te",
        "outputId": "aef3ca24-fba1-4ad4-d15d-c50c7672b177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"combined_model_with_lstm\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " res_net1d (ResNet1D)        multiple                  10116     \n",
            "                                                                 \n",
            " numeric_model (NumericMode  multiple                  4         \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional (Bidirection  multiple                  1680      \n",
            " al)                                                             \n",
            "                                                                 \n",
            " concatenate (Concatenate)   multiple                  0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  26        \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11829 (46.21 KB)\n",
            "Trainable params: 11541 (45.08 KB)\n",
            "Non-trainable params: 288 (1.12 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "5B1iQRfDmKIf",
        "outputId": "84b022b6-f951-472f-f713-9a20d6ffebf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x79a17dc0c040>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 97, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9c3b10a925c0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# After training, you can load the best weights using the following:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m combined_model.fit([X_Train3, X_Trainnum], YData.values,\n\u001b[0m\u001b[1;32m      3\u001b[0m                              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                              validation_data=([X_val3, X_valnum], YDataval.values),callbacks=[checkpoint_callback],batch_size=100,shuffle=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# no_variable_creation function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m         return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# After training, you can load the best weights using the following:\n",
        "combined_model.fit([X_Train3, X_Trainnum], YData.values,\n",
        "                             epochs=10000,\n",
        "                             validation_data=([X_val3, X_valnum], YDataval.values),callbacks=[checkpoint_callback],batch_size=100,shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t5UA6U-mOYj",
        "outputId": "2dbe98dc-9371-436e-90b7-d085d4f5b948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "152/152 [==============================] - 1s 7ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6.1151061475266"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "y_pred = combined_model.predict([X_Test3,X_Testnum])\n",
        "np.sum(np.square(YDatatest.values - y_pred.flatten()))/len(y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbJIwWWQmSLA",
        "outputId": "d353419d-02ca-4cee-e70c-ea549b4e0da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "152/152 [==============================] - 2s 7ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "7.590776882808135"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model2 = CombinedModelWithLSTM(input_shape_time_series=(input_shape_time_series), num_numeric_features=(num_numeric_features))\n",
        "dummy_input = (tf.zeros((1,) + input_shape_time_series), tf.zeros((1, num_numeric_features)))\n",
        "model2(dummy_input)\n",
        "\n",
        "# Load the weights\n",
        "\n",
        "model2.load_weights('lstm_IA_fi_2.h5')\n",
        "\n",
        "\n",
        "y_pred = model2.predict([X_Test3,X_Testnum])\n",
        "np.sum(np.square(YDatatest.values - y_pred.flatten()))/len(y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "JijWIfNauCee",
        "outputId": "c67201d2-2bd5-49bb-b075-8f4990689249"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd19a969ae0>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "plt.grid( which='major', color='k', linestyle='--')\n",
        "plt.grid(which='minor', color='r', linestyle='-', alpha=0.2)\n",
        "plt.plot(YDatatest.values)\n",
        "plt.plot(y_pred)\n",
        "plt.minorticks_on()\n",
        "plt.xlabel(\"Cycles\")\n",
        "plt.ylabel(\"CA50-Inj\")\n",
        "plt.legend(['actual','predicted'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jDCX4FXZuMEX",
        "outputId": "88e01c01-6053-4881-df04-defa2dfba6dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE for samples 1-97: 0.7977936854682628\n",
            "RMSE for samples 98-194: 0.3695687301613067\n",
            "RMSE for samples 195-291: 0.415313990063969\n",
            "RMSE for samples 292-388: 0.4219295928673975\n",
            "RMSE for samples 389-485: 0.36794033959517225\n",
            "RMSE for samples 486-582: 0.34417185581050985\n",
            "RMSE for samples 583-679: 0.32529131361239544\n",
            "RMSE for samples 680-776: 0.47763596962300564\n",
            "RMSE for samples 777-873: 1.3189718133510377\n",
            "RMSE for samples 874-970: 0.3218785695739428\n",
            "RMSE for samples 971-1067: 0.24019136185758208\n",
            "RMSE for samples 1068-1164: 0.25371588978742043\n",
            "RMSE for samples 1165-1261: 0.329205637049073\n",
            "RMSE for samples 1262-1358: 0.3225895528331097\n",
            "RMSE for samples 1359-1455: 0.34098053309555043\n",
            "RMSE for samples 1456-1552: 9.623963327774465\n",
            "RMSE for samples 1553-1649: 6.244430987949145\n",
            "RMSE for samples 1650-1746: 0.7001012952498182\n",
            "RMSE for samples 1747-1843: 0.46151440589659015\n",
            "RMSE for samples 1844-1940: 6.345053250419699\n",
            "RMSE for samples 1941-2037: 6.402410198727628\n",
            "RMSE for samples 2038-2134: 6.654837650408933\n",
            "RMSE for samples 2135-2231: 0.6719298644773495\n",
            "RMSE for samples 2232-2328: 0.4969640535078914\n",
            "RMSE for samples 2329-2425: 0.3859095966706029\n",
            "RMSE for samples 2426-2522: 0.42241723601985776\n",
            "RMSE for samples 2523-2619: 0.1979060824713087\n",
            "RMSE for samples 2620-2716: 0.17807389017168393\n",
            "RMSE for samples 2717-2813: 0.17213313249584572\n",
            "RMSE for samples 2814-2910: 0.16740503184240546\n",
            "RMSE for samples 2911-3007: 0.13983227504684825\n",
            "RMSE for samples 3008-3104: 0.1146670036817446\n",
            "RMSE for samples 3105-3201: 0.12440119117356437\n",
            "RMSE for samples 3202-3298: 0.1406150696735084\n",
            "RMSE for samples 3299-3395: 0.28033736553572236\n",
            "RMSE for samples 3396-3492: 0.2672478281312046\n",
            "RMSE for samples 3493-3589: 0.27832205043752134\n",
            "RMSE for samples 3590-3686: 0.2971232446308554\n",
            "RMSE for samples 3687-3783: 0.9479065482196952\n",
            "RMSE for samples 3784-3880: 0.5868813114160336\n",
            "RMSE for samples 3881-3977: 0.38895377415686216\n",
            "RMSE for samples 3978-4074: 0.3688289257445943\n",
            "RMSE for samples 4075-4171: 0.2722359670052642\n",
            "RMSE for samples 4172-4268: 0.2565099952710135\n",
            "RMSE for samples 4269-4365: 0.2036728550530575\n",
            "RMSE for samples 4366-4462: 0.19851504266698328\n",
            "RMSE for samples 4463-4559: 1.2346889241853158\n",
            "RMSE for samples 4560-4656: 2.631416623956838\n",
            "RMSE for samples 4657-4753: 5.6538163232971925\n",
            "RMSE for samples 4754-4850: 0.4383563118561776\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'RMSE(deg)')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.grid( which='major', color='k', linestyle='--')\n",
        "plt.grid(which='minor', color='r', linestyle='-', alpha=0.2)\n",
        "plt.plot((np.abs(YDatatest.values - y_pred.flatten())))\n",
        "plt.plot((np.abs(YDatatest.values - y_pred.flatten())),'+')\n",
        "plt.axhline(y=1, color='red', linestyle='--', label='y=5')\n",
        "\n",
        "plt.ylabel(\"Error(degree)\")\n",
        "plt.xlabel(\"Cycles\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.grid( which='major', color='k', linestyle='--')\n",
        "plt.grid(which='minor', color='r', linestyle='-', alpha=0.2)\n",
        "plt.plot((np.abs(YDatatest.values - y_pred.flatten()))*100/np.abs(YDatatest.values))\n",
        "plt.plot((np.abs(YDatatest.values - y_pred.flatten()))*100/np.abs(YDatatest.values),'+')\n",
        "plt.axhline(y=5, color='red', linestyle='--', label='y=5')\n",
        "\n",
        "plt.ylabel(\"Error %\")\n",
        "plt.xlabel(\"Cycles\")\n",
        "\n",
        "\n",
        "# Calculate RMSE for every 9 samples\n",
        "window_size = 97\n",
        "window_size2 = len(XData3test)/97\n",
        "\n",
        "rmse_list = []\n",
        "\n",
        "for i in range(0, len(y_pred), window_size):\n",
        "    # Extract a window of 9 samples\n",
        "    window = y_pred[i:i + window_size]\n",
        "    value =  YDatatest[i:i + window_size]\n",
        "    # Calculate the RMSE for the window\n",
        "    rmse = sqrt(mean_squared_error(window,value))\n",
        "    rmse_list.append(rmse)\n",
        "\n",
        "# Print the RMSE values for each window\n",
        "for i, rmse in enumerate(rmse_list):\n",
        "    print(f\"RMSE for samples {i * window_size + 1}-{(i + 1) * window_size}: {rmse}\")\n",
        "plt.plot(np.arange(1,window_size2+1),rmse_list)\n",
        "plt.scatter(np.arange(1,window_size2+1),rmse_list)\n",
        "\n",
        "plt.xlabel(\"Operating Conditions\")\n",
        "plt.ylabel(\"RMSE(deg)\")\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "plt.plot(YDatatest)\n",
        "plt.minorticks_on()\n",
        "plt.xlabel(\"Cycles\")\n",
        "plt.ylabel(\"CA50-Inj(deg)\")\n",
        "plt.legend(['actual'])\n",
        "\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.plot(YDatatest)\n",
        "plt.plot(y_pred)\n",
        "plt.minorticks_on()\n",
        "plt.xlabel(\"Cycles\")\n",
        "plt.ylabel(\"CA50-Inj(deg)\")\n",
        "plt.legend(['actual','predicted'])\n",
        "\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.plot(np.arange(1,window_size2+1),rmse_list)\n",
        "plt.scatter(np.arange(1,window_size2+1),rmse_list)\n",
        "plt.axhline(y=1, color='red', linestyle='--', label='y=5')\n",
        "\n",
        "plt.xlabel(\"Operating Conditions\")\n",
        "plt.ylabel(\"RMSE(deg)\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMaYtYHu+9P1P5y3hgB1nZh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}